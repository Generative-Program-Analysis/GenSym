\section{From Abstract Interpreters to Staged Abstract Interpreters} \label{sai}

In the previous sections, we have seen an unstaged abstract interpreter and a staged concrete interpreter, 
now we begin describing the implementation of their confluence -- a staged abstract interpreter. 
We present a principled approach to derive staged abstract interpreter
from its unstaged version. One guiding principle of our approach is that the code of the abstract semantics
and the code that optimizes should be separated. This it is an advantage of using staging 
for abstract interpreters: the designer of the analysis has no need to rewrite the analysis, and 
the performance improvement comes almost for free, without any sacrifice of soundness or precision.
Unsurprisingly, the staged abstract interpreter we present in this section has the same abstract 
semantics as the unstaged version we presented in Section~\ref{unstaged_abs}.

\subsection{Staged Lattices} 
In Section~\ref{stagedpoly_lat}, we exploited the higher-kinded type @R@
to leave space for staging lattices, now we instantiate the type @R@ to @Rep@ and 
still use powersets as an example to present its staged version. 

\begin{lstlisting}
trait RepLattice[A] extends Lattice[A, Rep]
implicit def RepSetLattice[T:Typ]: RepLattice[Set[T]] = 
  new RepLattice[Set[T]] {
    lazy val bot: Rep[Set[T]] = Set[T]()
    lazy val top: Rep[Set[T]] = throw new NotImplementedError()
    def $\sqsubseteq$(l1: Rep[Set[T]], l2: Rep[Set[T]]): 
      Rep[Boolean] = l1 subsetOf l2
    def $\sqcup$(l1: Rep[Set[T]], l2: Rep[Set[T]]): 
      Rep[Set[T]] = l1 union l2
    def $\sqcap$(l1: Rep[Set[T]], l2: Rep[Set[T]]): 
      Rep[Set[T]] = l1 intersect l2
  }
\end{lstlisting}

The type parameter @T:Typ@ of @RepLattice@ requires that the elements of sets
is can also be staged. Otherwise, without knowing how to stage the elements in the set,
we can not stage the set either.
The methods defined operate on type @Rep[Set[T]]@, thus the underlying implementation
such as @union@ and @intersect@ will be mapped to a node in the IR graph during staging and 
eventually emitted in the generated code. 
Again, other structures such as maps and tuples are implemented in a similar way.

\subsection{Staged Abstract Semantics} 
We have seen how to obtain a staged concrete semantics based on types, 
now we take the same approach to obtain a staged abstract semantics.
The basic operations are largely kept the same as in the unstaged version, except the types are changed to @Rep@.
Besides, when we update the environment, the identifier @x@ is known statically, but the environment map
has type @Rep[Map[Ident,Addr]]@, so we apply @lift@ to @x@ to turn it as a next-stage value.

\begin{lstlisting}
trait RepAbsInterpOps extends Abstract with LMSOps {
  type R[+T] = Rep[T]
  val $\rho$0: Rep[Env] = Map[Ident, Addr]()
  val $\sigma$0: Rep[Store] = Map[Addr, Value]()
  def get($\rho$: Rep[Env], x: Ident): Rep[Addr] = $\rho$(x)
  def put($\rho$: Rep[Env], x: Ident, a: Rep[Addr]): 
    Rep[Env] = $\rho$ + (lift(x) -> a)
  def get($\sigma$: Rep[Store], a: Rep[Addr]): Rep[Value] = 
    $\sigma$.getOrElse(a, RepLattice[Value].bot)
  def put($\sigma$: Rep[Store], a: Rep[Addr], v: Rep[Value]): 
    Rep[Store] = $\sigma$ + (a -> RepLattice[Value].$\sqcup$(v, get($\sigma$, a)))
  def alloc($\sigma$: Rep[Store], x: Ident): Rep[Addr] = Addr(x)
  def num(i: Lit): Rep[Value] = Set(NumV())
  def branch0(cnd: Rep[Value], thn: => Ans, els: => Ans): Ans =
    thn $\sqcup$ els
  def prim_eval(op: Symbol, 
                v1: Rep[Value], v2: Rep[Value]): Rep[Value] = 
    Set(NumV())
  ...
}
\end{lstlisting}

Once more, the way we handle closures is the same as in the staged concrete interpreter:
the recursive call to @ev@ with the body expression @e@ is compiled and specialized, 
the wrapper function @f@ will be a field value in a @CompiledClo@ object and be generated
for the next stage. At the end, we return a singleton set:

\begin{lstlisting}
def close(ev: EvalFun)($\lambda$: Lam, $\rho$: Rep[Env]): Rep[Value] = {
  val Lam(x, e) = $\lambda$
  val f: Rep[(Value, Store)]=>Rep[(Value,Store)] = {
    case (args: Rep[Value], $\sigma$: Rep[Store]) =>
      val args = as._1; val $\sigma$ = as._2; val $\alpha$ = alloc($\sigma$, x)
      ev(e, put($\rho$, x, $\alpha$), put($\sigma$, $\alpha$, args))
    }
  Set[AbsValue](CompiledClo(fun(f)))
}
def apply_closure(ev: EvalFun)
  (f: Rep[Value], arg: Rep[Value], $\sigma$: Rep[Store]): Ans = {
    reflectEffect(ApplyClosure(f, arg, $\sigma$))
  }
\end{lstlisting}

When generating code for an application, we can not directly apply the callee.
Instead, we emit code that calls a next-stage function @apply_closures_norep@.
As its unstaged counterpart, function @apply_closures_norep@ 
non-deterministically applies multiple target closures with the argument
and latest store, and finally returns the joined value and a single store.
We provide the definition of @apply_closures_norep@ in the runtime supporting code.

\begin{lstlisting}
case ApplyClosures(fs, arg, $\sigma$) =>
  emitValDef(sym, "apply_closures_norep(" + 
                  quote(fs) + "," + quote(arg) + 
                  "," + quote($\sigma$) + ")")
\end{lstlisting}

\subsection{Staged Fixpoint Iteration} 

Our fixed-point iteration again relies on two caches @in@ and @out@, but the iteration 
no longer be done at the current stage.
Because the @in@ and @out@ are both next-stage values, the test of whether 
@in@ and @out@ are equal is a generated expression in the next stage, and we can only 
know the comparison result at the next stage. 
In other words, we do not know how many iterations we need to reach the fixed-point.
To achieve this, we need to stage a function value --- @iter_aux@ is generated as a 
recursive function of type @Rep[Unit => (Value,Store)]@ and will be invoked at the next stage.

\begin{lstlisting}
def iter(e: Expr, $\rho$: Rep[Env], $\sigma$: Rep[Store]): 
Rep[(Value,Store)] = {
  def iter_aux: Rep[Unit => (Value,Store)] = fun { () =>
    in = out; out = Map[Config, (Value,Store)]()
    cached_ev(e, $\rho$, $\sigma$)
    if (in === out) out((unit(e), $\rho$, $\sigma$)) else iter_aux()
  }
  iter_aux() // generated code that invokes iter_aux()
}
\end{lstlisting}

However, the instrumented evaluation function that uses the @in@ cache and updates the @out@ cache
can be completely eliminated by staging. Each recursive call to @cached_ev@ will also be specialized
if it is applied on subexpressions of the analyzed program.

\begin{lstlisting}
def cached_ev(e: Expr, $\rho$: Rep[Env], $\sigma$: Rep[Store]): 
Rep[(Value, Store)] = {
  val cfg: Rep[Config] = (unit(e), $\rho$, $\sigma$)
  if (out.contains(cfg)) { out(cfg) }
  else {
    val ans0 = in.getOrElse(cfg, RepLattice[(Value, Store)].bot)
    out = out + (cfg -> ans0)
    val ans1 = evev(cached_ev)(e, $\rho$, $\sigma$)
    out = out + (cfg -> (ans0 $\sqcup$ ans1)); ans1
  }
}
\end{lstlisting}

\subsection{Specialized Data Structures} \label{staged_ds}

Now we have already obtained an end-to-end staged abstract interpreter that is able to 
specialize an analysis. However, we treat the data structures such as @Map@s as black-boxes,
which means any operations on a @Map@ becomes code in the next stage.
But, as we identified when introducing the generic interface, the keys of any environment maps are 
identifiers in the program, which are completely known statically. 
This leaves us a chance to further specialize the data structures. 
Assume the @Map[K,V]@ is implemented as a hash map, if the keys $K$ are known, then the indices can be computed
statically. Thus the specialized map would be an array @Array[Rep[V]]@ whose elements 
are next-stage values; all the accesses to the array is determined during staging.

Particularly, if we are specializing a monovariant analysis, the address space 
is equivalent to the identifiers, then the environment component can be entirely eliminated, 
and the store is a specialized map as array of @Rep[Value]@ elements.

\subsection{Modular Analysis for Free}

One of the challenges of modern static analysis is program usually depends on
large libraries programs~\cite{toman_et_al:LIPIcs:2017:7121}. 
Can we analyze programs and libraries separately and reuse the result without losing precision? 
Then we can reduce part of the overhead of repeatedly analyzing libraries for different programs.
Indeed, some static analyzers compute summary for a function or a module, which can be reused
later, however they are mostly too conservative or unsound, which both lead to imprecision.

The specialization of abstract interpreter provides a chance to obtain such partial analysis 
result in a mechanized way, but still keeps the analysis sound.
As we see when compiling the closures, we can specialize the abstract interpreter
with respect the body expression of the lambda term without knowing the actual argument.
The programs with some unknown variables are open programs, which is exactly the case 
if we want to analyze programs in a modular way.

For a concrete analysis, for example, $k$-CFA ($k$ > 0) is naturally a whole-program analysis,
because it is inter-procedural and needs the last $k$ calling contexts to distinguish
different call sites, where the calling contexts are dynamic values.
However, it is possible to analyze programs (libraries) separately through specializing an abstract interpreter 
that generates the analysis as the next-stage program and leave the unavailable programs and calling contexts 
as dynamic parameters, and then install these contexts when we have the whole program.

\iffalse
Another perspective: programs are data for an abstract interpreter, so if we have $n$ programs, 
then maybe there can be $n$ stages. 
Probably we can analyze first $m$ programs, and generate a residual abstract interpreter
waiting for the rest $(n-m)$ programs.
These $(n-m)$ programs might be (abstract) arguments for the first $n$ programs, and
the abstract interpreter itself might be a partial abstract interpreter.
\fi

\subsection{Discussion}
%We have presented all of our four different interpreters as shown in Figure~\ref{confluence}.
In the literature of partial evaluation, \citeauthor{10.1007/3-540-61580-6_11} provided guidelines
on what to do and not to do when specializing a concrete interpreter \cite{10.1007/3-540-61580-6_11}. 
We borrow these guidelines and extend them to abstract interpreters. We discuss decisions we made to 
achieve this and examine some alternatives.

\paragraph{Big-step vs Small-step.}
What we implemented is a big-step, compositional abstract interpreter, 
where "compositional" means that every recursive call of our abstract interpreter is applied to proper 
substructures of the current syntactic parameters \cite{10.1007/3-540-61580-6_11}.
This compositionality ensures that specialization can be done by unfolding,
as well as that the specialization procedure terminates.
Nevertheless, it is also possible to specialize a small-step operational abstract semantics ---
\citeauthor{Johnson:2013:OAA:2500365.2500604}
showed this in abstract compilation \cite{Boucher:1996:ACN:647473.727587} style as one of their optimizations
of Abstract Abstract Machines \cite{Johnson:2013:OAA:2500365.2500604}. 
However, the generated abstract bytecode still requires another small-step abstract machine 
to execute, which is an additional engineering efforts.
Another alternative approach for efficient specialization is to write the abstract interpreter 
in a big-step, monadic style \cite{DBLP:journals/pacmpl/DaraisLNH17}.

% direct-style vs CPS

\paragraph{Correctness and Soundness.}
Based on the assumption that LMS preserves the semantics during staging,
we are confident that the staged abstract interpreter does the same analysis compared 
with the unstaged one. Moreover, the optimization done by staging does not compromise
any soundness.

