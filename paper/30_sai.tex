\section{From Abstract Interpreters to Staged Abstract Interpreters} \label{sai}

In the previous section, we have seen an unstaged abstract interpreter and a staged concrete interpreter, 
now we begin describing the implementation of their confluence -- a staged abstract interpreter. 
We present a principled approach to derive staged abstract interpreter
from its unstaged version. One guiding principle of our approach is the code of abstract semantics
and the code that optimizes should be separated. Thus it is also an advantage of doing staging 
for abstract interpreters: the designer of analysis has no need to rewrite the analysis, and 
the performance improvement comes almost for free, without any sacrifice of soundness or precision.
Unsurprisingly, the staged abstract interpreter we present in this section has the same abstract 
semantics with the unstaged version we presented in Section~\ref{unstaged_abs}.

\subsection{Staged Lattices} 
In Section~\ref{stagedpoly_lat}, we exploited the higher-kinded type @R@
to leave space for staging lattices, now we instantiate @R@ to @Rep@ and still use powerset as an 
example to present its staged version. 

\begin{lstlisting}
trait RepLattice[A] extends Lattice[A, Rep]
implicit def RepSetLattice[T:Typ]: RepLattice[Set[T]] = 
  new RepLattice[Set[T]] {
    lazy val bot: Rep[Set[T]] = Set[T]()
    lazy val top: Rep[Set[T]] = throw new NotImplementedError()
    def $\sqsubseteq$(l1: Rep[Set[T]], l2: Rep[Set[T]]): 
      Rep[Boolean] = l1 subsetOf l2
    def $\sqcup$(l1: Rep[Set[T]], l2: Rep[Set[T]]): 
      Rep[Set[T]] = l1 union l2
    def $\sqcap$(l1: Rep[Set[T]], l2: Rep[Set[T]]): 
      Rep[Set[T]] = l1 intersect l2
  }
\end{lstlisting}

The type parameter @T:Typ@ of @RepLattice@ requires the element of sets
is also can be staged, otherwise without knowing how to stage the elements in the set
we can not stage the set as well.
The methods defined operate on type @Rep[Set[T]]@, thus the underlying implementation
such as @union@ and @intersect@ will be mapped to a node in IR graph and emitted in 
the generated code. Again, other structures such as maps and tuples are also implemented
in a similar way.

\subsection{Staged Abstract Semantics} 
We have seen how to obtain a staged concrete semantics
based on types, now we take the same approach to obtain a staged abstract semantics.
\todo{what are static, what are dynamic?}
The basic operations largely kept same as the unstaged version, except the types are changed to @Rep@.
Besides, when we update the environment, the identifier @x@ is known statically, but the map of environment
has type @Rep[Map[Ident,Addr]]@, so we apply @lift@ to @x@ to make it as a next-stage value.

\begin{lstlisting}
trait RepAbsInterpOps extends Abstract with LMSOps {
  type R[+T] = Rep[T]
  val $\rho$0: Rep[Env] = Map[Ident, Addr]()
  val $\sigma$0: Rep[Store] = Map[Addr, Value]()
  def get($\rho$: Rep[Env], x: Ident): Rep[Addr] = $\rho$(x)
  def put($\rho$: Rep[Env], x: Ident, a: Rep[Addr]): 
    Rep[Env] = $\rho$ + (lift(x) -> a)
  def get($\sigma$: Rep[Store], a: Rep[Addr]): Rep[Value] = 
    $\sigma$.getOrElse(a, RepLattice[Value].bot)
  def put($\sigma$: Rep[Store], a: Rep[Addr], v: Rep[Value]): 
    Rep[Store] = $\sigma$ + (a -> RepLattice[Value].$\sqcup$(v, get($\sigma$, a)))
  def alloc($\sigma$: Rep[Store], x: Ident): Rep[Addr] = Addr(x)
  def num(i: Lit): Rep[Value] = Set(NumV())
  def branch0(cnd: Rep[Value], thn: => Ans, els: => Ans): Ans =
    thn $\sqcup$ els
  def prim_eval(op: Symbol, 
                v1: Rep[Value], v2: Rep[Value]): Rep[Value] = 
    Set(NumV())
  ...
}
\end{lstlisting}

Once more, the way we handle closures is as same as the staged concrete interpreter:
the recursive call to @ev@ with the body expression @e@ is compiled and specialized, 
the wrapper function @f@ and will be a field value of @CompiledClo@ and be generated
for the next stage. At the end, we return a singleton set.

\begin{lstlisting}
def close(ev: EvalFun)($\lambda$: Lam, $\rho$: Rep[Env]): Rep[Value] = {
  val Lam(x, e) = $\lambda$
  val f: Rep[(Value, Store)]=>Rep[(Value,Store)] = {
    case (args: Rep[Value], $\sigma$: Rep[Store]) =>
      val args = as._1; val $\sigma$ = as._2; val $\alpha$ = alloc($\sigma$, x)
      ev(e, put($\rho$, x, $\alpha$), put($\sigma$, $\alpha$, args))
    }
  Set[AbsValue](CompiledClo(fun(f)))
}
def apply_closure(ev: EvalFun)
  (f: Rep[Value], arg: Rep[Value], $\sigma$: Rep[Store]): Ans = {
    reflectEffect(ApplyClosure(f, arg, $\sigma$))
  }
\end{lstlisting}

When generating code for application, we can not directly apply the callee.
Instead, we emit code that calls a next-stage function @apply_closures_norep@.
Function @apply_closures_norep@ does the same job as its unstaged version,
which non-deterministically applies multiple target closures with the argument
and latest store, and finally returns the joined value and a single store.
We provide the definition of @apply_closures_norep@ in the runtime supporting code.

\begin{lstlisting}
case ApplyClosures(fs, arg, $\sigma$) =>
  emitValDef(sym, "apply_closures_norep(" + 
                  quote(fs) + "," + quote(arg) + 
                  "," + quote($\sigma$) + ")")
\end{lstlisting}

\subsection{Staged Fixpoint Iteration} 

Our fixed-point iteration relies two caches @in@ and @out@, but the iteration 
can not be done at the current stage.
Because the @in@ and @out@ are both next-stage values, thus the test of whether 
@in@ and @out@ are equal is a generated expression to the next stage, and we can only 
know the comparison result at the next stage. 
In other words, we do not know how many iterations we need to reach the fixed-point.
To achieve this, we need to stage a function value --- @iter_aux@ is generated as a 
recursive function of type @Rep[Unit => (Value,Store)]@ and will be invoked at the next stage.

\begin{lstlisting}
def iter(e: Expr, $\rho$: Rep[Env], $\sigma$: Rep[Store]): 
Rep[(Value,Store)] = {
  def iter_aux: Rep[Unit => (Value,Store)] = fun { () =>
    in = out; out = Map[Config, (Value,Store)]()
    cached_ev(e, $\rho$, $\sigma$)
    if (in === out) out((unit(e), $\rho$, $\sigma$)) else iter_aux()
  }
  iter_aux() // generated code that invokes iter_aux()
}
\end{lstlisting}

However, the instrumented evaluation function that uses @in@ cache and updates @out@ cache
can be completely eliminated by staging. Each recursive call to @cached_ev@ will also be specialized
if it is applied on subexpressions of the analyzed program.

\begin{lstlisting}
def cached_ev(e: Expr, $\rho$: Rep[Env], $\sigma$: Rep[Store]): 
Rep[(Value, Store)] = {
  val cfg: Rep[Config] = (unit(e), $\rho$, $\sigma$)
  if (out.contains(cfg)) { out(cfg) }
  else {
    val ans0 = in.getOrElse(cfg, RepLattice[(Value, Store)].bot)
    out = out + (cfg -> ans0)
    val ans1 = evev(cached_ev)(e, $\rho$, $\sigma$)
    out = out + (cfg -> (ans0 $\sqcup$ ans1)); ans1
  }
}
\end{lstlisting}

\subsection{Specialized Data Structures} \label{staged_ds}

Now we already obtained an end-to-end staged abstract interpreter that is able to 
specialize an analysis. However, we treat the data structures such as @Map@s as black-boxes,
which means any operations on a @Map@ becomes code in the next stage.
But, as we identified before \todo{where}, the keys of environment are identifiers in the program,
which are completely known statically. This leaves us a chance to further specialize the 
data structures. Particularly, if we are specializing a monovariant analysis, the address space 
is equivalent to the identifiers, then the environment component can be entirely eliminated, 
and the store is a partially specialized map.
\todo{maps to arrays}

\subsection{Modular Analysis for Free}

Motivation: one of the challenges of modern static analysis is program usually depends on
large libraries programs\cite{toman_et_al:LIPIcs:2017:7121}. 
Can we analyze programs and libraries separately without losing precision? So that we can 
reduce part of the overhead of repeatedly analyzing libraries for different programs.
Similarly, some static analyzers compute summary for a function or a module, that can be reused
later (like Facebook Infer). But to my knowledge, they are mostly too conservative (context-insensitive) 
or unsound, which both lead to imprecision.

Application: for example, k-CFA (k > 0) is naturally a kind of whole program analysis,
because it is interprocedural and need the last k calling contexts to distinguish
different call sites.
But can we analyze programs (libraries) separately which generate the specialized 
analysis and leave the unavailable programs (for the moment) as dynamic parameters, 
and then install these contexts when we have the whole program.

Another perspective: programs are data for an abstract interpreter, so if we have $n$ programs, 
then maybe there can be $n$ stages. 
Probably we can analyze first $m$ programs, and generate a residual abstract interpreter
waiting for the rest $(n-m)$ programs.
These $(n-m)$ programs might be (abstract) arguments for the first $n$ programs, and
the abstract interpreter itself might be a partial abstract interpreter.

\subsection{Discussion}
%We have presented all of our four different interpreters as shown in Figure~\ref{confluence}.
In the literature of partial evaluation, \citeauthor{10.1007/3-540-61580-6_11} gave several
advice on what to do and not to do when specializing a concrete interpreter \cite{10.1007/3-540-61580-6_11}. 
We borrow these advice to abstract interpreters and discuss decisions we made to achieve this 
and examine some alternatives.

\paragraph{Big-step vs Small-step.}
What we implemented is a big-step, compositional \todo{or semi-compositional} abstract interpreter, 
where the "compositional" means every recursive call of our abstract interpreter is applied with proper 
substructures of the current syntactic parameters. 
The compositionality ensures the specialization can be done by unfolding, as well as the termination of specialization procedure.
Nevertheless, it is also possible to specialize a small-step, operational abstract semantics --- \citeauthor{Johnson:2013:OAA:2500365.2500604}
showed this in abstract compilation \cite{Boucher:1996:ACN:647473.727587} style as one of their optimization 
to Abstract Abstract Machines \cite{Johnson:2013:OAA:2500365.2500604}. 
However, the generated abstract bytecodes still requires another small-step abstract machine 
to execute, which is an additional engineering efforts.
Another alternative approach for efficient specialization is to write the abstract interpreter 
in a big-step, monadic style \cite{DBLP:journals/pacmpl/DaraisLNH17}.

% direct-style vs CPS

\paragraph{Correctness and Soundness.}
Based on the assumption that LMS preserves the semantics during staging, and indeed it is 
in our application, 
we are confident that the staged abstract interpreter does the same analysis compared 
with the unstaged one. Moreover, the optimization done by staging does not compromise
and soundness.

