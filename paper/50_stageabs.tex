\section{From Abstract Interpreters to Staged Abstract Interpreters} \label{sai}

In the previous sections, we have seen an unstaged abstract interpreter and a
staged concrete interpreter, now we begin describing the implementation of their
confluence -- a staged abstract interpreter.
Unsurprisingly, the staged abstract interpreter in this section has the same
abstract semantics with the unstaged version from Section~\ref{unstaged_abs}.
The approach from unstaged to staged abstract interpreter is modular,
principled, and does not sacrifice soundness or precision. The designer of the
analyzer has no need to rewrite the analysis. We first present the staged
lattices and staged monads, and then the staged version of primitive operations,
especially @close@, @ap_clo@ and @fix@, at last we discuss several optimizations
and our design.

\subsection{Staged Lattices}

In Section \ref{stagedpoly_lat}, we exploited the higher-kinded type @R@ to
achieve stage polymorphism. Now we instantiate the type @R@ to @Rep@ and
still use power sets as an example to present its staged version.

\begin{lstlisting}
  trait RepLattice[S] extends Lattice[S, Rep]
  def RepSetLattice[T]: RepLattice[Set[T]] = new RepLattice[Set[T]] {
    lazy val ⊥: Rep[Set[T]] = Set[T]()
    lazy val ⊤: Set[T] = error("No representation for ⊤")
    def ⊑(l1: Rep[Set[T]], l2: Rep[Set[T]]): Rep[Boolean] = l1 subsetOf  l2
    def ⊔(l1: Rep[Set[T]], l2: Rep[Set[T]]): Rep[Set[T]]  = l1 union     l2
    def ⊓(l1: Rep[Set[T]], l2: Rep[Set[T]]): Rep[Set[T]]  = l1 intersect l2
  }
\end{lstlisting}

As we can see, the @RepSetLattice@ is an instance of @RepLattice@, which is also
an abstraction of the manipulations on the foundamental data structure. For
staged versions, these operations eventually are delegated to the @subsetOf@,
@union@ and @intersect@ operation on @Rep[Set[T]]@. We even don't have to change
the implementation code, but just the types – from @Set[T]@ to @Rep[Set[T]]@. In
the code generation part, these operations emit their corresponding next-stage
code. Again, other lattice structures such as products and maps are implemented
in a similar way.

\subsection{Staged Abstract Semantics}

Now we implement the staged abstract semantics, where @R@ is instantiated as
@Rep@, and the abstract components are shared from the unstaged version.

\paragraph{Staged Monads for Abstract Interpretation}
We use the same moand stack structure as in the unstaged abstract interpreter,
but replacing \textit{all} the transformers to their staged version. The
following code shows this change. For the readability, we squeeze the three
inner transformers into a single monad @RepListReaderStateM[A, B, C]@, where @A@
and @B@ are both @Cache@ when used in @AnsM@. Now the result type is a pair of
two staged value: @Rep[List[(Value, Store)]]@ and @Rep[Cache]@.

\begin{lstlisting}
  trait StagedAbstractSemantics extends AbstractComponents {
    type R[T] = Rep[T]
    type RepListReaderStateM[A, B, C] = RepListT[RepReaderT[RepStateT[RepIdM, A, ?], B, ?], C]
    type AnsM[T] = ReaderT[StateT[RepListReaderStateM[Cache, Cache, ?], Store, ?], Env, T]
    type Result = (Rep[List[(Value, Store)]], Rep[Cache])
    ...
  }
\end{lstlisting}

Since our monad stack is uniformly using staged data, so the staged @ListT@ now
stores a staged value of type @Rep[List[A]]@, inside of a staged monad @M@.
\todo{staged ListT implementation}

\begin{lstlisting}
  case class ListT[M[_]: RepMonad, A](run: M[List[A]]) {
    def flatMap[B: Manifest](f: Rep[A] => ListT[M, B]): ListT[M, B] = ...
  }
\end{lstlisting}

\paragraph{Primitive Operations}

Once more, the way we handle closures is the same as in the staged concrete
interpreter: the recursive call to @ev@ with the body expression @e@ is compiled
and specialized, the wrapper function @f@ will be a field value in a
@CompiledClo@ object and be generated for the next stage. At the end, we return
a singleton set:

\begin{lstlisting}
  def close(ev: EvalFun)(λ: Lam, ρ: Rep[Env]): Rep[Value] = {
    val Lam(x, e) = λ
    val f: (Rep[Value],Rep[Store],Rep[Cache],Rep[Cache]) => Rep[(List[(Value,Store)],Cache)] = {
      case (arg, σ, in, out) =>
        val α = alloc(σ, x); val ρ_* = ρ + (unit(x) → α); val σ_* = σ ⊔ Map(α → arg)
        ev(e)(ρ_*)(σ_*)(in)(out)
    }
    Set[AbsValue](emit_compiled_clo(f, λ, ρ))
  }
\end{lstlisting}

\begin{lstlisting}
  def lift_nd[T](vs: Rep[List[T]]): AnsM[T]
  def ap_clo(ev: EvalFun)(rator: Rep[Value], rand: Rep[Value]): Ans = for {
    clo <- lift_nd[AbsValue](rator.toList)
    σ <- get_store; in <- ask_in_cache; out <- get_out_cache
    val res: Rep[(List[(Value, Store)], Cache)] = emit_ap_clo(clo, rand, σ, in, out)
    _ <- put_out_cache(res._2)
    vs <- lift_nd[(Value, Store)](res._1)
    _ <- put_store(vs._2)
  } yield vs._1
\end{lstlisting}

\paragraph{Staged Caching and Fixpoint Iteration} 

Our fixed-point iteration again relies on two caches @in@ and @out@, but the
iteration no longer be done at the current stage. Because the @in@ and @out@ are
both next-stage values, the test of whether @in@ and @out@ are equal is a
generated expression in the next stage, and we can only know the comparison
result at the next stage. In other words, we do not know how many iterations we
need to reach the fixed-point. To achieve this, we need to stage a function
value --- @iter_aux@ is generated as a recursive function of type @Rep[Unit => (Value,Store)]@ 
and will be invoked at the next stage.

\begin{figure}[t!]
  \centering
\begin{lstlisting}
  def fix(ev: EvalFun => EvalFun): EvalFun = e => for {
    ρ <- ask_env; σ <- get_store; in <- ask_in_cache; out <- get_out_cache
    val cfg: Rep[Config] = (unit(e), ρ, σ)
    val res: Rep[(List[(Value, Store)], Cache)] =
      if (out.contains(cfg)) (out(cfg).toList, out) // a next-stage if expression
      else { val m: Ans = for {
               _ <- put_out_cache(out + (cfg → in.getOrElse(cfg, ⊥)))
               v <- ev(fix(ev))(e)
               σ <- get_store
               _ <- update_out_cache(cfg, (v, σ))
             } yield v
             m(ρ)(σ)(in)(out) }
    _ <- put_out_cache(res._2); vs <- lift_nd(res._1); _ <- put_store(vs._2)
  } yield vs._1
\end{lstlisting}
\vspace{-1em}
\caption{The staged co-inductive caching algorithm.}
\label{fig:staged_coind_cache}
\end{figure}

However, the instrumented evaluation function that uses the @in@ cache and
updates the @out@ cache can be completely eliminated by staging. Each recursive
call to @cached_ev@ will also be specialized if it is applied on subexpressions
of the analyzed program.

\begin{lstlisting}
\end{lstlisting}

\paragraph{Code Generation} Code generation

\subsection{Optimizations} \label{staged_ds}

Our staging schema works well in theory, but in practice would suffer from code
explosion, and possibly runtime GC overhead when analyzing (specializing) large
programs. In this section, we present several optimizations that largely
mitigate these issues. Still, implementing these optimizations do not need to
change the generic interpreter.

\paragraph{Specialized Data Structures} \todo{need revise}

Now we have already obtained an end-to-end staged abstract interpreter that is
able to specialize an analysis. However, we treat the data structures such as
@Map@s as black-boxes, which means any operations on a @Map@ become code in the
next stage. But, as we identified when introducing the generic interface, the
keys of any environment maps are identifiers in the program, which are
completely known statically. This leaves us a chance to further specialize the
data structures. Assume the @Map[K,V]@ is implemented as a hash map, if the keys
$K$ are known, then the indices can be computed statically. Thus the specialized
map would be an array @Array[Rep[V]]@ whose elements are next-stage values; all
the accesses to the array is determined during staging.

Particularly, if we are specializing a monovariant analysis, the address space
is equivalent to the identifiers, then the environment component can be entirely
eliminated, and the store is a specialized map as array of @Rep[Value]@
elements.

\paragraph{Selective Caching} It is observed that the two-fold co-inductive
caching is used for every recursive call of our abstract interpreter. But this
is not necessary and even very expensive when generating code for atomic
expressions such as literals or variables, as they always terminate. Borrowing
the idea from the partition in ANF \cite{Flanagan:1993:ECC:155090.155113}, we
may use a selective caching algorithm:

\begin{lstlisting}
  def fix_select: EvalFun = e => e match {
    case Lit(_) | Var(_) | Lam(_, _) => eval(fix_select)(e)
    case _ => fix_cache(e)
  }
\end{lstlisting}

\paragraph{Partially-static Data}

Our treatment to binding-times is coarse-grained: @Expr@s is static, the rest of
the world are all dynamic. But this is not always true, since the static data
has to be used somewhere with the dyanmic operation. Partial-static data is a
way to improve binding-times and optimize the generated code.

For example, to fold a singleton list (often appears in @ListT@), e.g.,
@List(x).foldLeft(init)(f)@ where @x@ and @init@ are staged values, a naive code
generator would faithfully apply @foldLeft@ to the list with an anonymous
function. But we can also utilize the algebraic property of @foldLeft@ to
generate cheaper code: @List(x).foldLeft(init)(f) = f(init, x)@. Since the
function @f@ is known at current stage, we completely eliminate the fold
operation and function application. We apply several rewritings enabled by
partial-static data structures, such as @List@ and @Map@, which greatly reduces
the size of residual programs.

\paragraph{Heterogeneous Staging} The generated program is in A-Normal form. \todo{TODO}

\subsection{Discussion}

We have gradually presented the confluence of specialization and abstraction of
concrete interpreters. In this section, we discuss several decisions we made to
achieve this and examine some alternatives.

\paragraph{Are Monads Necessary?} No. One can always inline the monads and
obtain an abstract interpreter in continuation-passing style, or simply use
explicit side-effects such mutation to implement the artifact with the same
abstract semantics. In either cases, we can still apply the staging schema to
the abstract interpreter. As an evidence, we provide an staged abstract
interpreter written in direct-style in the accompanying artifact.

\paragraph{Big-step vs Small-step}

What we implemented is a big-step, compositional abstract interpreter in monadic
style, where \textit{compositional} means that every recursive call of our abstract
interpreter is applied to proper substructures of the current syntactic
parameters \cite{10.1007/3-540-61580-6_11}. This compositionality ensures that
specialization can be done by unfolding, as well as guarantees the termination
of specialization procedure. It is also possible to specialize small-step
operational abstract semantics through abstract compilation
\cite{Boucher:1996:ACN:647473.727587} -- as
\citet{Johnson:2013:OAA:2500365.2500604} implemented it for
optimizating Abstract Abstract Machines. However, the generated abstract
bytecode still requires another small-step abstract machine to execute, which is
an additional engineering efforts.

% direct-style vs CPS

\paragraph{Correctness and Soundness}

As one of the merit of our approach, the staging does not compromise any
soundness, because we only change the underlying monads and data structures to
the staged version, not any part of the abstract semantics. Based on the
assumption that MSP system and staged data structure preserve the equivalence
during staging, we are confident that the staged abstract interpreter does the
same analysis as the unstaged one.

