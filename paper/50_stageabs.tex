\section{From Abstract Interpreters to Staged Abstract Interpreters} \label{sai}

In the previous sections, we have seen an unstaged abstract interpreter and a
staged concrete interpreter, now we begin describing the implementation of their
confluence -- a staged abstract interpreter. We present a principled approach to
derive staged abstract interpreter from its unstaged version. One guiding
principle of our approach is that the code of the abstract semantics and the
code that optimizes should be separated. This it is an advantage of using
staging for abstract interpreters: the designer of the analysis has no need to
rewrite the analysis, and the performance improvement comes almost for free,
without any sacrifice of soundness or precision. Unsurprisingly, the staged
abstract interpreter we present in this section has the same abstract semantics
as the unstaged version we presented in Section~\ref{unstaged_abs}.

\subsection{Staged Lattices}

In Section~\ref{stagedpoly_lat}, we exploited the higher-kinded type @R@ to
leave space for staging lattices, now we instantiate the type @R@ to @Rep@ and
still use powersets as an example to present its staged version.

\begin{lstlisting}
trait RepLattice[A] extends Lattice[A, Rep]
implicit def RepSetLattice[T:Typ]: RepLattice[Set[T]] = 
  new RepLattice[Set[T]] {
    lazy val bot: Rep[Set[T]] = Set[T]()
    lazy val top: Rep[Set[T]] = throw new NotImplementedError()
    def $\sqsubseteq$(l1: Rep[Set[T]], l2: Rep[Set[T]]): 
      Rep[Boolean] = l1 subsetOf l2
    def $\sqcup$(l1: Rep[Set[T]], l2: Rep[Set[T]]): 
      Rep[Set[T]] = l1 union l2
    def $\sqcap$(l1: Rep[Set[T]], l2: Rep[Set[T]]): 
      Rep[Set[T]] = l1 intersect l2
  }
\end{lstlisting}

The type parameter @T:Typ@ of @RepLattice@ requires that the elements of sets
can also be staged. Otherwise, without knowing how to stage the elements in the
set, we can not stage the set either. The methods defined operate on type
@Rep[Set[T]]@, thus the underlying implementation such as @union@ and
@intersect@ will be mapped to a node in the IR graph during staging and
eventually emitted in the generated code. Again, other structures such as maps
and tuples are implemented in a similar way.

\subsection{Staged Abstract Semantics}

We have seen how to obtain a staged concrete semantics based on types, now we
take the same approach to obtain a staged abstract semantics. The basic
operations are largely kept the same as in the unstaged version, except the
types are changed to @Rep@. Besides, when we update the environment, the
identifier @x@ is known statically, but the environment map has type
@Rep[Map[Ident,Addr]]@, so we apply @lift@ to @x@ to turn it as a next-stage
value.

\begin{lstlisting}
trait RepAbsInterpOps extends Abstract with LMSOps {
  type R[+T] = Rep[T]
  val $\rho$0: Rep[Env] = Map[Ident, Addr]()
  val $\sigma$0: Rep[Store] = Map[Addr, Value]()
  def get($\rho$: Rep[Env], x: Ident): Rep[Addr] = $\rho$(x)
  def put($\rho$: Rep[Env], x: Ident, a: Rep[Addr]): 
    Rep[Env] = $\rho$ + (lift(x) -> a)
  def get($\sigma$: Rep[Store], a: Rep[Addr]): Rep[Value] = 
    $\sigma$.getOrElse(a, RepLattice[Value].bot)
  def put($\sigma$: Rep[Store], a: Rep[Addr], v: Rep[Value]): 
    Rep[Store] = $\sigma$ + (a -> RepLattice[Value].$\sqcup$(v, get($\sigma$, a)))
  def alloc($\sigma$: Rep[Store], x: Ident): Rep[Addr] = Addr(x)
  def num(i: Lit): Rep[Value] = Set(NumV())
  def branch0(cnd: Rep[Value], thn: => Ans, els: => Ans): Ans =
    thn $\sqcup$ els
  def prim_eval(op: Symbol, 
                v1: Rep[Value], v2: Rep[Value]): Rep[Value] = 
    Set(NumV())
  ...
}
\end{lstlisting}

Once more, the way we handle closures is the same as in the staged concrete
interpreter: the recursive call to @ev@ with the body expression @e@ is compiled
and specialized, the wrapper function @f@ will be a field value in a
@CompiledClo@ object and be generated for the next stage. At the end, we return
a singleton set:

\begin{lstlisting}
def close(ev: EvalFun)($\lambda$: Lam, $\rho$: Rep[Env]): Rep[Value] = {
  val Lam(x, e) = $\lambda$
  val f: Rep[(Value, Store)]=>Rep[(Value,Store)] = {
    case (args: Rep[Value], $\sigma$: Rep[Store]) =>
      val args = as._1; val $\sigma$ = as._2; val $\alpha$ = alloc($\sigma$, x)
      ev(e, put($\rho$, x, $\alpha$), put($\sigma$, $\alpha$, args))
    }
  Set[AbsValue](CompiledClo(fun(f)))
}
def apply_closure(ev: EvalFun)
  (f: Rep[Value], arg: Rep[Value], $\sigma$: Rep[Store]): Ans = {
    reflectEffect(ApplyClosure(f, arg, $\sigma$))
  }
\end{lstlisting}

When generating code for an application, we can not directly apply the callee.
Instead, we emit code that calls a next-stage function @apply_closures_norep@.
As its unstaged counterpart, function @apply_closures_norep@
non-deterministically applies multiple target closures with the argument and
latest store, and finally returns the joined value and a single store. We
provide the definition of @apply_closures_norep@ in the runtime supporting code.

\begin{lstlisting}
case ApplyClosures(fs, arg, $\sigma$) =>
  emitValDef(sym, "apply_closures_norep(" + 
                  quote(fs) + "," + quote(arg) + 
                  "," + quote($\sigma$) + ")")
\end{lstlisting}

\subsection{Staged Fixpoint Iteration} 

Our fixed-point iteration again relies on two caches @in@ and @out@, but the
iteration no longer be done at the current stage. Because the @in@ and @out@ are
both next-stage values, the test of whether @in@ and @out@ are equal is a
generated expression in the next stage, and we can only know the comparison
result at the next stage. In other words, we do not know how many iterations we
need to reach the fixed-point. To achieve this, we need to stage a function
value --- @iter_aux@ is generated as a recursive function of type @Rep[Unit => (Value,Store)]@ 
and will be invoked at the next stage.

\begin{lstlisting}
def iter(e: Expr, $\rho$: Rep[Env], $\sigma$: Rep[Store]): 
Rep[(Value,Store)] = {
  def iter_aux: Rep[Unit => (Value,Store)] = fun { () =>
    in = out; out = Map[Config, (Value,Store)]()
    cached_ev(e, $\rho$, $\sigma$)
    if (in === out) out((unit(e), $\rho$, $\sigma$)) else iter_aux()
  }
  iter_aux() // generated code that invokes iter_aux()
}
\end{lstlisting}

However, the instrumented evaluation function that uses the @in@ cache and
updates the @out@ cache can be completely eliminated by staging. Each recursive
call to @cached_ev@ will also be specialized if it is applied on subexpressions
of the analyzed program.

\begin{lstlisting}
def cached_ev(e: Expr, $\rho$: Rep[Env], $\sigma$: Rep[Store]): 
Rep[(Value, Store)] = {
  val cfg: Rep[Config] = (unit(e), $\rho$, $\sigma$)
  if (out.contains(cfg)) { out(cfg) }
  else {
    val ans0 = in.getOrElse(cfg, RepLattice[(Value, Store)].bot)
    out = out + (cfg -> ans0)
    val ans1 = evev(cached_ev)(e, $\rho$, $\sigma$)
    out = out + (cfg -> (ans0 $\sqcup$ ans1)); ans1
  }
}
\end{lstlisting}


\subsection{Optimizations} \label{staged_ds}

Our staging schema works well in theory, but would suffer from code explosion,
and possibly runtime GC overhead when analyzing (specializing) large programs.
We present several optimizations that largely mitigate these issues. Still,
implementing these optimizations do not need to change the generic interpreter.

\paragraph{Specialized Data Structures}

Now we have already obtained an end-to-end staged abstract interpreter that is
able to specialize an analysis. However, we treat the data structures such as
@Map@s as black-boxes, which means any operations on a @Map@ become code in the
next stage. But, as we identified when introducing the generic interface, the
keys of any environment maps are identifiers in the program, which are
completely known statically. This leaves us a chance to further specialize the
data structures. Assume the @Map[K,V]@ is implemented as a hash map, if the keys
$K$ are known, then the indices can be computed statically. Thus the specialized
map would be an array @Array[Rep[V]]@ whose elements are next-stage values; all
the accesses to the array is determined during staging.

Particularly, if we are specializing a monovariant analysis, the address space
is equivalent to the identifiers, then the environment component can be entirely
eliminated, and the store is a specialized map as array of @Rep[Value]@
elements.

\paragraph{Selective Caching} It is observed that the two-fold caching is
wrapped for recursive call of our abstract interpreter. But this is even more
expensive when generating code for atomic expressions such as literal numbers or
variables. \todo{caching prevents nontermination}

\paragraph{Partially-static Data}

For example, to fold a singleton list, e.g., @List(x).foldLeft(init)(f)@ where
@x@ and @init@ are staged values, a naive code generator would faithfully
apply @foldLeft@ to the list with an anonymous function. But we can also utilize
the algebraic property of @foldLeft@ to generate cheaper code:
@List(x).foldLeft(init)(f) = f(init, x)@.
Since the function @f@ is known at current stage, we completely eliminate the
fold operation and function application. We apply several rewritings enabled by
partial-static data structures, such as @List@ and @Map@, which greatly reduce
the size of residual programs.

\paragraph{Heterogeneous Staging} The generated program is in A-Normal form, 

\subsection{Discussion}

We have gradually presented the confluence of specialization and abstraction of
concrete interpreters. In this section, we discuss several decisions we made to
achieve this and examine some alternatives.

\paragraph{Are Monads Necessary?} No. One can always inline the monads and
obtain an abstract interpreter in continuation-passing style, or simply use
explicit side-effects such mutation to implement the artifact with the same
abstract semantics. In either cases, we can still apply the staging schema to
the abstract interpreter. As an evidence, we provide an staged abstract
interpreter written in direct-style in the accompanying artifact.

\paragraph{Big-step vs Small-step}

What we implemented is a big-step, compositional abstract interpreter in monadic
style, where \textit{compositional} means that every recursive call of our abstract
interpreter is applied to proper substructures of the current syntactic
parameters \cite{10.1007/3-540-61580-6_11}. This compositionality ensures that
specialization can be done by unfolding, as well as guarantees the termination
of specialization procedure. It is also possible to specialize small-step
operational abstract semantics through abstract compilation
\cite{Boucher:1996:ACN:647473.727587} -- as
\citet{Johnson:2013:OAA:2500365.2500604} implemented it for
optimizating Abstract Abstract Machines. However, the generated abstract
bytecode still requires another small-step abstract machine to execute, which is
an additional engineering efforts.

% direct-style vs CPS

\paragraph{Correctness and Soundness}

As one of the merit of our approach, the staging does not compromise any
soundness, because we only change the underlying monads and data structures to
the staged version, not any part of the abstract semantics. Based on the
assumption that MSP system and staged data structure preserve the equivalence
during staging, we are confident that the staged abstract interpreter does the
same analysis as the unstaged one.

