\section{From Abstract Interpreters to Staged Abstract Interpreters} \label{sai}

In the previous sections, we have seen an unstaged abstract interpreter and a
staged concrete interpreter. Now we begin describing the implementation of
their confluence -- a staged abstract interpreter.  Unsurprisingly, the staged
abstract interpreter in this section uses the same abstract semantics as the
unstaged version in Section~\ref{unstaged_abs}.  The approach we use to refactor
the unstaged one to the staged abstract interpreter is modular, and does not
sacrifice soundness or precision. The designer of the analyzer therefore does not
have to rewrite the analysis. We first present the staged lattices and staged
monads, and then discuss the staged version of primitive operations, especially
@close@, @ap_clo@ and @fix@. In the end, we discuss several optimizations.

\subsection{Staged Lattices}

In Section \ref{stagedpoly_lat}, we exploited the higher-kinded type @R@ to
achieve stage polymorphism. Now we instantiate the type @R@ as @Rep@ and
still use the power-set as example to describe its staged version.
\begin{lstlisting}[escapechar=!]
  trait RepLattice[S] extends Lattice[S, Rep]
  def RepSetLattice[T]: RepLattice[Set[T]] = new RepLattice[Set[T]] {
    lazy val ⊥: !\hl{Rep[Set[T]]}! = Set[T]()
    lazy val ⊤: !\hl{Rep[Set[T]]}! = error("No representation for ⊤")
    def ⊑(l1: !\hl{Rep[Set[T]]}!, l2: !\hl{Rep[Set[T]]}!): !\hl{Rep[Boolean]}! = l1 subsetOf  l2
    def ⊔(l1: !\hl{Rep[Set[T]]}!, l2: !\hl{Rep[Set[T]]}!): !\hl{Rep[Set[T]]}!  = l1 union     l2
    def ⊓(l1: !\hl{Rep[Set[T]]}!, l2: !\hl{Rep[Set[T]]}!): !\hl{Rep[Set[T]]}!  = l1 intersect l2
  }
\end{lstlisting}

The trait @RepLattice@ is shown by instantiating @Lattice@ with the type @Rep@.
For all type @T@, method @RepSetLattice@ provides an instance of @RepLattice@
for @Set[T]@, where the lattice operations are eventually delegated to the
operations on the staged set data type, such as @subsetOf@, @union@ and
@intersect@. 
Compared with the unstaged version, we do not need to change the
implementations, except the types -- such as changing them from @Set[T]@ to
@Rep[Set[T]]@.  We show the changes in light gray code. 
%The LMS library provides implicit conversions that lift the current-stage values to
%their next-stage representations. 
In the code generation part, these operations on staged sets emit their
corresponding next-stage code. Again, other lattice structures such as products
and maps can be implemented in a similar way.

\subsection{Staged Abstract Semantics}

Now we can implement the staged abstract semantics, under that the binding-time
type @R@ is instantiated as @Rep@. The types of abstract components are reused
from the unstaged version.

\paragraph{Staged Monads for Abstract Interpretation}
We use the same monad stack structure as the unstaged abstract interpreter,
but replacing \textit{all} the monad transformers with their staged versions. The
following code snippet shows this change.
We manually fuse the three inner transformers (@IdM@ is omitted) into a single
monad @RepSetReaderStateM[R, S, A]@, where @R@ is the type parameter for the
reader effects, and @S@ is the type for the state.  In our monad stack scheme,
@R@ and @S@ are both instantiated with the type @Cache@. Similar to the unstaged
version, the grounded result type is a pair of two staged values:
@Rep[Set[(Value, Store)]]@ and @Rep[Cache]@.
\begin{lstlisting}
  trait StagedAbstractSemantics extends AbstractComponents {
    type R[T] = Rep[T]
    type AnsM[T] = RepReaderT[RepStateT[RepSetReaderStateM[Cache, Cache, ?], Store, ?], Env, T]
    type Result = (Rep[Set[(Value, Store)]], Rep[Cache])
    ... }
\end{lstlisting}

\paragraph{Primitive Operations} 
When deriving the staged concrete interpreter from its unstaged counterpart, we
notice that where the evaluation @ev(e)@ is invoked are shifted from @ap_clo@ to
@close@, where @e@ is the body expression of a $\lambda$-term.
In other words, when staging, we eagerly specialize the interpreter and
generate code every time we reach a $\lambda$-term, instead of lazily
calling @ev@ when the applications happen.  Similarly, here we use the same way
to handle staged $\lambda$-terms.  Additionally, in the staged abstract
interpreter, we have to handle nondeterminism incurred by the
over-approximation of runtime behavior.  In the rest of this part, we keep our
roadmap and focus on discussing the closure representation and function
application for the staged version.

The @close@ method now is a mix of the \textit{staged} concrete version and
unstaged \textit{abstract} version. We first build a current-stage function
@f@, which takes four next-stage values, including the argument and latest
store, and @in/out@ caches additionally. Inside @f@, we collapse the monadic
value of type @Ans@ to grounded values of type @Result@, by providing the
desired arguments, i.e., the new environment, new store, and two caches. The
collapsing of monadic values happens at the current-stage, so the invocation of
@ev@ is unfolded during staging. Finally, we generate a singleton set
containing the compiled next-stage closure (@emit_compiled_clo@), which is
represented by an IR node in LMS.
\begin{lstlisting}
  def emit_compiled_clo(f: (Rep[Value], Rep[Store], Rep[Cache], Rep[Cache])
                           => Rep[(Set[(Value,Store)], Cache)], λ: Lam, ρ: Exp[Env]): Rep[AbsValue]
  def close(ev: Expr => Ans)(λ: Lam, ρ: Rep[Env]): Rep[Value] = {
    val Lam(x, e) = λ
    val f: (Rep[Value],Rep[Store],Rep[Cache],Rep[Cache]) => Rep[(Set[(Value,Store)],Cache)] = {
      case (arg, σ, in, out) =>
        val α = alloc(σ, x)
        ev(e)(ρ + (unit(x) → α))(σ ⊔ Map(α → arg))(in)(out)
    }; Set[AbsValue](emit_compiled_clo(f, λ, ρ)) }
\end{lstlisting}

The @ap_clo@ method for function applications is also similarly mixing the two
previous versions.  We use the staged version of @lift_nd@ to lift the
next-stage set of functions into the monad stack.
For each closure @clo@ in the set, we generate a next-stage value, representing
the function application @clo(arg)@, by using @emit_ap_clo@.  Again, the method
@emit_ap_clo@ produces a current-stage representation of the future-stage
application result.  Finally, we reify the @out@ cache, store, and values,
which are all from the future-stage, back into the current-stage monadic value.
\begin{lstlisting}
  def emit_ap_clo(fun: Rep[AbsValue], arg: Rep[Value], σ: Rep[Store],
                  in: Rep[Cache], out: Rep[Cache]): Rep[(Set[ValSt], Cache)]
  def ap_clo(ev: Expr => Ans)(funs: Rep[Value], arg: Rep[Value]): Ans = for {
    σ   <- get_store;      clo <- lift_nd[AbsValue](funs)
    in  <- ask_in_cache;   out <- get_out_cache
    res <- lift_nd[(Set[ValSt], Cache)](Set(emit_ap_clo(clo, arg, σ, in, out)))
    _   <- put_out_cache(res._2);  vs <- lift_nd[ValSt](res._1);  _ <- put_store(vs._2)
  } yield vs._1
\end{lstlisting}

\paragraph{Staged Caching and Fixpoint Iteration} The fixed-point iteration
again relies on the two caches @in@ and @out@, which are both staged maps now.
Therefore, the query of whether the @in/out@ cache contains the current
configuration will produce next-stage Boolean values, i.e., of type
@Rep[Boolean]@, and the branching condition cannot be determined statically.
We have to generate code for the whole @if@ expression.
Figure \ref{fig:staged_coind_cache} shows the staged version of @fix@. The
variable @res@ represents the next-stage result, consisting of a next-stage
@if@ expression. The true branch simply returns a pair of the query result
form the @out@ cache, and the @out@ cache itself. The else branch constructs a
monadic value @m@ of type @Ans@ first, which evaluates @e@ under the new @out@
cache.  After, we use a similar technique that collapses the monadic
value @m@ to grounded values, by providing its desired environment and etc.
Finally, we have a current-stage representation of the future-stage values
@res@, and we reify the content of @res@ back into the current-stage monad stack.

\begin{figure}[t]
\centering
\begin{lstlisting}
  def fix_cache(e: Expr): Ans = for {
    ρ   <- ask_env;  σ <- get_store;  in <- ask_in_cache;  out <- get_out_cache
    cfg <- lift_nd[Config](Set((unit(e), ρ, σ)))
    res <- lift_nd[(Set[ValSt], Cache)](Set(
      // a next-stage value of type Rep[(Set[ValSt], Cache)] generated by if
      if (out.contains(cfg)) (out(cfg), out)
      else { val m: Ans = for {
               _ <- put_out_cache(out + (cfg → in.getOrElse(cfg, ⊥)))
               v <- eval(fix_cache)(e);  σ <- get_store;  _ <- update_out_cache(cfg, (v, σ))
             } yield v
             m(ρ)(σ)(in)(out) }))
    _ <- put_out_cache(res._2);  vs <- lift_nd(res._1);  _ <- put_store(vs._2)
  } yield vs._1
\end{lstlisting}
\vspace{-0.5em}
\caption{The staged co-inductive caching algorithm.}
\label{fig:staged_coind_cache}
\vspace{-1.5em}
\end{figure}

\subsection{A Little Bit of Code Generation, Again}
The code generation for compiled closures and function applications are similar
to their counterparts in the staged concrete interpreter. We have two IR nodes
implemented as case classes; they also take additional caches as arguments. We
elide the code generation part for these IR nodes.
\begin{lstlisting}
case class IRCompiledClo(f: (Rep[Value], Rep[Store], Rep[Cache], Rep[Cache])
                          => Rep[(Set[ValSt], Cache)], λ: Lam, ρ: Rep[Env]) extends Def[AbsValue]
case class IRApClo(clo: Rep[AbsValue], arg: Rep[Value], σ: Rep[Store],
                    in: Rep[Cache], out: Rep[Cache]) extends Def[(Set[(Value, Store)], Cache)]
\end{lstlisting}

\subsection{Optimizations} \label{staged_ds}

Our staging schema works by unfolding the interpreter over the abstract syntax
tree of the input program. In practice, however, it would suffer from code
explosion when analyzing (specializing) large programs, which increases
compile time. If we generate next-stage programs running on the JVM, such large
generated programs would also incur GC overhead at runtime. In this
section, we present optimizations that largely mitigate these issues. 
Implementing all of those optimizations do not need to change the
generic interpreter.

\paragraph{Specialized Data Structures}

In the staged interpreters, all instances of @Env@ and @Store@ are staged. The
data structures representing these components are treated as black-boxes, i.e.,
@Rep[Map]@, which means that the operations on a @Rep[Map]@ directly become
next-stage code, and we do not inspect any further inside. As we identified when
introducing the generic interface, the keys of an @Env@ are string-represented
identifiers in the program, which are completely known statically. This
observation gives us a chance to further specialize the data structures for environments.
For example, let us assume that the @Map[K, V]@ is implemented as a hash-map.
If all the keys of type @K@ are all known statically, then the indices for
those keys can also be computed statically. Thus, the specialized map would be
an array of type @Array[Rep[V]]@, whose elements are next-stage values, and the
size of the array is known statically as the program has finite number of
identifiers. As result, an access to the map is staged into an access to the array
with the pre-computed index.

Particularly, if we are specializing a monovariant analysis, the address space
is equivalent to the set of all identifiers in the program. Utilizing this fact, the
result of accesses to the environment can be computed statically and we may
generate addresses directly during staging. After which, the store can be
specialized as an array by using the way mentioned above.

\paragraph{Selective Caching} We observe that the two-fold co-inductive
caching is used for every recursive call in our abstract interpreter. But this
is unnecessary and redundant when generating code for atomic
expressions such as literals or variables, because they always terminate.
Borrowing the idea from the partition of expressions used in A-Normal Form
$\lambda$-calculus \cite{Flanagan:1993:ECC:155090.155113}, we can use a
selective caching algorithm that does not generate caching code for atomic
expressions:
\begin{lstlisting}
  def fix_select: Expr => Ans = e => e match {
    case Lit(_) | Var(_) | Lam(_, _) => eval(fix_select)(e)
    case _ => fix_cache(e)
  }
\end{lstlisting}

\paragraph{Partially-static Data}
Our treatment of binding-times is coarse-grained: @Expr@s is static, the rest
of the components are all dynamic. But this is not always true, because the
static data have to be used somewhere with the dynamic operations.
Partial-static data is a way to improve binding-times and optimize the
generated code.
For example, to fold a static singleton set (often appears in @SetT@), e.g.,
@Set(x).foldLeft(init)(f)@ where @x@ and @init@ are staged values, a naive code
generator would faithfully generate code that applies @foldLeft@ with the set
and function @f@. But we can also utilize algebraic properties of @foldLeft@
to generate cheaper code, e.g., @Set(x).foldLeft(init)(f) = f(init, x)@. Since the
function @f@ is known at the current stage, we completely eliminate the fold
operation and function application. We apply several rewritings enabled by
partial-static patterns, such as for @Set@, @Map@, and @Tuple@. This
optimization greatly reduces the size of residual programs.

\section{Discussion} \label{discussion}

We have gradually presented the confluence of specialization and abstraction of
concrete interpreters from an operational perspective.  In this section, we
review and summarize our recipe to achieve the staged abstract interpreter and
discuss correctness issues and different design choices.

\subsection{Summarizing the Approach}

We summarize our approach to staging an abstract interpreter as follows:

\begin{itemize}
  \item First, we construct a generic interpreter that abstracts over binding times,
    value domains, and primitive operations. In this paper, the generic interpreter
    is implemented in monadic style; therefore, the semantics can be encapsulated
    into monads.
  \item Then, we implement an unstaged abstract interpreter modularly using the
    appropriate monad stack. This step has been explored in the previous
    literatures.
  \item Finally, we replace the monad stack with a staged monad stack, and
    refactor related primitive operations.  Such staged monads operate on
    staged data types, i.e., next-stage values.
\end{itemize}

Monadic interpreters are known to be able to decouple the interpretation
procedure and the underlying semantics. The key insight in this paper is that
by making the monadic interpreter stage polymorphic
\cite{Ofenbeck:2017:SGP:3136040.3136060, Amin:2017:CTI:3177123.3158140}, the
abstract interpreter can be extended to generate efficient code. The
underlying semantics and staging are two orthogonal dimensions. It is important
to note that the computation encapsulated by the monads are not staged: only
data (such as sets and maps) are staged. All the monadic computation, i.e.,
functions passed to the monadic bind operation @flatMap@, are statically known.
This is why we can eliminate the monadic layer and its associated overhead.

\paragraph{What has been eliminated?} In the generated code, all
primitive operations (such as @eval@, @fix@, @ap_clo@, etc.) and monadic
operations (such as @flatMap@ and @map@) are eliminated. The residual program
consists of statements and expressions that purely manipulate the environment,
store, and two caches, whose underlying representations are all @Map[K,V]@.
We also have several operations on tuples and lists, which are residualized from
the internal code fragments of the monads. 

\subsection{Correctness}

Soundness is the central concern of static analysis, and as such, is vital for
prospective users. Our approach does not touch the soundness of the analysis,
i.e, if the unstaged one does not produces false negative result, the staged
one also does not. This soundness preservation indeed relies on the correctness
of the staged implementations and of the underlying MSP system. We now briefly
examine how this is achieved. Note that the rationale listed
here is based on empirical evidence; a rigorous proof for soundness
preservation of our approach remains an open challenge.

\begin{itemize}
  \item We usually assume the underlying MSP system preserves the meaning
    during staging, i.e., the generated program and original program produce
    the same result given the same arguments.
    In this paper, we use the LMS framework, which 1) checks binding-times by
    checking types which is enabled by the Scala compiler, and 2) generates
    code in A-Normal form \cite{Flanagan:1993:ECC:155090.155113} that preserves
    the execution order within a stage \cite{DBLP:conf/birthday/Rompf16}.
    However, in general, it is possible to subvert these guarantees by using
    unsafe features, such as casts.
  \item As shown in the previous sections, the generic interpreter is untouched
    and shared by the four artifacts.  This allows the programmer to
    check the correctness of the implementation of staged monads modularly.
   % We only instantiate binding-time annotations and the monadic types. 
  \item We build the staged monads and staged data structures in a
    correct-by-construct way that directly corresponds to their unstaged
    versions. For instance, the staged data structures we use here are simply
    black boxes that wrap the data structures in Scala library.
  \item In our experiments, the staged abstract interpreter produces the same
    result with the unstaged one on all benchmark programs we tested.
\end{itemize}

\subsection{Alternatives}

\paragraph{Monadic-style vs Direct-style} We use a monadic interpreter
throughout the paper, but it is not necessary for staging. One can inline the
monadic operations and obtain an abstract interpreter in continuation-passing
style, or even translate back to a direct-style that may use explicit
side-effects such as mutations. In either case, we can still apply the staging
idea to the abstract interpreter and remove the interpretation overhead.
However, monads allow the staged abstract interpreter to be implemented in a
modular and extensible way.

\paragraph{Big-step vs Small-step}

We have implemented a big-step, compositional abstract interpreter in monadic
style, where \textit{compositional} means that every recursive call of @eval@ %our abstract
is applied to the proper substructures of the current syntactic
parameters \cite{10.1007/3-540-61580-6_11}. This compositionality ensures that
specialization can be done by unfolding, as well as guarantees the termination
of the specialization procedure. It is also possible to specialize a small-step
operational abstract semantics through abstract compilation
\cite{Boucher:1996:ACN:647473.727587} -- as
\citet{Johnson:2013:OAA:2500365.2500604} presented for
optimizing Abstract Abstract Machines. However, the generated abstract
bytecode still requires another small-step abstract machine to execute, which is
an additional engineering effort.
