\section{From Abstract Interpreters to Staged Abstract Interpreters} \label{sai}

In the previous sections, we have seen an unstaged abstract interpreter and a
staged concrete interpreter, now we begin describing the implementation of their
confluence -- a staged abstract interpreter.
Unsurprisingly, the staged abstract interpreter in this section has the same
abstract semantics with the unstaged version from Section~\ref{unstaged_abs}.
The approach from unstaged to staged abstract interpreter is modular,
principled, and does not sacrifice soundness or precision. The designer of the
analyzer has no need to rewrite the analysis. We first present the staged
lattices and staged monads, and then the staged version of primitive operations,
especially @close@, @ap_clo@ and @fix@, at last, we discuss several optimizations
and our design.

\subsection{Staged Lattices}

In Section \ref{stagedpoly_lat}, we exploited the higher-kinded type @R@ to
achieve stage polymorphism. Now we instantiate the type @R@ to @Rep@ and
still use power sets as an example to present its staged version.

\begin{lstlisting}
  trait RepLattice[S] extends Lattice[S, Rep]
  def RepSetLattice[T]: RepLattice[Set[T]] = new RepLattice[Set[T]] {
    lazy val ⊥: Rep[Set[T]] = Set[T]()
    lazy val ⊤: Set[T] = error("No representation for ⊤")
    def ⊑(l1: Rep[Set[T]], l2: Rep[Set[T]]): Rep[Boolean] = l1 subsetOf  l2
    def ⊔(l1: Rep[Set[T]], l2: Rep[Set[T]]): Rep[Set[T]]  = l1 union     l2
    def ⊓(l1: Rep[Set[T]], l2: Rep[Set[T]]): Rep[Set[T]]  = l1 intersect l2
  }
\end{lstlisting}

As we can see, the @RepSetLattice@ is an instance of @RepLattice@, which is also
an abstraction of the underlying data structure. The lattice operations
eventually are delegated to @subsetOf@, @union@ and @intersect@ on
@Rep[Set[T]]@. We even don't have to change the implementation code, but just
the types – from @Set[T]@ to @Rep[Set[T]]@. LMS library provides an implicit
conversion lifting to next-stage values. In the code generation part, these
operations emit their corresponding next-stage code. Again, other lattice
structures such as products and maps are implemented in a similar way.

\subsection{Staged Abstract Semantics}

Now we implement the staged abstract semantics, where @R@ is instantiated as
@Rep@, and the abstract components are shared from the unstaged version.

\paragraph{Staged Monads for Abstract Interpretation}
We use the same monad stack structure as in the unstaged abstract interpreter,
but replacing \textit{all} the transformers to their staged version. The
following code shows this change. For the readability, we squeeze the three
inner transformers into a single monad @RepSetReaderStateM[A, B, C]@, where @A@
and @B@ are both @Cache@ when used in @AnsM@. Now the result type is a pair of
two staged value: @Rep[Set[(Value, Store)]]@ and @Rep[Cache]@.

\begin{lstlisting}
  trait StagedAbstractSemantics extends AbstractComponents {
    type R[T] = Rep[T]
    type RepSetReaderStateM[A, B, C] = RepSetT[RepReaderT[RepStateT[RepIdM, A, ?], B, ?], C]
    type AnsM[T] = RepReaderT[RepStateT[RepSetReaderStateM[Cache, Cache, ?], Store, ?], Env, T]
    type Result = (Rep[Set[(Value, Store)]], Rep[Cache])
    ...
  }
\end{lstlisting}

Since our monad stack is uniformly using staged data, so the staged @SetT@ now
stores a staged value of type @Rep[Set[A]]@, inside of another staged monad @M@.

\begin{lstlisting}
  case class SetT[M[_]: RepMonad, A](run: M[Set[A]]) {
    def flatMap[B: Manifest](f: Rep[A] => SetT[M, B]): SetT[M, B] = ...
  }
\end{lstlisting}

\paragraph{Primitive Operations} We keep our road map that focuses on functions
and applications. The @close@ method now is a rough mixing of the staged
concrete and unstaged abstract version: we have a current-stage function @f@,
that takes four next-stage values, @in@ and @out@ additionally; inside of @f@,
we collapse the @Ans@ monad to values of type @Result@ by providing the desired
arguments, i.e., the new environment, new store, and caches. The collapse
happens at current-stage, so the call of @ev@ is unfolded after staging. At
last, we generate a singleton set containing the compiled closure
@emit_compiled_clo(f)@, represented by an IR node in LMS.

\begin{lstlisting}
  def emit_compiled_clo(f: (Rep[Value], Rep[Store], Rep[Cache], Rep[Cache])
                           => Rep[(Set[(Value,Store)], Cache)]): Rep[AbsValue]
  def close(ev: Expr => Ans)(λ: Lam, ρ: Rep[Env]): Rep[Value] = {
    val Lam(x, e) = λ
    val f: (Rep[Value],Rep[Store],Rep[Cache],Rep[Cache]) => Rep[(Set[(Value,Store)],Cache)] = {
      case (arg, σ, in, out) =>
        val α = alloc(σ, x); val ρ_* = ρ + (unit(x) → α); val σ_* = σ ⊔ Map(α → arg)
        ev(e)(ρ_*)(σ_*)(in)(out)
    }; Set[AbsValue](emit_compiled_clo(f))
  }
\end{lstlisting}

The @ap_clo@ method is also similar: we use the staged version of @lift_nd@ to
lift the set of closures into the monad stack, and generate a next-stage value
using @emit_ap_clo@, which represents the result of application from future.
Finally, we put the future @out@ cache, store, and values back into the
current-stage monads. The @emit_ap_clo@ method takes the target closure @clo@,
argument @rand@, store, and caches.

\begin{lstlisting}
  def lift_nd[T](vs: Rep[Set[T]]): AnsM[T]
  def emit_ap_clo(rator: Rep[AbsValue], rand: Rep[Value], σ: Rep[Store],
                  in: Rep[Cache], out: Rep[Cache]): Rep[(Set[(Value, Store)], Cache)]
  def ap_clo(ev: Expr => Ans)(rator: Rep[Value], rand: Rep[Value]): Ans = for {
    σ <- get_store;     clo <- lift_nd[AbsValue](rator)
    in <- ask_in_cache; out <- get_out_cache
    val res: Rep[(Set[(Value, Store)], Cache)] = emit_ap_clo(clo, rand, σ, in, out)
    _ <- put_out_cache(res._2)
    vs <- lift_nd[(Value, Store)](res._1)
    _ <- put_store(vs._2)
  } yield vs._1
\end{lstlisting}

\paragraph{Staged Caching and Fixpoint Iteration} 

Our fixed-point iteration again relies on the two caches @in@ and @out@, which are
both staged values now. Therefore, the query of whether the @out@ cache contains
the current configuration produces a next-stage Boolean value, i.e.,
@Rep[Boolean]@. Consequently, the branching cannot be determined statically --
we need to generate code for @if@. Figure \ref{fig:staged_coind_cache} shows the
staged version of @fix@.

\vspace{-1em}
\begin{figure}[h!]
  \centering
\begin{lstlisting}
  def fix(ev: (Expr => Ans) => (Expr => Ans)): Expr => Ans = e => for {
    ρ <- ask_env; σ <- get_store; in <- ask_in_cache; out <- get_out_cache
    val cfg: Rep[Config] = (unit(e), ρ, σ)
    val res: Rep[(Set[(Value, Store)], Cache)] =
      if (out.contains(cfg)) (out(cfg), out) // a next-stage if expression
      else { val m: Ans = for {
               _ <- put_out_cache(out + (cfg → in.getOrElse(cfg, ⊥)))
               v <- ev(fix(ev))(e)
               σ <- get_store
               _ <- update_out_cache(cfg, (v, σ))
             } yield v
             m(ρ)(σ)(in)(out) }
    _ <- put_out_cache(res._2); vs <- lift_nd(res._1); _ <- put_store(vs._2)
  } yield vs._1
\end{lstlisting}
\vspace{-1em}
\caption{The staged co-inductive caching algorithm.}
\label{fig:staged_coind_cache}
\end{figure}
\vspace{-1em}

The @res@ variable is a next-stage result, which consists of a next-stage @if@
expression. The true branch simply returns a pair of queried result form the
@out@ cache and the @out@ cache. The else branch constructs a monad @m@ first,
which evaluates @e@ under the new @out@ cache. After which, we collapse the
monad @m@ with desired arguments. Finally, we obtain a placeholder @res@ since
it is a next-stage value, and put the content of @res@ back into the monad
stack.

\paragraph{Code Generation} The IR nodes for compiled closures and
applications are defined as follows:

\begin{lstlisting}
  case class IRCompiledClo(f: (Rep[Value], Rep[Store], Rep[Cache], Rep[Cache])
                           => Rep[(Set[(Value, Store)], Cache)], λ: Lam, ρ: Rep[Env]) extends Def[AbsValue]
  case class IRApClo(clo: Rep[AbsValue], arg: Rep[Value], σ: Rep[Store],
                     in: Rep[Cache], out: Rep[Cache]) extends Def[(Set[(Value, Store)], Cache)]
\end{lstlisting}

The code generator for these two IR nodes is similar to staged concrete
interpreter, which generates a next-stage @CompiledClo@ object and a next-stage
function call to @f@, respectively.
                   
\subsection{Optimizations} \label{staged_ds}

\iffalse
Revision: Solving Practical Challenges.
theoretically, all the things should work nicely. Unfolding the interpreter over the AST.
But the generated code is blowed up. For example .... This should not affect the correctness, 
but poses burden on the MSP system (ie LMS) and the next stage compiler/runtime (ie, scalac and JVM).
1) LMS becomes slower since a large IR graph is contructed during the staging.
2) Scalac becomes slower when reading a such large source code.
JVM has certain limitation on the size of a single method.

TODO: can we formulate selective caching as a partially-static data law.
TODO: lambda lifting for if
\fi

Our staging schema works by unfolding the interpreter over the input AST. In
practice, it would suffer from code explosion when analyzing (specializing) large
programs, which increases the compile time.
If we generate next stage programs running on JVM, such large generated program
would also cause runtime GC overhead. In this section, we present several
optimizations that largely mitigate these issues. For all that, implementing
these optimizations do not need to change the generic interpreter.

\paragraph{Specialized Data Structures}

Although every component except @Expr@s are staged, we treat the data
structures such as @Map@s as black-boxes, which means the operations on a @Map@
directly become code in the next stage, but we don't inspect any further inside.
As we identified when introducing the generic interface, the keys of @Env@s are
identifiers (i.e., strings) in the program, which are completely known
statically. This leaves us a chance to further specialize the data structures.
For example, let's assume that the @Map[K, V]@ is implemented as a hash map. If
the keys of type @K@ are all known statically, then the indices also can be
computed statically. Thus the specialized map would be an array of type
@Array[Rep[V]]@, whose elements are next-stage values; the size of the array is
also known statically. An access to the map is translated into an access to the
array with pre-determined index during staging.

Particularly, if we are specializing a monovariant analysis, the address space
is equivalent to the set of identifiers in the program. Then the accesses to the
environment can be completely determined statically and generates the addresses
directly. After which, the stores can be specialized as arrays of @Rep[Value]@
elements.

\paragraph{Selective Caching} It is observed that the two-fold co-inductive
caching is used for every recursive call of our abstract interpreter. But this
is not necessary and even very expensive when generating code for atomic
expressions such as literals or variables, as they always terminate. Borrowing
the idea from the partition of A-Normal Form \cite{Flanagan:1993:ECC:155090.155113},
we may use a selective caching algorithm:

\begin{lstlisting}
  def fix_select: Expr => Ans = e => e match {
    case Lit(_) | Var(_) | Lam(_, _) => eval(fix_select)(e)
    case _ => fix_cache(e)
  }
\end{lstlisting}

\paragraph{Partially-static Data}

Our treatment to binding-times is coarse-grained: @Expr@s is static, the rest of
the world are all dynamic. But this is not always true since the static data
has to be used somewhere with the dynamic operation. Partial-static data is a
way to improve binding-times and optimize the generated code.

For example, to fold a singleton set (often appears in @SetT@), e.g.,
@Set(x).foldLeft(init)(f)@ where @x@ and @init@ are staged values, a naive code
generator would faithfully apply @foldLeft@ to the set with an anonymous
function. But we can also utilize the algebraic property of @foldLeft@ to
generate cheaper code: @Set(x).foldLeft(init)(f) = f(init, x)@. Since the
function @f@ is known at the current stage, we completely eliminate the fold
operation and function application. We apply several rewritings enabled by
partial-static data structures, such as @Set@ and @Map@, which greatly reduces
the size of residual programs.

%\paragraph{Heterogeneous Staging} The generated program is in A-Normal form. \todo{TODO}

\section{Discussion}

We have gradually presented the confluence of specialization and abstraction of
concrete interpreters. In this section, we review and summarize our recipe
to achieve the staged abstract interpreter, discuss the correctness issue
and different design choices.

\subsection{Summarizing the Approach}

\todo{1. construct generic monadic interpreter that abstract over binding-time,
  primtive operations, return type.
  2. implement unstaged abstract interpreter with monads, ie. using approriate
  monad stack
  3. if we want to generate code, we just need to change the monad stack.
  the data wrapped are staged.
  we show readerT monad, stateT monad, fused monad with nondeterminism. }

\paragraph{What has been eliminated?} In the generated code, all the
primitive operations (such as @eval@, @fix@, @ap_clo@, etc.) and monadic
operations (such as @flatMap@ and @map@) are eliminated. The residual program
consists of statements and expressions that purely manipulate the environment,
store, and two caches, whose underlying representations are all @Rep[Map[K,V]]@. We
also have several operations on tuples and lists, which are residualized from
the use of monads internally.

\todo{may be show an small example?}

\subsection{Correctness}

As the central concern of static analysis, the soundness is important for
every user who would like to use it. Here we briefly summarize that how our
approach preserve the corretness and soundness of the analysis, under the
assumption that the unstaged one is sound. It is worth noting that the rationale
listed here are based on empirical evidence, a rigorous proof of soundness
perservation of the staged abstract interpreters is still an open question.

\begin{itemize}
  \item We usually assume the underlying MSP system achieves cross-stage
    persistent, i.e., the generated program does not change the observational
    behaviours of the original program. In the case of this paper, we use the
    LMS framework, which 1) relies on the type checking facilities of Scala, and
    2) generates code in A-Normal form that preserves the
    execution order within a stage \todo{cite WF16}. However, in general, it is
    possible to subvert these guarantees by using unsafe features, such as
    casts.
  \item As shown in the previous section, the generic interpreter is untouched
    and shared by the four artifacts. We only instantiate binding-time
    annotations and the monadic types. This allows the programmer to easily
    check the correctness of implementation of staged monads modularlly.
  \item We construct the staged monads and staged data structures in a
    correct-by-construct way that directly corresponds to their unstaged
    version. For instance, the staged data structure we used here are simply
    a black-box that wraps the Scala's library data structures (in the next
    stage).
  \item In our experiments, the staged abstract interpreter produces the same
    result with the unstaged one on all benchmark programs we tested.
\end{itemize}

\subsection{Design Choices}

\paragraph{Are Monads Necessary?} No. One can always inline the monads and
obtain an abstract interpreter in continuation-passing style, or simply use
explicit side-effects such mutation to implement the artifact with the same
abstract semantics. In either case, we can still apply the staging schema to
the abstract interpreter. As evidence, we provide a staged abstract
interpreter written in direct-style in the accompanying artifact.

\paragraph{Big-step vs Small-step}

What we implemented is a big-step, compositional abstract interpreter in monadic
style, where \textit{compositional} means that every recursive call of our abstract
interpreter is applied to proper substructures of the current syntactic
parameters \cite{10.1007/3-540-61580-6_11}. This compositionality ensures that
specialization can be done by unfolding, as well as guarantees the termination
of specialization procedure. It is also possible to specialize small-step
operational abstract semantics through abstract compilation
\cite{Boucher:1996:ACN:647473.727587} -- as
\citet{Johnson:2013:OAA:2500365.2500604} implemented it for
optimizing Abstract Abstract Machines. However, the generated abstract
byte-code still requires another small-step abstract machine to execute, which is
an additional engineering effort.
