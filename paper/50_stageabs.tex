\section{From Abstract Interpreters to Staged Abstract Interpreters} \label{sai}

In the previous sections, we have seen an unstaged abstract interpreter and a
staged concrete interpreter, now we begin describing the implementation of their
confluence -- a staged abstract interpreter.
Unsurprisingly, the staged abstract interpreter in this section has the same
abstract semantics with the unstaged version from Section~\ref{unstaged_abs}.
The approach from unstaged to staged abstract interpreter is modular,
and does not sacrifice soundness or precision. The designer of the
analyzer therefore has no need to rewrite the analysis. We first present the
staged lattices and staged monads, and then the staged version of primitive
operations, especially @close@, @ap_clo@ and @fix@. At last, we discuss several
optimizations and summarize our approach.

\subsection{Staged Lattices}

In Section \ref{stagedpoly_lat}, we exploited the higher-kinded type @R@ to
achieve stage polymorphism. Now we instantiate the type @R@ to @Rep@ and
still use power sets as an example to present its staged version.
\begin{lstlisting}[escapechar=!]
  trait RepLattice[S] extends Lattice[S, Rep]
  def RepSetLattice[T]: RepLattice[Set[T]] = new RepLattice[Set[T]] {
    lazy val ⊥: !\hl{Rep[Set[T]]}! = Set[T]()
    lazy val ⊤: !\hl{Rep[Set[T]]}! = error("No representation for ⊤")
    def ⊑(l1: !\hl{Rep[Set[T]]}!, l2: !\hl{Rep[Set[T]]}!): !\hl{Rep[Boolean]}! = l1 subsetOf  l2
    def ⊔(l1: !\hl{Rep[Set[T]]}!, l2: !\hl{Rep[Set[T]]}!): !\hl{Rep[Set[T]]}!  = l1 union     l2
    def ⊓(l1: !\hl{Rep[Set[T]]}!, l2: !\hl{Rep[Set[T]]}!): !\hl{Rep[Set[T]]}!  = l1 intersect l2
  }
\end{lstlisting}

The trait @RepLattice@ is obtained by specializing @Lattice@ with @Rep@.
The @RepSetLattice@ is an instance of @RepLattice@, where the lattice operations
eventually are delegated to operations on a staged set data type, including
@subsetOf@, @union@ and @intersect@. We even do not have to change the
implementation code, but just the types -- from @Set[T]@ to @Rep[Set[T]]@, shown
in light gray code. The LMS library provides an implicit conversion that lifts a
current-stage set value to next-stage values. In the code generation part, these
operations emit their corresponding next-stage code. Again, other lattice
structures such as products and maps are implemented in a similar way.

\subsection{Staged Abstract Semantics}

Now we can implement the staged abstract semantics, where @R@ is instantiated as
@Rep@, and the abstract components are reused from the unstaged version.

\paragraph{Staged Monads for Abstract Interpretation}
We use the same monad stack structure from the unstaged abstract interpreter,
but replacing \textit{all} the transformers to their staged version. The
following code shows this change. For readability, we squeeze the three inner
transformers (@IdM@ omitted) into a single monad @RepSetReaderStateM[A, B, C]@,
where @A@ is the type parameter for reader effect and @B@ for the state effects.
In our monad stack scheme, they are both instantiated with @Cache@. Now the
result type is a pair of two staged value: @Rep[Set[(Value, Store)]]@ and
@Rep[Cache]@.
\begin{lstlisting}
  trait StagedAbstractSemantics extends AbstractComponents {
    type R[T] = Rep[T]
    type AnsM[T] = RepReaderT[RepStateT[RepSetReaderStateM[Cache, Cache, ?], Store, ?], Env, T]
    type Result = (Rep[Set[(Value, Store)]], Rep[Cache])
    ...
  }
\end{lstlisting}

\iffalse
Since our monad stack is uniformly using staged data, the staged @SetT@ now
stores a staged value of type @Rep[Set[A]]@, inside of another staged monad @M@.
\begin{lstlisting}
  case class SetT[M[_]: RepMonad, A](run: M[Set[A]]) {
    def flatMap[B: Manifest](f: Rep[A] => SetT[M, B]): SetT[M, B] = ...
  }
\end{lstlisting}
\fi

\paragraph{Primitive Operations} When deriving the staged concrete interpreter
from the unstaged counterpart, we notice that recursive calls of @ev@ on the
lambda body are shifted from @ap_clo@ to @close@. In other word, we need to
eagerly specialize the interpreter and generate code every time when we see a
$\lambda$ term, instead of lazily when we actually need to apply a function.
With no surprise, it is similar in the staged abstract interpreter,
additionally, we need to handle nondeterminism. So we keep our track and focus
on discussing closure representations and function applications for the staged
version.

The @close@ method now is a mixing of the \textit{staged} concrete version and
unstaged \textit{abstract} version: we build a current-stage function @f@, which
takes four next-stage values, including the argument and latest store, and
@in/out@ caches additionally. Inside of @f@, we collapse the @Ans@ monad to
grounded values of type @Result@ by providing the desired arguments, i.e., the
new environment, new store, and caches. The collapsing of monad happens at
current-stage, so the call of @ev@ is unfolded through staging. Finally, we
generate a singleton set containing the compiled next-stage closure
@emit_compiled_clo(f)@, represented by an IR node in LMS.
\begin{lstlisting}
  def emit_compiled_clo(f: (Rep[Value], Rep[Store], Rep[Cache], Rep[Cache])
                           => Rep[(Set[(Value,Store)], Cache)], λ: Lam, ρ: Exp[Env]): Rep[AbsValue]
  def close(ev: Expr => Ans)(λ: Lam, ρ: Rep[Env]): Rep[Value] = {
    val Lam(x, e) = λ
    val f: (Rep[Value],Rep[Store],Rep[Cache],Rep[Cache]) => Rep[(Set[(Value,Store)],Cache)] = {
      case (arg, σ, in, out) =>
        val α = alloc(σ, x)
        ev(e)(ρ + (unit(x) → α))(σ ⊔ Map(α → arg))(in)(out)
    }; Set[AbsValue](emit_compiled_clo(f, λ, ρ))
  }
\end{lstlisting}

The @ap_clo@ method is also similar: we use the staged version of @lift_nd@ to
lift the next-stage set of closures into the monad stack, and generate a
next-stage value for each closure @clo@ in the set, by using @emit_ap_clo@. The
method @emit_ap_clo@ generates the current-stage representation of the
future-stage result of application.
Finally, we put the future @out@ cache, store, and values back into
the current-stage monads.
\begin{lstlisting}
  def emit_ap_clo(rator: Rep[AbsValue], rand: Rep[Value], σ: Rep[Store],
                  in: Rep[Cache], out: Rep[Cache]): Rep[(Set[ValSt], Cache)]
  def ap_clo(ev: Expr => Ans)(rator: Rep[Value], rand: Rep[Value]): Ans = for {
    σ   <- get_store;      clo <- lift_nd[AbsValue](rator)
    in  <- ask_in_cache;   out <- get_out_cache
    res <- lift_nd[(Set[ValSt], Cache)](Set(emit_ap_clo(clo, arg, σ, in, out)))
    _   <- put_out_cache(res._2)
    vs  <- lift_nd[ValSt](res._1)
    _   <- put_store(vs._2)
  } yield vs._1
\end{lstlisting}

% Monad is the bridge that connects future-stage values

\begin{figure}[h!]
  \centering
\begin{lstlisting}
  def fix_cache(e: Expr): Ans = for {
    ρ   <- ask_env;  σ <- get_store;  in <- ask_in_cache;  out <- get_out_cache
    cfg <- lift_nd[Config](Set((unit(e), ρ, σ)))
    res <- lift_nd[(Set[ValSt], Cache)](Set(
      // a next-stage value of type Rep[(Set[ValSt], Cache)] generated by if
      if (out.contains(cfg)) (repMapToMapOps(out).apply(cfg).toList, out)
      else { val m: Ans = for {
               _ <- put_out_cache(out + (cfg → in.getOrElse(cfg, ⊥)))
               v <- eval(fix_cache)(e)
               σ <- get_store
               _ <- update_out_cache(cfg, (v, σ))
             } yield v
             m(ρ)(σ)(in)(out) }))
    _ <- put_out_cache(res._2);  vs <- lift_nd(res._1);  _ <- put_store(vs._2)
  } yield vs._1
\end{lstlisting}
\caption{The staged co-inductive caching algorithm.}
\label{fig:staged_coind_cache}
\end{figure}

\paragraph{Staged Caching and Fixpoint Iteration} Our fixed-point iteration
again relies on the two caches @in@ and @out@, which are both staged maps now.
Therefore, the query of whether the @out@ cache contains the current
configuration will produce a next-stage Boolean value, i.e., of type
@Rep[Boolean]@. Consequently, the branching condition cannot be determined
statically -- we need to generate code for @if@. Figure
\ref{fig:staged_coind_cache} shows the staged version of @fix@. The
monadic-binded variable @res@ is a next-stage result, consisting of a next-stage
@if@ expression. The true branch simply returns a pair of queried result form
the @out@ cache and the @out@ cache itself. The else branch constructs a monadic
value @m@ of type @Ans@ first, which evaluates @e@ under the new @out@ cache.
After which, we use the similar technique that collapses the monad @m@ to
grounded values by providing its desired environment and etc. Finally, we have a
current-stage representation of the future-stage values @res@, and put the
content of @res@ back into the monad stack.

\subsection{A Little Bit of Code Generation, Again}
The code generation for compiled closures and function applications are similar
to the staged concrete interpreter. We have two the IR node as case classes, they
take additional caches as arguments. We also elide how to generate next-stage Scala
code for them.
\begin{lstlisting}
case class IRCompiledClo(f: (Rep[Value], Rep[Store], Rep[Cache], Rep[Cache])
                          => Rep[(Set[ValSt], Cache)], λ: Lam, ρ: Rep[Env]) extends Def[AbsValue]
case class IRApClo(clo: Rep[AbsValue], arg: Rep[Value], σ: Rep[Store],
                    in: Rep[Cache], out: Rep[Cache]) extends Def[(Set[(Value, Store)], Cache)]
\end{lstlisting}

\subsection{Optimizations} \label{staged_ds}

\iffalse
Revision: Solving Practical Challenges.
theoretically, all the things should work nicely. Unfolding the interpreter over the AST.
But the generated code is blowed up. For example .... This should not affect the correctness,
but poses burden on the MSP system (ie LMS) and the next stage compiler/runtime (ie, scalac and JVM).
1) LMS becomes slower since a large IR graph is contructed during the staging.
2) Scalac becomes slower when reading a such large source code.
JVM has certain limitation on the size of a single method.

TODO: can we formulate selective caching as a partially-static data law.
TODO: lambda lifting for if
\fi

Our staging schema works by unfolding the interpreter over the input AST. In
practice, however, it would suffer from code explosion when analyzing
(specializing) large programs, which increases the compile time. If we generate
next stage programs running on JVM, such large generated program would also
cause runtime GC overhead. In this section, we present several optimizations
that largely mitigate these issues. For all that, implementing these
optimizations do not need to change the generic interpreter.

\paragraph{Specialized Data Structures}

Although every component except @Expr@s are staged, we treat the data
structures such as @Map@s as black-boxes, which means the operations on a @Map@
directly become code in the next stage, but we don't inspect any further inside.
As we identified when introducing the generic interface, the keys of @Env@s are
identifiers (i.e., strings) in the program, which are completely known
statically. This leaves us a chance to further specialize the data structures.
For example, let's assume that the @Map[K, V]@ is implemented as a hash map. If
the keys of type @K@ are all known statically, then the indices also can be
computed statically. Thus the specialized map would be an array of type
@Array[Rep[V]]@, whose elements are next-stage values; the size of the array is
also known statically. An access to the map is translated into an access to the
array with pre-determined index during staging.

Particularly, if we are specializing a monovariant analysis, the address space
is equivalent to the set of identifiers in the program. Then the accesses to the
environment can be completely determined statically and generates the addresses
directly. After which, the stores can be specialized as arrays of @Rep[Value]@
elements.

\paragraph{Selective Caching} It is observed that the two-fold co-inductive
caching is used for every recursive call of our abstract interpreter. But this
is not necessary and even very expensive when generating code for atomic
expressions such as literals or variables, because they always terminate.
Borrowing the idea from the partition of expressions used in A-Normal Form
\cite{Flanagan:1993:ECC:155090.155113}, we may use a selective caching
algorithm that does not generate caching for atomic expressions:
\begin{lstlisting}
  def fix_select: Expr => Ans = e => e match {
    case Lit(_) | Var(_) | Lam(_, _) => eval(fix_select)(e)
    case _ => fix_cache(e)
  }
\end{lstlisting}

\paragraph{Partially-static Data}

Our treatment to binding-times is coarse-grained: @Expr@s is static, the rest of
the world are all dynamic. But this is not always true since the static data
has to be used somewhere with the dynamic operation. Partial-static data is a
way to improve binding-times and optimize the generated code.

For example, to fold a singleton set (often appears in @SetT@), e.g.,
@Set(x).foldLeft(init)(f)@ where @x@ and @init@ are staged values, a naive code
generator would faithfully apply @foldLeft@ to the set with an anonymous
function. But we can also utilize the algebraic property of @foldLeft@ to
generate cheaper code: @Set(x).foldLeft(init)(f) = f(init, x)@. Since the
function @f@ is known at the current stage, we completely eliminate the fold
operation and function application. We apply several rewritings enabled by
partial-static data structures, such as @Set@ and @Map@, which greatly reduces
the size of residual programs.

%\paragraph{Heterogeneous Staging} The generated program is in A-Normal form. \todo{TODO}

\section{Discussion}

We have gradually presented the confluence of specialization and abstraction of
concrete interpreters. In this section, we review and summarize our recipe
to achieve the staged abstract interpreter, discuss correctness issues,
and different design choices.

\subsection{Summarizing the Approach}

We summarize our approach to staging an abstract interpreter as follows:

\begin{itemize}
  \item Constructing a generic interpreter that abstracts over binding-time,
    value domains, and primitive operations. In this paper, the generic interpreter
    is implemented in monadic style; therefore, the semantics can be encapsulated
    into monads.
  \item By using an appropriate monad stack, we can implement a modular, unstaged,
    abstract interpreter. This step has been explored in the previous literature.
  \item Replacing the monad stack to staged monad stack, such monads operate on
    staged data types, i.e., next-stage values. We also need to reimplement several
    primitive operations on staged values, in a modular fashion.
\end{itemize}

Monadic interpreters are known to be able to decouple the interpretation process
and the underlying semantics. The key insight in this paper is that by making the
monadic interpreter binding-time polymorphic, it can be extended for generating
efficient code in a staging framework. The underlying semantics and staging are
two orthogonal dimensions. It is important to note that the computation
encapsulated by the monads are not staged; only data (such as sets and maps) are
staged. All the monadic computation, i.e., functions passed to the monadic bind
operation @flatMap@, are statically known. This is why we can eliminate the
monadic layer and its associated overhead.

\paragraph{What has been eliminated?} In the generated code, all the
primitive operations (such as @eval@, @fix@, @ap_clo@, etc.) and monadic
operations (such as @flatMap@ and @map@) are eliminated. The residual program
consists of statements and expressions that purely manipulate the environment,
store, and two caches, whose underlying representations are all @Rep[Map[K,V]]@. We
also have several operations on tuples and lists, which are residualized from
the use of monads internally.

%\todo{maybe show a small example?}

\subsection{Correctness}

Soundness is the central concern of static analysis, and as such, is vital
for prospective users. We now briefly examine how our
approach preserves the correctness and soundness of the analysis, under the
assumption that the unstaged one is sound. It is worth noting that the rationale
listed here is based on empirical evidence; a rigorous proof of soundness
preservation of the staged abstract interpreters remains an open question.

\begin{itemize}
  \item We usually assume the underlying MSP system achieves cross-stage
    persistence, i.e., the generated program does not change the observational
    behaviors of the original program. In the case of this paper, we use the
    LMS framework, which 1) relies on the type checking facilities of Scala, and
    2) generates code in A-Normal Form \cite{Flanagan:1993:ECC:155090.155113}
    that preserves the execution order within a stage \cite{DBLP:conf/birthday/Rompf16}.
    However, in general, it is possible to subvert these guarantees by
    using unsafe features, such as casts.
  \item As shown in the previous section, the generic interpreter is untouched
    and shared by the four artifacts. We only instantiate binding-time
    annotations and the monadic types. This allows the programmer to easily
    check the correctness of implementation of staged monads modularly.
  \item We construct the staged monads and staged data structures in a
    correct-by-construct way that directly corresponds to their unstaged
    version. For instance, the staged data structures we used here are simply
    black boxes that wrap the Scala's library data structures (in the next
    stage).
  \item In our experiments, the staged abstract interpreter produces the same
    result with the unstaged one on all benchmark programs we tested.
\end{itemize}

\subsection{Design Choices}

\paragraph{Are Monads Necessary?} No. One can always inline the monads and
obtain an abstract interpreter in continuation-passing style, or simply use
explicit side-effects such as mutation to implement the artifact with the same
abstract semantics. In either case, we can still apply the staging schema to
the abstract interpreter. As evidence, we provide a staged abstract
interpreter written in direct-style in the accompanying artifact.

\paragraph{Big-step vs Small-step}

We implemented a big-step, compositional abstract interpreter in monadic
style, where \textit{compositional} means that every recursive call of our abstract
interpreter is applied to proper substructures of the current syntactic
parameters \cite{10.1007/3-540-61580-6_11}. This compositionality ensures that
specialization can be done by unfolding, as well as guarantees the termination
of the specialization procedure. It is also possible to specialize small-step
operational abstract semantics through abstract compilation
\cite{Boucher:1996:ACN:647473.727587} -- as
\citet{Johnson:2013:OAA:2500365.2500604} implemented it for
optimizing Abstract Abstract Machines. However, the generated abstract
bytecode still requires another small-step abstract machine to execute, which is
an additional engineering effort.
