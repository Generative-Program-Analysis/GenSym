\section{From Abstract Interpreters to Staged Abstract Interpreters} \label{sai}

In the previous sections, we have seen an unstaged abstract interpreter and a
staged concrete interpreter, now we begin describing the implementation of their
confluence -- a staged abstract interpreter.
Unsurprisingly, the staged abstract interpreter in this section has the same
abstract semantics with the unstaged version from Section~\ref{unstaged_abs}.
The approach from unstaged to staged abstract interpreter is modular,
principled, and does not sacrifice soundness or precision. The designer of the
analyzer has no need to rewrite the analysis. We first present the staged
lattices and staged monads, and then the staged version of primitive operations,
especially @close@, @ap_clo@ and @fix@, at last we discuss several optimizations
and our design.

\subsection{Staged Lattices}

In Section \ref{stagedpoly_lat}, we exploited the higher-kinded type @R@ to
achieve stage polymorphism. Now we instantiate the type @R@ to @Rep@ and
still use power sets as an example to present its staged version.

\begin{lstlisting}
  trait RepLattice[S] extends Lattice[S, Rep]
  def RepSetLattice[T]: RepLattice[Set[T]] = new RepLattice[Set[T]] {
    lazy val ⊥: Rep[Set[T]] = Set[T]()
    lazy val ⊤: Set[T] = error("No representation for ⊤")
    def ⊑(l1: Rep[Set[T]], l2: Rep[Set[T]]): Rep[Boolean] = l1 subsetOf  l2
    def ⊔(l1: Rep[Set[T]], l2: Rep[Set[T]]): Rep[Set[T]]  = l1 union     l2
    def ⊓(l1: Rep[Set[T]], l2: Rep[Set[T]]): Rep[Set[T]]  = l1 intersect l2
  }
\end{lstlisting}

As we can see, the @RepSetLattice@ is an instance of @RepLattice@, which is also
an abstraction of the underlying data structure. The lattice operations
eventually are delegated to @subsetOf@, @union@ and @intersect@ on
@Rep[Set[T]]@. We even don't have to change the implementation code, but just
the types – from @Set[T]@ to @Rep[Set[T]]@. LMS library provides an implicit
conversion lifting to next-stage values. In the code generation part, these
operations emit their corresponding next-stage code. Again, other lattice
structures such as products and maps are implemented in a similar way.

\subsection{Staged Abstract Semantics}

Now we implement the staged abstract semantics, where @R@ is instantiated as
@Rep@, and the abstract components are shared from the unstaged version.

\paragraph{Staged Monads for Abstract Interpretation}
We use the same moand stack structure as in the unstaged abstract interpreter,
but replacing \textit{all} the transformers to their staged version. The
following code shows this change. For the readability, we squeeze the three
inner transformers into a single monad @RepListReaderStateM[A, B, C]@, where @A@
and @B@ are both @Cache@ when used in @AnsM@. Now the result type is a pair of
two staged value: @Rep[List[(Value, Store)]]@ and @Rep[Cache]@.

\begin{lstlisting}
  trait StagedAbstractSemantics extends AbstractComponents {
    type R[T] = Rep[T]
    type RepListReaderStateM[A, B, C] = RepListT[RepReaderT[RepStateT[RepIdM, A, ?], B, ?], C]
    type AnsM[T] = ReaderT[StateT[RepListReaderStateM[Cache, Cache, ?], Store, ?], Env, T]
    type Result = (Rep[List[(Value, Store)]], Rep[Cache])
    ...
  }
\end{lstlisting}

Since our monad stack is uniformly using staged data, so the staged @ListT@ now
stores a staged value of type @Rep[List[A]]@, inside of a staged monad @M@.
\todo{staged ListT implementation}

\begin{lstlisting}
  case class ListT[M[_]: RepMonad, A](run: M[List[A]]) {
    def flatMap[B: Manifest](f: Rep[A] => ListT[M, B]): ListT[M, B] = ...
  }
\end{lstlisting}

\paragraph{Primitive Operations} We keep our road map that focuses on functions
and applications. The @close@ method now is a rough mixing of the staged
concrete and unstaged abstract version: we have a current-stage function @f@,
that takes four next-stage values, @in@ and @out@ additionally; inside of @f@,
we collapse the @Ans@ monad to values of type @Result@ by providing the desired
arguments, i.e., the new environment, new store, and caches. The collapse
happens at current-stage, so the call of @ev@ is unfolded after staging. At
last, we generate a singleton set containing the compiled closure
@emit_compiled_clo(f)@, represented by an IR node in LMS.

\begin{lstlisting}
  def emit_compiled_clo(f: (Rep[Value], Rep[Store], Rep[Cache], Rep[Cache])
                           => Rep[(List[(Value,Store)], Cache)]): Rep[AbsValue]
  def close(ev: EvalFun)(λ: Lam, ρ: Rep[Env]): Rep[Value] = {
    val Lam(x, e) = λ
    val f: (Rep[Value],Rep[Store],Rep[Cache],Rep[Cache]) => Rep[(List[(Value,Store)],Cache)] = {
      case (arg, σ, in, out) =>
        val α = alloc(σ, x); val ρ_* = ρ + (unit(x) → α); val σ_* = σ ⊔ Map(α → arg)
        ev(e)(ρ_*)(σ_*)(in)(out)
    }; Set[AbsValue](emit_compiled_clo(f))
  }
\end{lstlisting}

The @ap_clo@ method is also similar: we use the staged version of @lift_nd@ to
lift the set of closures into the monad stack, and generate a next-stage value
using @emit_ap_clo@, which represets the result of application, and finally we
put the @out@ cache, store and values back into the monads. The @emit_ap_clo@
method takes the target closure @clo@, argument @rand@, store and caches.

\begin{lstlisting}
  def lift_nd[T](vs: Rep[List[T]]): AnsM[T]
  def emit_ap_clo(rator: Rep[AbsValue], rand: Rep[Value], σ: Rep[Store],
                  in: Rep[Cache], out: Rep[Cache]): Rep[(List[(Value, Store)], Cache)]
  def ap_clo(ev: EvalFun)(rator: Rep[Value], rand: Rep[Value]): Ans = for {
    clo <- lift_nd[AbsValue](rator.toList)
    σ <- get_store; in <- ask_in_cache; out <- get_out_cache
    val res: Rep[(List[(Value, Store)], Cache)] = emit_ap_clo(clo, rand, σ, in, out)
    _ <- put_out_cache(res._2)
    vs <- lift_nd[(Value, Store)](res._1)
    _ <- put_store(vs._2)
  } yield vs._1
\end{lstlisting}

\paragraph{Staged Caching and Fixpoint Iteration} 

Our fixed-point iteration again relies on two caches @in@ and @out@, which are
both staged values now. Therefore, the query of whether the @out@ cache contains
the current configuration produces a next-stage Boolean value, i.e.,
@Rep[Boolean]@. Consequently, the branching can not be determined statically --
we need to generate code for @if@. Figure \ref{fig:staged_coind_cache} shows the
staged version of @fix@.

\vspace{-1em}
\begin{figure}[h!]
  \centering
\begin{lstlisting}
  def fix(ev: EvalFun => EvalFun): EvalFun = e => for {
    ρ <- ask_env; σ <- get_store; in <- ask_in_cache; out <- get_out_cache
    val cfg: Rep[Config] = (unit(e), ρ, σ)
    val res: Rep[(List[(Value, Store)], Cache)] =
      if (out.contains(cfg)) (out(cfg).toList, out) // a next-stage if expression
      else { val m: Ans = for {
               _ <- put_out_cache(out + (cfg → in.getOrElse(cfg, ⊥)))
               v <- ev(fix(ev))(e)
               σ <- get_store
               _ <- update_out_cache(cfg, (v, σ))
             } yield v
             m(ρ)(σ)(in)(out) }
    _ <- put_out_cache(res._2); vs <- lift_nd(res._1); _ <- put_store(vs._2)
  } yield vs._1
\end{lstlisting}
\vspace{-1em}
\caption{The staged co-inductive caching algorithm.}
\label{fig:staged_coind_cache}
\end{figure}
\vspace{-1em}

The @res@ variable is a next-stage result, which consists of a next-stage @if@
expression. The true branch simply returns a pair of queried result form the
@out@ cache and the @out@ cache. The else branch constructs a monad @m@ first,
which evaluates @e@ under the new @out@ cache. After which, we collapse the
monad @m@ with desired arguments. Finally, we obtain a placeholder @res@ since
it is a next-stage value, and put the content of @res@ back into the moand
stack.

\paragraph{Code Generation} The IR nodes for compiled closures and
applications are defined as following:

\begin{lstlisting}
  case class IRCompiledClo(f: (Rep[Value], Rep[Store], Rep[Cache], Rep[Cache])
                           => Rep[(List[(Value, Store)], Cache)]) extends Rep[AbsValue]
  case class IRApClo(clo: Rep[AbsValue], arg: Rep[Value], σ: Rep[Store],
                     in: Rep[Cache], out: Rep[Cache]) extends Rep[(List[(Value, Store)], Cache)]
\end{lstlisting}

The code generator for these two IR nodes is similar to staged concrete
interpreter, which generates a next-stage @CompiledClo@ object and a next-stage
function call to @f@, respectively.
                   
\subsection{Optimizations} \label{staged_ds}

Our staging schema works well in theory, but in practice would suffer from code
explosion, and possibly runtime GC overhead (at the next stage) when analyzing
(specializing) large programs. In this section, we present several optimizations
that largely mitigate these issues. Still, implementing these optimizations do
not need to change the generic interpreter.

\paragraph{Specialized Data Structures}

Although every components except @Expr@s are staged, we treat the data
structures such as @Map@s as black-boxes, which means the operations on a @Map@
directly become code in the next stage, but we don't inspect any further inside.
As we identified when introducing the generic interface, the keys of @Env@s are
identifiers (i.e., strings) in the program, which are completely known
statically. This leaves us a chance to further specialize the data structures.
For example, let's assume that the @Map[K, V]@ is implemented as a hash map. If
the keys of type @K@ are all known statically, then the indices also can be
computed statically. Thus the specialized map would be an array of type
@Array[Rep[V]]@, whose elements are next-stage values; the size of the array is
also known statically. An access to the map is translated to an access to the
array with pre-determined index during staging.

Particularly, if we are specializing a monovariant analysis, the address space
is equivalent to the set of identifiers in the program. Then the accesses to the
environment can be completely determined statically, and generates the addresses
directly. After which, the stores can be specialized as arrays of @Rep[Value]@
elements.

\paragraph{Selective Caching} It is observed that the two-fold co-inductive
caching is used for every recursive call of our abstract interpreter. But this
is not necessary and even very expensive when generating code for atomic
expressions such as literals or variables, as they always terminate. Borrowing
the idea from the partition in ANF \cite{Flanagan:1993:ECC:155090.155113}, we
may use a selective caching algorithm:

\begin{lstlisting}
  def fix_select: EvalFun = e => e match {
    case Lit(_) | Var(_) | Lam(_, _) => eval(fix_select)(e)
    case _ => fix_cache(e)
  }
\end{lstlisting}

\paragraph{Partially-static Data}

Our treatment to binding-times is coarse-grained: @Expr@s is static, the rest of
the world are all dynamic. But this is not always true, since the static data
has to be used somewhere with the dyanmic operation. Partial-static data is a
way to improve binding-times and optimize the generated code.

For example, to fold a singleton list (often appears in @ListT@), e.g.,
@List(x).foldLeft(init)(f)@ where @x@ and @init@ are staged values, a naive code
generator would faithfully apply @foldLeft@ to the list with an anonymous
function. But we can also utilize the algebraic property of @foldLeft@ to
generate cheaper code: @List(x).foldLeft(init)(f) = f(init, x)@. Since the
function @f@ is known at current stage, we completely eliminate the fold
operation and function application. We apply several rewritings enabled by
partial-static data structures, such as @List@ and @Map@, which greatly reduces
the size of residual programs.

%\paragraph{Heterogeneous Staging} The generated program is in A-Normal form. \todo{TODO}

\subsection{Discussion}

We have gradually presented the confluence of specialization and abstraction of
concrete interpreters. In this section, we discuss several decisions we made to
achieve this and examine some alternatives.

\paragraph{What the generated code look like?} In the generated code, all the
primtive operations (such as @eval@, @fix@, @ap_clo@, etc.) and moandic
operations (such as @flatMap@ and @map@) are eliminated. The residual program
consists of statements and expressions that purely manipulate the environment,
store, and two caches, whose underlying representions are all @Rep[Map[K,V]]@. We
also have several operations on tuples and lists, which are residualized from
the use of monads internally.

\paragraph{Are Monads Necessary?} No. One can always inline the monads and
obtain an abstract interpreter in continuation-passing style, or simply use
explicit side-effects such mutation to implement the artifact with the same
abstract semantics. In either cases, we can still apply the staging schema to
the abstract interpreter. As an evidence, we provide an staged abstract
interpreter written in direct-style in the accompanying artifact.

\paragraph{Big-step vs Small-step}

What we implemented is a big-step, compositional abstract interpreter in monadic
style, where \textit{compositional} means that every recursive call of our abstract
interpreter is applied to proper substructures of the current syntactic
parameters \cite{10.1007/3-540-61580-6_11}. This compositionality ensures that
specialization can be done by unfolding, as well as guarantees the termination
of specialization procedure. It is also possible to specialize small-step
operational abstract semantics through abstract compilation
\cite{Boucher:1996:ACN:647473.727587} -- as
\citet{Johnson:2013:OAA:2500365.2500604} implemented it for
optimizating Abstract Abstract Machines. However, the generated abstract
bytecode still requires another small-step abstract machine to execute, which is
an additional engineering efforts.

\paragraph{Correctness and Soundness}

As one of the merit of our approach, the staging does not compromise any
soundness, because we only change the underlying monads and data structures to
the staged version, not any part of the abstract semantics. Based on the
assumption that MSP system and staged data structure preserve the equivalence
during staging, we are confident that the staged abstract interpreter does the
same analysis as the unstaged one.

