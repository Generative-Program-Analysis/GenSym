\section{From Abstract Interpreters to Staged Abstract Interpreters} \label{sai}

In the previous sections, we have seen an unstaged abstract interpreter and a
staged concrete interpreter. Now we begin describing the implementation of
their confluence -- a staged abstract interpreter.  Unsurprisingly, the staged
abstract interpreter in this section uses the same abstract semantics from the
unstaged version in Section~\ref{unstaged_abs}.  The approach we use to refactor
the unstaged one to the staged abstract interpreter is modular, and does not
sacrifice soundness or precision. The designer of the analyzer therefore has no
need to rewrite the analysis. We first present the staged lattices and staged
monads, and then discuss the staged version of primitive operations, especially
@close@, @ap_clo@ and @fix@. In the end, we discuss several optimizations.

\subsection{Staged Lattices}

In Section \ref{stagedpoly_lat}, we exploited the higher-kinded type @R@ to
achieve the stage polymorphism. Now we instantiate the type @R@ as @Rep@ and
still use the power-set as example to describe its staged version.
\begin{lstlisting}[escapechar=!]
  trait RepLattice[S] extends Lattice[S, Rep]
  def RepSetLattice[T]: RepLattice[Set[T]] = new RepLattice[Set[T]] {
    lazy val ⊥: !\hl{Rep[Set[T]]}! = Set[T]()
    lazy val ⊤: !\hl{Rep[Set[T]]}! = error("No representation for ⊤")
    def ⊑(l1: !\hl{Rep[Set[T]]}!, l2: !\hl{Rep[Set[T]]}!): !\hl{Rep[Boolean]}! = l1 subsetOf  l2
    def ⊔(l1: !\hl{Rep[Set[T]]}!, l2: !\hl{Rep[Set[T]]}!): !\hl{Rep[Set[T]]}!  = l1 union     l2
    def ⊓(l1: !\hl{Rep[Set[T]]}!, l2: !\hl{Rep[Set[T]]}!): !\hl{Rep[Set[T]]}!  = l1 intersect l2
  }
\end{lstlisting}

The trait @RepLattice@ is shown by specializing @Lattice@ with type @Rep@.
The @RepSetLattice@ is an instance of @RepLattice@, where the lattice operations
are eventually delegated to the operations on a staged set data type, such as
@subsetOf@, @union@ and @intersect@. We do not need to change the
implementation code, except the types -- such as from @Set[T]@ to @Rep[Set[T]]@.
We show the changes in light gray code. The LMS library provides implicit
conversions that lift the current-stage values to their next-stage values. In
the code generation part, these operations emit their corresponding next-stage
code. Again, other lattice structures such as products and maps are implemented
in a similar way.

\subsection{Staged Abstract Semantics}

Now we can implement the staged abstract semantics, under that @R@ is instantiated as
@Rep@. The types of abstract components are reused from the unstaged version.

\paragraph{Staged Monads for Abstract Interpretation}
We use the same monad stack structure from the unstaged abstract interpreter,
but replacing \textit{all} the monad transformers with their staged versions. The
following code snippet shows this change. We manually fuse the three inner
transformers (@IdM@ omitted) into a single monad @RepSetReaderStateM[A, B, C]@,
where @A@ is the type parameter for the reader effects and @B@ for the state effects.
In our monad stack scheme, they are both instantiated with the type @Cache@. 
Similar to the unstaged version, the grounded result type is a pair of two
staged values: @Rep[Set[(Value, Store)]]@ and @Rep[Cache]@.
\begin{lstlisting}
  trait StagedAbstractSemantics extends AbstractComponents {
    type R[T] = Rep[T]
    type AnsM[T] = RepReaderT[RepStateT[RepSetReaderStateM[Cache, Cache, ?], Store, ?], Env, T]
    type Result = (Rep[Set[(Value, Store)]], Rep[Cache])
    ...
  }
\end{lstlisting}

\iffalse
Since our monad stack is uniformly using staged data, the staged @SetT@ now
stores a staged value of type @Rep[Set[A]]@, inside of another staged monad @M@.
\begin{lstlisting}
  case class SetT[M[_]: RepMonad, A](run: M[Set[A]]) {
    def flatMap[B: Manifest](f: Rep[A] => SetT[M, B]): SetT[M, B] = ...
  }
\end{lstlisting}
\fi

\paragraph{Primitive Operations} 
When deriving the staged concrete interpreter from its unstaged counterpart, we
notice that the recursive calls of the evaluator @ev@ on the lambda body are shifted from
@ap_clo@ to @close@. In other words, we eagerly specialize the interpreter and
generate code every time when we see a $\lambda$ term, instead of lazily when the
applications happen.  With no surprise, it is similar here. Additionally,
in the staged abstract interpreter, we have to handle nondeterminism
incurred by the over-approximation of runtime behavior.
In the rest of this part, we keep our roadmap and focus on discussing closure
representations and function applications for the staged version.

The @close@ method now is a mix of the \textit{staged} concrete version and
unstaged \textit{abstract} version. We first build a current-stage function @f@, which
takes four next-stage values, including the argument and latest store, and
@in/out@ caches additionally. Inside of @f@, we collapse the @Ans@ monadic value to
a grounded value of type @Result@, by providing the desired arguments, i.e., the
new environment, new store, and caches. The collapsing of monad happens at
current-stage, so the calls of @ev@ is unfolded within staging. Finally, we
generate a singleton set containing the compiled next-stage closure
@emit_compiled_clo(f)@, which is represented by an IR node in LMS.
\begin{lstlisting}
  def emit_compiled_clo(f: (Rep[Value], Rep[Store], Rep[Cache], Rep[Cache])
                           => Rep[(Set[(Value,Store)], Cache)], λ: Lam, ρ: Exp[Env]): Rep[AbsValue]
  def close(ev: Expr => Ans)(λ: Lam, ρ: Rep[Env]): Rep[Value] = {
    val Lam(x, e) = λ
    val f: (Rep[Value],Rep[Store],Rep[Cache],Rep[Cache]) => Rep[(Set[(Value,Store)],Cache)] = {
      case (arg, σ, in, out) =>
        val α = alloc(σ, x)
        ev(e)(ρ + (unit(x) → α))(σ ⊔ Map(α → arg))(in)(out)
    }; Set[AbsValue](emit_compiled_clo(f, λ, ρ))
  }
\end{lstlisting}

The @ap_clo@ method is also similar: we use the staged version of @lift_nd@ to
lift the next-stage set of closures into the monad stack, and generate a
next-stage value for each closure @clo@ in the set, by using @emit_ap_clo@. The
method @emit_ap_clo@ generates the current-stage representation of the
future-stage application result.
Finally, we put the future @out@ cache, store, and values back into
the current-stage monads.
\begin{lstlisting}
  def emit_ap_clo(fun: Rep[AbsValue], arg: Rep[Value], σ: Rep[Store],
                  in: Rep[Cache], out: Rep[Cache]): Rep[(Set[ValSt], Cache)]
  def ap_clo(ev: Expr => Ans)(funs: Rep[Value], arg: Rep[Value]): Ans = for {
    σ   <- get_store;      clo <- lift_nd[AbsValue](funs)
    in  <- ask_in_cache;   out <- get_out_cache
    res <- lift_nd[(Set[ValSt], Cache)](Set(emit_ap_clo(clo, arg, σ, in, out)))
    _   <- put_out_cache(res._2);  vs <- lift_nd[ValSt](res._1);  _ <- put_store(vs._2)
  } yield vs._1
\end{lstlisting}

\paragraph{Staged Caching and Fixpoint Iteration} Our fixed-point iteration
again relies on the two caches @in@ and @out@, which are both staged maps now.
Therefore, the query of whether the @in/out@ cache contains the current
configuration will produce next-stage Boolean values, i.e., of type
@Rep[Boolean]@. Consequently, the branching condition cannot be determined
statically -- we need to generate code for the whole @if@ expression. Figure
\ref{fig:staged_coind_cache} shows the staged version of @fix@. The
variable @res@ represents the next-stage result, consisting of a next-stage
@if@ expression. The true branch simply returns a pair of the queried result form
the @out@ cache and the @out@ cache itself. The else branch constructs a monadic
value @m@ of type @Ans@ first, which evaluates @e@ under the new @out@ cache.
After which, we use a similar technique that collapses the monadic value @m@ to
grounded values, by providing its desired environment and etc. Finally, we have a
current-stage representation of the future-stage values @res@, and put the
content of @res@ back into the monad stack.

\begin{figure}[h!]
\centering
\begin{lstlisting}
  def fix_cache(e: Expr): Ans = for {
    ρ   <- ask_env;  σ <- get_store;  in <- ask_in_cache;  out <- get_out_cache
    cfg <- lift_nd[Config](Set((unit(e), ρ, σ)))
    res <- lift_nd[(Set[ValSt], Cache)](Set(
      // a next-stage value of type Rep[(Set[ValSt], Cache)] generated by if
      if (out.contains(cfg)) (out(cfg), out)
      else { val m: Ans = for {
               _ <- put_out_cache(out + (cfg → in.getOrElse(cfg, ⊥)))
               v <- eval(fix_cache)(e);  σ <- get_store;  _ <- update_out_cache(cfg, (v, σ))
             } yield v
             m(ρ)(σ)(in)(out) }))
    _ <- put_out_cache(res._2);  vs <- lift_nd(res._1);  _ <- put_store(vs._2)
  } yield vs._1
\end{lstlisting}
\caption{The staged co-inductive caching algorithm.}
\label{fig:staged_coind_cache}
\vspace{-2em}
\end{figure}

\subsection{A Little Bit of Code Generation, Again}
The code generation for compiled closures and function applications are similar
to their counterparts in the staged concrete interpreter. We have two IR nodes
implemented as case classes; they also take additional caches as arguments. We
elide the code generation part for these IR nodes.
\begin{lstlisting}
case class IRCompiledClo(f: (Rep[Value], Rep[Store], Rep[Cache], Rep[Cache])
                          => Rep[(Set[ValSt], Cache)], λ: Lam, ρ: Rep[Env]) extends Def[AbsValue]
case class IRApClo(clo: Rep[AbsValue], arg: Rep[Value], σ: Rep[Store],
                    in: Rep[Cache], out: Rep[Cache]) extends Def[(Set[(Value, Store)], Cache)]
\end{lstlisting}

\subsection{Optimizations} \label{staged_ds}

Our staging schema works by unfolding the interpreter over the input abstract
syntax tree. In practice, however, it would suffer from code explosion when
analyzing (specializing) large programs, which increases the compile time. If
we generate next-stage programs running on JVM, such large generated program
would also potentially cause runtime GC overhead. In this section, we present
optimizations that largely mitigate these issues. For all that,
implementing these optimizations do not need to change the generic interpreter.

\paragraph{Specialized Data Structures}

All components in the interpreter except @Expr@s are staged. The data
structures representing these components are treated as black-boxes, such as
@Map@, which means that the operations on a @Map@ directly become nex-stage code,
but we don't inspect any further inside. As we identified when
introducing the generic interface, the keys of @Env@s are identifiers (i.e.,
strings) in the program, which are completely known statically. This
observation gives us a chance to further specialize the data structures.  For
example, let us assume that the @Map[K, V]@ is implemented as a hash map. If
all the keys of type @K@ are all known statically, then the indices also can be
computed statically. Thus, the specialized map would be an array of type
@Array[Rep[V]]@, whose elements are next-stage values, and the size of the array is
known statically. An access to the map is translated into an access to the
array with the pre-computed index during staging.

Particularly, if we are specializing a monovariant analysis, the address space
is equivalent to the set of identifiers in the program. Then the accesses to
the environment can be completely computed statically and we may generate the
addresses directly during staging. After which, the stores are specialized
as arrays with element type @Rep[Value]@.

\paragraph{Selective Caching} We observe that the two-fold co-inductive
caching is used for every recursive call in our abstract interpreter. But this
is not necessary and redundant when generating code for atomic
expressions such as literals or variables, because they always terminate.
Borrowing the idea from the partition of expressions used in A-Normal Form
$\lambda$-calculus \cite{Flanagan:1993:ECC:155090.155113}, we can use a
selective caching algorithm that does not generate caching code for atomic
expressions:
\begin{lstlisting}
  def fix_select: Expr => Ans = e => e match {
    case Lit(_) | Var(_) | Lam(_, _) => eval(fix_select)(e)
    case _ => fix_cache(e)
  }
\end{lstlisting}

\paragraph{Partially-static Data}
Our treatment of binding-times is coarse-grained: @Expr@s is static, the rest
of the components are all dynamic. But this is not always true, because the static
data have to be used somewhere with the dynamic operations. Partial-static data is a
way to improve binding-times and optimize the generated code.

For example, to fold a static singleton set (often appears in @SetT@), e.g.,
@Set(x).foldLeft(init)(f)@ where @x@ and @init@ are staged values, a naive code
generator would faithfully generates code that applies @foldLeft@ with the set and
function @f@. But we can also utilize the algebraic property of @foldLeft@ to
generate cheaper code: @Set(x).foldLeft(init)(f) = f(init, x)@. Since the
function @f@ is known at the current stage, we completely eliminate the fold
operation and function application. We apply several rewritings enabled by
partial-static patterns, such as for @Set@ and @Map@. This optimizations
greatly reduces the size of residual programs.

\section{Discussion} \label{discussion}

We have gradually presented the confluence of specialization and abstraction of
concrete interpreters from an operational perspective.  In this section, we
review and summarize our recipe to achieve the staged abstract interpreter and
discuss correctness issues and different design choices.

\subsection{Summarizing the Approach}

We summarize our approach to staging an abstract interpreter as follows:

\begin{itemize}
  \item Constructing a generic interpreter that abstracts over binding times,
    value domains, and primitive operations. In this paper, the generic interpreter
    is implemented in monadic style; therefore, the semantics can be encapsulated
    into monads.
  \item Then we can implement an unstaged abstract interpreter modularly using
    the appropriate monad stack. This step has been explored in the previous
    literatures.
  \item Finally, we replace the monad stack with a staged monad stack, and
    refactor related primitive operations.  Such staged monads operate on
    staged data types, i.e., next-stage values.
\end{itemize}

Monadic interpreters are known to be able to decouple the interpretation
procedure and the underlying semantics. The key insight in this paper is that
by making the monadic interpreter binding-time polymorphic, the interpreter can
be extended for generating efficient code. The underlying semantics and staging
are two orthogonal dimensions. It is important to note that the computation
encapsulated by the monads are not staged: only data (such as sets and maps)
are staged. All the monadic computation, i.e., functions passed to the monadic
bind operation @flatMap@, are statically known. This is why we can eliminate
the monadic layer and its associated overhead.

\paragraph{What has been eliminated?} In the generated code, all the
primitive operations (such as @eval@, @fix@, @ap_clo@, etc.) and monadic
operations (such as @flatMap@ and @map@) are eliminated. The residual program
consists of statements and expressions that purely manipulate the environment,
store, and two caches, whose underlying representations are all @Rep[Map[K,V]]@. We
also have several operations on tuples and lists, which are residualized from
the internal code fragments of monads. 

\subsection{Correctness}

Soundness is the central concern of static analysis, and as such, is vital
for prospective users. We now briefly examine how our
approach preserves the correctness and soundness of the analysis, under the
assumption that the unstaged one is sound. It is worth noting that the rationale
listed here is based on empirical evidence; a rigorous proof for soundness
preservation of the staged abstract interpreter remains an open challenge.

\begin{itemize}
  \item We usually assume the underlying MSP system preserves the meaning during staging,
    i.e., the generated program does not change the observational
    behaviors of the original program. In the case of this paper, we use the
    LMS framework, which 1) relies on the type checking facilities of Scala, and
    2) generates code in A-Normal form \cite{Flanagan:1993:ECC:155090.155113}
    that preserves the execution order within a stage \cite{DBLP:conf/birthday/Rompf16}.
    However, in general, it is possible to subvert these guarantees by
    using unsafe features, such as casts.
  \item As shown in the previous sections, the generic interpreter is untouched
    and shared by the four artifacts. We only instantiate binding-time
    annotations and the monadic types. This allows the programmer to easily
    check the correctness of the implementation of staged monads modularly.
  \item We build the staged monads and staged data structures in a
    correct-by-construct way that directly corresponds to their unstaged
    versions. For instance, the staged data structures we use here are simply
    black boxes that wrap the data structures in Scala library.
  \item In our experiments, the staged abstract interpreter produces the same
    result with the unstaged one on all benchmark programs we tested.
\end{itemize}

\subsection{Design Choices}

\paragraph{Monadic-style vs Direct-style} We use a monadic interpreter
throughout the paper, but it is not necessary for staging. One can inline the
monadic operations and obtain an abstract interpreter in continuation-passing
style, or even translate back to a direct-style that may use explicit
side-effects such as mutations. In either case, we can still apply the staging
idea to the abstract interpreter and remove the interpretation overhead.
However, monads allow the staged abstract interpreter to be implemented in a
modular and extensible way.

\paragraph{Big-step vs Small-step}

We implemented a big-step, compositional abstract interpreter in monadic
style, where \textit{compositional} means that every recursive call of our abstract
interpreter is applied to the proper substructures of the current syntactic
parameters \cite{10.1007/3-540-61580-6_11}. This compositionality ensures that
specialization can be done by unfolding, as well as guarantees the termination
of the specialization procedure. It is also possible to specialize a small-step
operational abstract semantics through abstract compilation
\cite{Boucher:1996:ACN:647473.727587} -- as
\citet{Johnson:2013:OAA:2500365.2500604} presented for
optimizing Abstract Abstract Machines. However, the generated abstract
bytecode still requires another small-step abstract machine to execute, which is
an additional engineering effort.
