\section{Case Studies} \label{cases_study}

In Section~\ref{sai}, we use a toy language to show that staging an abstract
interpreter is feasible and can be systematically derived from staged concrete
interpreters and unstaged abstract interpreter.
In this section, we conduct several case studies to further show that this
methodology is practically useful and widely applicable to diverse analyses.

\subsection{Abstract Compilation \`a la Staging} \label{cs_ac}

We first revisit a similar technique called \textit{abstract compilation} (AC).
AC is introduced by \citet{Boucher:1996:ACN:647473.727587} as a new
implementation technique for abstract interpretation based static analysis.
Similar to the present paper, the idea is inspired by partial evaluation:
the program is known statically and the interpretive overhead can be
eliminated. In AC, the compiled analysis can be represented by either text or
closures (higher-order functions). The minute difference is that the closures
can be executed immediately, while the textual programs need to be compiled and
loaded.

Specifically, \citeauthor{Boucher:1996:ACN:647473.727587} showed how to compile a
monovariant control-flow analysis \cite{Shivers:1991:SSC:115865.115884,
Shivers:1988:CFA:53990.54007} for continuation-passing style (CPS) programs.
Since the analyzed program is in CPS form, the analyzer is a big-step
control-environment abstract interpreter. The abstract compilation transforms
the analysis as a closure just taking an environment argument. Therefore the
overhead of traversing the AST of input program has been eliminated.

In this section, we show that \citeauthor{Boucher:1996:ACN:647473.727587}'s AC
(closure generation as example) can be understood and implemented as an instance
of staging abstract interpreters. We first revisit the original implementation
of abstract compilation of 0-CFA, and then reproduce their result by simply
adding stage annotations.
The generated program of our approach improves approximately the same extent of
speed, but without changing a single line of the analyzer program using
type-based staging annotations. However, AC requires more engineering effort,
i.e., rewriting the whole analyzer into the closure generation form. Moreover,
as shown in Section~\ref{staged_ds}, our approach is able to not only remove the
interpretive overhead, but also enabling several optimizations such specialized
data structures and equivalent rewritings.

\begin{figure*}
  \centering
  \begin{subfigure}[h]{0.49\textwidth}
    \centering
    \begin{lstlisting}[style=extrasmall]
type CompAnalysis = Store => Store
def compProgram(prog: Expr): CompAnalysis = compCall(prog)
def compCall(call: Expr): CompAnalysis = call match {
  case Letrec(bds, body) =>
    val C1 = compCall(body); val C2 = compArgs(bds.map(_.value))
    ($\sigma$: Store) => C1(C2($\sigma$.update(bds.map(_.name), 
       bds.map(b => Set(b.value.asInstanceOf[Lam])))))
  case App(f, args) =>
    val C1 = compApp(f, args); val C2 = compArgs(args)
    ($\sigma$: Store) => C1(C2($\sigma$))
}
def compApp(f: Expr, args: List[Expr]): CompAnalysis = f match {
  case Var(x) => ($\sigma$: Store) => 
    analysisAbsApp($\sigma$.lookup(x), args, $\sigma$)
  case Op(_) => compArgs(args)
  case Lam(vars, body) =>
    val C = compCall(body)
    ($\sigma$: Store) => C($\sigma$.update(vars, args.map(primEval(_, $\sigma$))))
}
def compArgs(args: List[Expr]): CompAnalysis = args match {
  case Nil => ($\sigma$: Store) => $\sigma$
  case (arg@Lam(vars, body))::rest =>
    val C1 = compCall(body); val C2 = compArgs(rest)
    ($\sigma$: Store) => C2(C1($\sigma$))
  case _::rest => compArgs(rest)
}
  \end{lstlisting}
  \end{subfigure}
\hfill
  \begin{subfigure}[h]{0.49\textwidth}
    \centering
    \begin{lstlisting}[style=extrasmall]
def analyzeProgram(prog: Expr, $\sigma$: Rep[Store]): Rep[Store] = 
  analyzeCall(prog, $\sigma$)
def analyzeCall(call: Expr, $\sigma$: Rep[Store]): Rep[Store] = 
  call match {
    case Letrec(bds, body) =>
      val $\sigma$_* = $\sigma$.update(bds.map(_.name), 
        bds.map(b => Set(b.value.asInstanceOf[Lam])))
      val $\sigma$_** = analyzeArgs(bds.map(_.value), $\sigma$_*)
      analyzeCall(body, $\sigma$_**)
    case App(f, args) => analyzeApp(f, args, analyzeArgs(args, $\sigma$))
  }
def analyzeApp(f: Expr, args: List[Expr], $\sigma$: Rep[Store]): 
  Rep[Store] = f match {
    case Var(x) => analyzeAbsApp(args, $\sigma$(x), $\sigma$)
    case Op(_) => analyzeArgs(args, $\sigma$)
    case Lam(vars, body) =>
      val $\sigma$_* = $\sigma$.update(vars, args.map(primEval(_, $\sigma$)))
      analyzeCall(body, $\sigma$_*)
  }
def analyzeArgs(args: List[Expr], $\sigma$: Rep[Store]): Rep[Store] = 
  args match {
    case Nil => $\sigma$
    case Lam(vars, body)::rest => 
      analyzeArgs(rest, analyzeCall(body, $\sigma$))
    case _::rest => analyzeArgs(rest, $\sigma$)
  }
  \end{lstlisting}

  \end{subfigure}
  \caption{Comparison of AC (left) and SAI (right). Only core code are shown.}
  \label{compare_ac_sai}
\end{figure*}

\paragraph{Original AC}
The analysis presented by \citeauthor{Boucher:1996:ACN:647473.727587} is 0-CFA
for a small CPS language consisting of abstractions, applications, recursion and
primtive operators. Figure \ref{compare_ac_sai} (left) shows the AC
implementation: different syntactic constructs are handled in different functions
(e.g., @compCall@, @compApp@, etc.).
Before refactoring into AC, these functions would take the store as a
regular argument; now every function returns a value of type @CompAnalysis@,
which is a function (closure) value that takes and returns stores. After the
first run of analysis, we have traversed the AST and obtained a single closure
takes a store and returns a store. Therefore, the ASTs are static terms, and the
stores are dynamic terms. Now we can invoke the compiled closure with an initial
store to complete the analysis.

\paragraph{AC through Staging}

On the other side, our approach does exactly the same thing through staging: the
AST are static, and stores are dynamic, therefore we annotate the type @Store@
with @Rep@ (shown in Figure \ref{compare_ac_sai} (right)). In fact, the only
changes of our appraoch using LMS framework are the types of @Store@ to
@Rep[Store]@, since they will be known at the next stage. The generated code
will just looks-up and update the store. The functions at current stage are
eliminated in the residual program, therefore we do not generate higher-order
functions, which provides additional performance improvement. Nevertheless,
the support from LMS framework is necessary, such as staging support for @Map@,
we consider these efforts are reusable and complete orthogonal to the
implementation of analysis.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Control-flow Analysis} \label{cfa}

In this section, to demonstrate the SAI approach is widely applicable, we extend
and refactor the staged abstract interpreter we presented in Section \ref{sai}
to various control-flow analysis (CFA). We first discuss calling-context
sensitive analysis for staged analyzers; then we will see how to implement
store-widening with staging, which is a common optimization in CFA; finally, we
integrate abstract garbage collection with staging, \todo{where the addresses to
  be collected can be computed at the staging time}.

\subsubsection{Context-sensitivity}

\subsubsection{Store-widened Analysis}

\subsubsection{Abstract Garbage Collection at Staging Time}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5

\subsubsection{A More Precise Store Model}

The store-widened analysis we implemented in Section~\ref{sai} uses a single
store to approximate the runtime store. It can be efficently computed in
polynomial time together with 0-CFA-like abstraction, but sometimes we may
desire a more precise result that distinguishes the final values and stores for
different closure targets. To achieve this goal, we need to tweak our abstract
interpreter and type instantiation. The answer type @Ans@ is changed to a set of
@VS@s where a @VS@ is a pair of sets of abstract values (such as closures or
abstract numbers) and a store.

\begin{lstlisting}
type VS = (Set[AbsValue], Store)
type Ans = R[Set[VS]]
\end{lstlisting}

Note that the type @Ans@ uses our stage polymorphic type @R@, meaning that under
staging the type @Ans@ represents a next stage value. Then, our generic
interpreter is also changed when handling function applications.

\begin{lstlisting}
case App(e1, e2) =>
  val e1ans = ev(e1, $\rho$, $\sigma$)
  val e1vs  = choices(e1ans)
  val e2ans = ev(e2, $\rho$, e1vs.$\sigma$)
  val e2vs  = choices(e2ans)
  apply_closure(ev)(e1vs.v, e2vs.v, e2vs.$\sigma$)
\end{lstlisting}

The idea to handle the application is to explore all possible closures from @e1@
and meanwhile use the latest store. What @choices@ does is similar to McCarthy's
@amb@ operator ~\cite{MCCARTHY196333}: it non-deterministically returns an
element of type @VS@ from its argument, e.g., @e1ans@, to its right-hand side
receiver, i.e., @e1vs@. The function @choices@ internally uses the delimited
control operator @shift@ to capture the continuation, which is the code block
after its call site. This allows us to perform nesting depth-first evaluation
for function application while still writing the program in direct-style
\cite{Wei:2018:RAA:3243631.3236800}. Again, we can stage this part as we did for
the naive abstract interpreter.

\subsubsection{Context-Sensitivity}

We add k-CFA-like context-sensitivity to the analysis by introducing an abstract
timestamp, whose concrete instantiation is a finite list of expressions that
track $k$ recent calling contexts. The definition of abstract addresses is
changed to a tuple of identifiers and the time it get allocated, meaning that
this address points to some values under such calling context. If $k$ is 0, we
obtain a monovariant analysis as demonstrated before; if $k > 0$, we obtain a
family of analysis with increasing precision.

\begin{lstlisting}
type Time = List[Expr]
\end{lstlisting}

Every time when we call the @eval@ function, we refresh the timestamp by calling
function @tick@, which returns a new timestamp. Here we adopt a $k$-CFA-like
allocation strategy, therefore the @tick@ function can be implemented as
appending the current expression being evaluated to the existing calling
context, and then taking the first $k$ elements from the list.

\begin{lstlisting}
def tick(e: Expr, $\tau$: R[Time]): R[Time] = (e :: $\tau$).takewhile
def eval(ev: EvalFun)
  (e: Expr, $\rho$: R[Env], $\sigma$: R[Store], $\tau$: R[Time]): Ans = {
    val $\tau$_* = tick(e, $\tau$)
    e match {
      case Lit(i) => ...
      ...
    }
  }
\end{lstlisting}

Accordingly, the type of return value is accompanied by the timestamp. For a
recursive call of @ev@, which would also return the latest timestamp, and that
timestamp will be used for the evaluation afterward.

\begin{lstlisting}
type VST = (Value, Store, Time)
type Ans = R[Set[VST]]
\end{lstlisting}

Using other allocation strategies to achieve different sensitivities is also
possible \cite{DBLP:conf/icfp/Gilray0M16} and can be staged under our framework.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Modular Analysis for Free} \label{modular}

% \subsubsection{Mixed Sensitivity}
% using different k?

One of the challenges of modern static analysis is program usually depends on
large libraries programs~\cite{toman_et_al:LIPIcs:2017:7121}. Can we analyze
programs and libraries separately and reuse the result without losing precision?
Then we can reduce part of the overhead of repeatedly analyzing libraries for
different programs. Indeed, some static analyzers compute summary for a function
or a module, which can be reused later, however they are mostly too conservative
or unsound, which both lead to imprecision.

The specialization of abstract interpreter provides a chance to obtain such
partial analysis result in a mechanized way, but still keeps the analysis sound.
As we see when compiling the closures, we can specialize the abstract
interpreter with respect the body expression of the lambda term without knowing
the actual argument. The programs with some unknown variables are open programs,
which is exactly the case if we want to analyze programs in a modular way.

For a concrete analysis, for example, $k$-CFA ($k$ > 0) is naturally a
whole-program analysis, because it is inter-procedural and needs the last $k$
calling contexts to distinguish different call sites, where the calling contexts
are dynamic values. However, it is possible to analyze programs (libraries)
separately through specializing an abstract interpreter that generates the
analysis as the next-stage program and leave the unavailable programs and
calling contexts as dynamic parameters, and then install these contexts when we
have the whole program.

\iffalse
Another perspective: programs are data for an abstract interpreter, so if we
have $n$ programs, then maybe there can be $n$ stages. Probably we can analyze
first $m$ programs, and generate a residual abstract interpreter waiting for the
rest $(n-m)$ programs. These $(n-m)$ programs might be (abstract) arguments for
the first $n$ programs, and the abstract interpreter itself might be a partial
abstract interpreter.
\fi

\subsection{Numerical Analysis in Imperative Languages} \label{cases_imp}

Now we consider in a first-order imperative language, we may care more about the
data-flow because the control-flow is relatively easy to obtain. In this
section, we show the staging of other abstract domains, particularly an interval
domain for numbers. It has been shown that specializing abstract domains with
respect to the structure of analyzed program significantly improves the
performance: a recent example is online decomposition of polyhedra
\cite{DBLP:conf/popl/SinghPV17, Singh:2017:PCD:3177123.3158143}. In this
section, we first show how to support imperative language features in the
generic abstract interpreter. Then we present a similar idea for the interval
domain and show that the specialization is feasible by staging systematically.

\subsubsection{Scaling to Imperative Languages}

To evaluates an assignment, we first evaluate its right-hand side, and then put
the value into the slot where the address of @v@ points to. For simplicity, we
elect to make the value of the assignment be an @void()@ value.

\begin{lstlisting}
case Assign(x, e) =>
  val (v, $\sigma$_*) = ev(e, $\rho$, $\sigma$)
  (void(), put($\sigma$_*, get($\rho$, x), v))
\end{lstlisting}

To evaluates a @while@ loop statement, we evaluate the condition first. Then
similar to how we treat @branch0@, we have a generic @branch@ function but works
on boolean values. For the @true@ branch, we recursively call @ev@ on a newly
constructed expression @Seq(e, While(t, e))@ meaning that first evaluates @e@
and the repeat the loop. Otherwise, for the other branch, we simply return a
void value and current store.

\begin{lstlisting}
case While(t, e) =>
  val (tv, t$\sigma$) = ev(t, $\rho$, $\sigma$)
  branch(tv, ev(Seq(e, While(t, e)), $\rho$, t$\sigma$), (void(), t$\sigma$))
\end{lstlisting}

\subsubsection{Staged Interval}

Interval domain is a relative simple domain which consists of two numeric
fields, an upper bound and a lower bound. Here we annotate the upper bound and
lower bound fields to be of type @Rep[Double]@. The operations such as @+@ of
two intervals produce a new

\begin{lstlisting}
case class Interval(lb: Rep[Double], ub: Rep[Double]) {
  def +(that: Interval): Interval = that match {
    case Interval(lb_, ub_) => Interval(lb + lb_, ub + ub_)
  }
  ...
}
\end{lstlisting}

