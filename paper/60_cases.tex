\section{Case Studies} \label{cases_study}

In Section~\ref{sai}, we use a toy language to show that staging an abstract
interpreter is feasible and can be systematically derived from staged concrete
interpreters and unstaged abstract interpreter.
In this section, we conduct several case studies to further show that this
methodology is practically useful and widely applicable to diverse analyses.

\subsection{Abstract Compilation \`a la Staging} \label{cs_ac}

We first revisit a similar technique called \textit{abstract compilation} (AC).
AC was introduced by \citet{Boucher:1996:ACN:647473.727587} as an
implementation technique for abstract interpretation based static analysis.
Similar to the present paper, the idea is inspired by partial evaluation:
the program is known statically and the interpretive overhead can be
eliminated. In AC, the compiled analysis can be represented by either text or
closures (higher-order functions). The minute difference is that the closures
can be executed immediately, while the textual programs need to be compiled and
loaded.

Specifically, \citeauthor{Boucher:1996:ACN:647473.727587} showed how to compile a
monovariant control-flow analysis \cite{Shivers:1991:SSC:115865.115884,
Shivers:1988:CFA:53990.54007} for continuation-passing style (CPS) programs.
Since the analyzed program is in CPS form, the analyzer is a big-step
control-environment abstract interpreter. The abstract compilation transforms
the analysis as a closure just taking an environment argument. Therefore the
overhead of traversing the AST of input program has been eliminated.

In this section, we show that the AC (closure generation as example) can be
understood and implemented as an instance of staging abstract interpreters. We
first revisit the original implementation of abstract compilation of 0-CFA, and
then reproduce their result by simply adding stage annotations.
The generated program of our approach improves approximately the same extent of
speed, but without changing a single line of the analyzer program using
type-based staging annotations. However, AC requires more engineering effort,
i.e., rewriting the whole analyzer into the closure generation form. Moreover,
as shown in Section~\ref{staged_ds}, our approach is able to not only remove the
interpretive overhead, but also enabling several optimizations such specialized
data structures and equivalent rewritings.

\begin{figure*}
  \centering
  \begin{subfigure}[h]{0.49\textwidth}
    \centering
    \begin{lstlisting}[style=extrasmall]
type CompAnalysis = Store => Store
def compProgram(prog: Expr): CompAnalysis = compCall(prog)
def compCall(call: Expr): CompAnalysis = call match {
  case Letrec(bds, body) =>
    val C1 = compCall(body); val C2 = compArgs(bds.map(_.value))
    ($\sigma$: Store) => C1(C2($\sigma$.update(bds.map(_.name), 
       bds.map(b => Set(b.value.asInstanceOf[Lam])))))
  case App(f, args) =>
    val C1 = compApp(f, args); val C2 = compArgs(args)
    ($\sigma$: Store) => C1(C2($\sigma$))
}
def compApp(f: Expr, args: List[Expr]): CompAnalysis = f match {
  case Var(x) => ($\sigma$: Store) => 
    analyzeAbsApp(args, $\sigma$(x), $\sigma$)
  case Op(_) => compArgs(args)
  case Lam(vars, body) =>
    val C = compCall(body)
    ($\sigma$: Store) => C($\sigma$.update(vars, args.map(primEval(_, $\sigma$))))
}
def compArgs(args: List[Expr]): CompAnalysis = args match {
  case Nil => ($\sigma$: Store) => $\sigma$
  case (arg@Lam(vars, body))::rest =>
    val C1 = compCall(body); val C2 = compArgs(rest)
    ($\sigma$: Store) => C2(C1($\sigma$))
  case _::rest => compArgs(rest)
}
  \end{lstlisting}
  \end{subfigure}
\hfill
  \begin{subfigure}[h]{0.49\textwidth}
    \centering
    \begin{lstlisting}[style=extrasmall]
def analyzeProgram(prog: Expr, $\sigma$: Rep[Store]): Rep[Store] = 
  analyzeCall(prog, $\sigma$)
def analyzeCall(call: Expr, $\sigma$: Rep[Store]): Rep[Store] = 
  call match {
    case Letrec(bds, body) =>
      val $\sigma$_* = $\sigma$.update(bds.map(_.name), 
        bds.map(b => Set(b.value.asInstanceOf[Lam])))
      val $\sigma$_** = analyzeArgs(bds.map(_.value), $\sigma$_*)
      analyzeCall(body, $\sigma$_**)
    case App(f, args) => analyzeApp(f, args, analyzeArgs(args, $\sigma$))
  }
def analyzeApp(f: Expr, args: List[Expr], $\sigma$: Rep[Store]): 
  Rep[Store] = f match {
    case Var(x) => analyzeAbsApp(args, $\sigma$(x), $\sigma$)
    case Op(_) => analyzeArgs(args, $\sigma$)
    case Lam(vars, body) =>
      val $\sigma$_* = $\sigma$.update(vars, args.map(primEval(_, $\sigma$)))
      analyzeCall(body, $\sigma$_*)
  }
def analyzeArgs(args: List[Expr], $\sigma$: Rep[Store]): Rep[Store] = 
  args match {
    case Nil => $\sigma$
    case Lam(vars, body)::rest => 
      analyzeArgs(rest, analyzeCall(body, $\sigma$))
    case _::rest => analyzeArgs(rest, $\sigma$)
  }
  \end{lstlisting}

  \end{subfigure}
  \caption{Comparison of AC (left) and SAI (right). Only core code are shown.}
  \label{compare_ac_sai}
\end{figure*}

\paragraph{Original AC}
The analysis presented by \citeauthor{Boucher:1996:ACN:647473.727587} is 0-CFA
for a small CPS language consisting of abstractions, applications, recursion and
primitive operators. Figure \ref{compare_ac_sai} (left) shows the AC
implementation: different syntactic constructs are handled in different functions
(e.g., @compCall@, @compApp@, etc.).
Before refactoring into AC, these functions would take the store as a
regular argument; now every function returns a value of type @CompAnalysis@,
which is a function (closure) value that takes and returns stores. After the
first run of analysis, we have traversed the AST and obtained a single closure that
takes a store and returns a store. Therefore, the ASTs are static terms, and the
stores are dynamic terms. Now we can invoke the compiled closure with an initial
store to complete the analysis.

\paragraph{AC through Staging}

On the other side, our approach does exactly the same thing through staging: the
AST are static, and stores are dynamic, therefore we annotate the type @Store@
with @Rep@ (shown in Figure \ref{compare_ac_sai} (right)). In fact, the only
changes of our approach using LMS framework are the types of @Store@ to
@Rep[Store]@, since they will be known at the next stage; other parts do not need to change. 
The generated code
will just looks-up and update the store. The functions at current stage are
eliminated in the residual program, therefore we do not generate higher-order
functions, which provides additional performance improvement. Nevertheless,
the support from LMS framework is necessary, such as staging support for @Map@,
we consider these efforts are reusable and complete orthogonal to the
implementation of analysis.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Control-flow Analysis} \label{cfa}

In this section, to demonstrate the SAI approach is widely applicable, we
extend and refactor the staged abstract interpreter we presented in Section
\ref{sai} to various control-flow analyses (CFA). We first discuss
calling-context sensitive analysis for staged analyzers by tweaking the
allocation strategy. Then we describe how to implement store-widening with
staged monads, which is a common optimization in CFA. Finally, we integrate
abstract garbage collection with staging. All of them are in fact orthogonal to
staging.

\subsubsection{Context-sensitivity}

The abstract addresses we introduced in Section \ref{sai} are simply identical
to the variable names, which is a context-insensitive schema.  Through tweaking
the address allocation strategy, we can achieve various context-sensitive
analyses \cite{DBLP:conf/icfp/Gilray0M16}. In this section, we demonstrate a
calling-context-sensitive analysis, i.e., $k$-CFA-like analysis
\cite{DBLP:journals/jfp/HornM12}, can be implemented our staged abstract
interpreter.
\begin{lstlisting}
  type Time = List[Expr]
  case class KCFAAddr(x: Ident, time: Time) extends Addr
\end{lstlisting}

The timestamp is essentially a finite list of expressions that tracks the $k$
recent calling contexts. The definition of abstract addresses @KCFAAddr@ is also
changed to include the identifier and the time when it get allocated, meaning that
this address points to some values under such calling context. If $k$ is 0, we
obtain a monovariant analysis as demonstrated before; if $k > 0$, we obtain a
family of analysis with increasing precision. Note that when $k = 0$, the
address space is statically determined by the set of variables appeared in the
program, and we may exploit it to optimize the generated code. But when $k > 0$,
the address generation happens at the next stage, i.e., analysis time.
\begin{lstlisting}
  type AnsM[T] = RepReaderT[RepStateT[
                   RepStateT[RepSetReaderStateM[Cache, Cache, ?], Time, ?], //add Time with a RepStateT
                 Store, ?], Env, T]
  type Result = (Rep[Set[(Value, Store, Time)]], Rep[Cache])
\end{lstlisting}

We then integrate the timestamp into our monad stack by adding a staged
@StateT@ monad. The result type is accordingly updated to contain a set of
tuples, where the time is added.
\begin{lstlisting}
  def tick(e: Expr): AnsM[Unit] = for {
    τ <- get_time; u <- liftM(StateTMonad.put((e::τ).take($k$)))
  } yield u
  def alloc(x: String): AnsM[Addr] = for {
    τ <- get_time
  } yield unit(KCFAAddr(x, τ)) // we use unit to turn a current-stage constant to next-stage value
\end{lstlisting}

Every time when we call the @eval@ function, we refresh the timestamp by
calling the @tick@ function, which updates the timestamp in the state monad.
The @tick@ function is implemented as prepending the current control expression
being evaluated (@e@) to the existing calling context, and then taking the
first $k$ elements from the list.  The type @Config@ used in caching is also
changed to include timestamps.

\subsubsection{Store-widened Analysis}

A commonly used technique to improve the running time in control-flow analysis
is store-widening. As shown by \cite{Darais:2015:GTM:2814270.2814308,
DBLP:journals/pacmpl/DaraisLNH17}, store-widening analysis can be easily
implemented in monadic abstract interpreter by swapping the @StateT@ (for store)
and the @SetT@ (for nondeterminism). In this section, we show this optimization
is orthogonal to staging and we just need to similarly swap the same \textit{staged}
version of monads. After the adjustment, we have the following monad stack:
\begin{lstlisting}
  type AnsM[T] = RepReaderT[RepSetT[RepStateT[
                   RepReaderT[RepStateT[IdM, Cache, ?], Cache, ?], Store, ?], ?], Env, T]
\end{lstlisting}

\subsubsection{Abstract Garbage Collection}

Abstract garbage collection is a technique to reclaim unreachable addresses in
the store while doing the abstract interpretation
\cite{Might:2006:IFA:1159803.1159807}. \citet{DBLP:journals/pacmpl/DaraisLNH17}
shows that, for big-step monadic abstract interpreter, we may add a @Reader@
monad to track a set of root addresses and compute the reachable addresses every
time when we obtain a value. Here, we adopt the idea with staged monads:
\begin{lstlisting}
  type AnsM[T] = RepReaderT[RepReaderT[
                   RepStateT[RepSetReaderStateM[Cache, Cache, ?], Store, ?],
                 Set[Addr], ?], Env, T] //add a reader monad for Set[Addr]
\end{lstlisting}

The idea of abstract garbage collection is shown in Figure \ref{fixgc}. We
update the @fix@ function with caching (from Section \ref{sai}) to @fix_gc@,
but the caching part is omitted in the presented code. Several auxiliary
functions are used: @ask_root@ retrieves the current root addresses from the
monad stack; @root@ computes the reachable addresses from a value; and @gc@
performs garbage collection according to the reachable addresses and returns a new
store. For the generic interpreter, we also need to compute the root addresses
for compound expressions, readers may refer to
\cite{DBLP:journals/pacmpl/DaraisLNH17}.

\vspace{-1em}
\begin{figure}[h!]
\begin{lstlisting}
  def fix_gc(ev: (Expr => Ans) => (Expr => Ans))(e: Expr): Ans = for {
    ψ <- ask_roots ... // omit code for retrieving ρ, σ, in and out
    val res: Rep[(Set[(Value,Store)], Cache)] = ...  // omit code for checking caches
    _ <- put_out_cache(res._2);  vs <- lift_nd(res._1)
    σ <- gc(ψ ⊔ root(vs._2));     _ <- put_store(σ)  // gc performs abstract garbage collection
  } yield v
\end{lstlisting}
\caption{\texttt{fix\_gc} for abstract garbage collection} \label{fixgc}
\end{figure}

%def root(v: Rep[Value]): Set[Addr]  def ask_root: AnsM[Set[Addr]]  def gc(ra: Rep[Set[Addr]]): AnsM[Store]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\vspace{-2em}
\subsection{Modular Staged Analysis for Free} \label{modular}

One of the challenges of modern static analysis is that program
usually depends on large libraries programs
\cite{toman_et_al:LIPIcs:2017:7121}. If we are able to analyze
programs and libraries separately and reuse the result without losing
precision, then we can reduce the overhead of repeatedly analyzing
libraries. Some static analyzers compute a summary for a function or a
module, which can be reused later. However, when lacking the context
information, those analyses can be too conservative or unsound, which
both lead to imprecision. In this section, we show that
meta-programming can provide a trade-off between summary-based modular
analysis and slow whole-program analysis, by utilizing an existing
whole-program analyzer.

As we have already seen from Section \ref{sai}, and recall that the
primitive operation @close@ can denote a $\lambda$ term to a next-stage Scala function.  The specializing of the staged abstract interpreter to
$\lambda$ terms is independent to other parts of the analyzed program.
\begin{lstlisting}
  def close(ev: Eval => Ans)(λ: Expr, ρ: Rep[Env]): Rep[Value]
\end{lstlisting}

Consider there is a library that contains a list map function @map@:
\begin{lstlisting}
  (define (map xs f) (if (null? xs) '() (cons (f (car xs)) (map (cdr xs) f))))
\end{lstlisting}

The specialization (0CFA and without store-widening) of @close@ on
@map@ generates the following next-stage Scala code, that is, the
function @x39@:
\begin{lstlisting}
  val x39 = {(x40: List[Set[AbsValue]],          // argument value list
              x41: Map[Addr, Set[AbsValue]],     // store
              x42: Map[Config, Set[ValSt]],      // in cache
              x43: Map[Config, Set[ValSt]]) =>   // out cache
    val x50 = List[Addr](ZCFAAddr("xs"), ZCFAAddr("f")) // addresses of arguments
    val x51 = x50.zip(x40)                              // addresses paired with their values
    val x61 = x51.foldLeft (x41) { case (x52, x53) =>   // prepare a new store that joins
      val x54 = x53._1;  val x55 = x53._2               //   the arguments and their values
        val x58 = x52.getOrElse(x54, x57)               //   into the latest store, initially x41
        val x59 = x58.union(x55);  x52 + (x54 -> x59) } 
    ... }
\end{lstlisting}

We borrow the notation from traditional modular analysis
\cite{DBLP:conf/cc/CousotC02}: such generated function is a
reusable artifact which can be used modularly, i.e., independent to its context. 
The generated functions already eliminate the interpretation overhead and
monadic overhead, therefore can be consider as partial summaries. Although, we
still need to run the generated function at next stage to obtain the analysis
result.  At next stage, we can provide different abstract arguments, store and
caches, depending on the context where it is called.

Compared with traditional summary-based analysis, our partial
summaries are represented by executable functions that are
parameterized to the context information. In our approach, the
transition from a whole-program analyzer to modular staged analyzer is
mechanized and for free via staging. We do not need to design any
intermediate representation or data structures of summaries.  In some
scenario, traditional summary-based analyses are able to carry more
information within a module, e.g., the summary of their internal
sub-modules. The staged approach presented here may still need to run
through the fixed-point for sub-modules.

\iffalse
\paragraph{Partially-Static Context} Under a polyvariant analysis (like
$k$-CFA), such summary generated by specializing static analyzer even could do
more for calling context by partially-static data optimization. For example,
consider that we have a library consists of three functions @f@, @g@ and @h@,
where @f@ calls @g@ and @h@ internally and returns @y@ finally (we extend the
language to have multiple arguments). @g@ and @h@ do not depend on @f@. Now we
would like to modularly specialize these functions, starting from @f@ with an
initial environment containing @f@, @g@ and @h@.
\begin{lstlisting}
  def f(a, b, c) = val x = g(a, b); val y = h(x, c);  y
\end{lstlisting}

Since $k$ is bounded to a fixed number, after the analysis running $k$ steps
inside of @f@, the calling context will purely depend on the static structure of
our module being analyzed.
\fi

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\iffalse
Another perspective: programs are data for an abstract interpreter, so if we
have $n$ programs, then maybe there can be $n$ stages. Probably we can analyze
first $m$ programs, and generate a residual abstract interpreter waiting for the
rest $(n-m)$ programs. These $(n-m)$ programs might be (abstract) arguments for
the first $n$ programs, and the abstract interpreter itself might be a partial
abstract interpreter.

\subsection{Numerical Analysis in Imperative Languages} \label{cases_imp}

Now we consider in a first-order imperative language, we may care more about the
data-flow because the control-flow is relatively easy to obtain. In this
section, we show the staging of other abstract domains, particularly an interval
domain for numbers. It has been shown that specializing abstract domains with
respect to the structure of analyzed program significantly improves the
performance: a recent example is online decomposition of polyhedron
\cite{DBLP:conf/popl/SinghPV17, Singh:2017:PCD:3177123.3158143}. In this
section, we first show how to support imperative language features in the
generic abstract interpreter. Then we present a similar idea for the interval
domain and show that the specialization is feasible by staging systematically.

\subsubsection{Scaling to Imperative Languages}

To evaluates an assignment, we first evaluate its right-hand side, and then put
the value into the slot where the address of @v@ points to. For simplicity, we
elect to make the value of the assignment be an @void()@ value.
\begin{lstlisting}
case Assign(x, e) =>
  val (v, $\sigma$_*) = ev(e, $\rho$, $\sigma$)
  (void(), put($\sigma$_*, get($\rho$, x), v))
\end{lstlisting}

To evaluates a @while@ loop statement, we evaluate the condition first. Then
similar to how we treat @branch0@, we have a generic @branch@ function but works
on Boolean values. For the @true@ branch, we recursively call @ev@ on a newly
constructed expression @Seq(e, While(t, e))@ meaning that first evaluates @e@
and the repeat the loop. Otherwise, for the other branch, we simply return a
void value and current store.
\begin{lstlisting}
case While(t, e) =>
  val (tv, t$\sigma$) = ev(t, $\rho$, $\sigma$)
  branch(tv, ev(Seq(e, While(t, e)), $\rho$, t$\sigma$), (void(), t$\sigma$))
\end{lstlisting}

\subsubsection{Staged Interval}

Interval domain is a relative simple domain which consists of two numeric
fields, an upper bound and a lower bound. Here we annotate the upper bound and
lower bound fields to be of type @Rep[Double]@. The operations such as @+@ of
two intervals produce a new
\begin{lstlisting}
case class Interval(lb: Rep[Double], ub: Rep[Double]) {
  def +(that: Interval): Interval = that match {
    case Interval(lb_, ub_) => Interval(lb + lb_, ub + ub_)
  }
  ...
}
\end{lstlisting}

\fi
