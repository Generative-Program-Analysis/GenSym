\section{Case Studies} \label{cases_study}

In Section~\ref{sai}, we use a toy language to show that staging an abstract
interpreter is feasible and can be systematically derived from staged concrete
interpreters and unstaged abstract interpreter.
In this section, we conduct several case studies to further show that this
methodology is practically useful and widely applicable to diverse analyses.

\subsection{Abstract Compilation \`a la Staging} \label{cs_ac}

We first revisit a similar technique called \textit{abstract compilation} (AC).
AC was introduced by \citet{Boucher:1996:ACN:647473.727587} as an
implementation technique for abstract interpretation based static analysis.
Similar to the present paper, the idea is inspired by partial evaluation:
the program is known statically and the interpretive overhead can be
eliminated. In AC, the compiled analysis can be represented by either text or
closures (higher-order functions). The minute difference is that the closures
can be executed immediately, while the textual programs need to be compiled and
loaded.

Specifically, \citeauthor{Boucher:1996:ACN:647473.727587} showed how to compile a
monovariant control-flow analysis \cite{Shivers:1991:SSC:115865.115884,
Shivers:1988:CFA:53990.54007} for continuation-passing style (CPS) programs.
Since the analyzed program is in CPS form, the analyzer is a big-step
control-environment abstract interpreter. The abstract compilation transforms
the analysis as a closure just taking an environment argument. Therefore the
overhead of traversing the AST of input program has been eliminated.

In this section, we show that \citeauthor{Boucher:1996:ACN:647473.727587}'s AC
(closure generation as example) can be understood and implemented as an instance
of staging abstract interpreters. We first revisit the original implementation
of abstract compilation of 0-CFA, and then reproduce their result by simply
adding stage annotations.
The generated program of our approach improves approximately the same extent of
speed, but without changing a single line of the analyzer program using
type-based staging annotations. However, AC requires more engineering effort,
i.e., rewriting the whole analyzer into the closure generation form. Moreover,
as shown in Section~\ref{staged_ds}, our approach is able to not only remove the
interpretive overhead, but also enabling several optimizations such specialized
data structures and equivalent rewritings.

\begin{figure*}
  \centering
  \begin{subfigure}[h]{0.49\textwidth}
    \centering
    \begin{lstlisting}[style=extrasmall]
type CompAnalysis = Store => Store
def compProgram(prog: Expr): CompAnalysis = compCall(prog)
def compCall(call: Expr): CompAnalysis = call match {
  case Letrec(bds, body) =>
    val C1 = compCall(body); val C2 = compArgs(bds.map(_.value))
    ($\sigma$: Store) => C1(C2($\sigma$.update(bds.map(_.name), 
       bds.map(b => Set(b.value.asInstanceOf[Lam])))))
  case App(f, args) =>
    val C1 = compApp(f, args); val C2 = compArgs(args)
    ($\sigma$: Store) => C1(C2($\sigma$))
}
def compApp(f: Expr, args: List[Expr]): CompAnalysis = f match {
  case Var(x) => ($\sigma$: Store) => 
    analysisAbsApp($\sigma$.lookup(x), args, $\sigma$)
  case Op(_) => compArgs(args)
  case Lam(vars, body) =>
    val C = compCall(body)
    ($\sigma$: Store) => C($\sigma$.update(vars, args.map(primEval(_, $\sigma$))))
}
def compArgs(args: List[Expr]): CompAnalysis = args match {
  case Nil => ($\sigma$: Store) => $\sigma$
  case (arg@Lam(vars, body))::rest =>
    val C1 = compCall(body); val C2 = compArgs(rest)
    ($\sigma$: Store) => C2(C1($\sigma$))
  case _::rest => compArgs(rest)
}
  \end{lstlisting}
  \end{subfigure}
\hfill
  \begin{subfigure}[h]{0.49\textwidth}
    \centering
    \begin{lstlisting}[style=extrasmall]
def analyzeProgram(prog: Expr, $\sigma$: Rep[Store]): Rep[Store] = 
  analyzeCall(prog, $\sigma$)
def analyzeCall(call: Expr, $\sigma$: Rep[Store]): Rep[Store] = 
  call match {
    case Letrec(bds, body) =>
      val $\sigma$_* = $\sigma$.update(bds.map(_.name), 
        bds.map(b => Set(b.value.asInstanceOf[Lam])))
      val $\sigma$_** = analyzeArgs(bds.map(_.value), $\sigma$_*)
      analyzeCall(body, $\sigma$_**)
    case App(f, args) => analyzeApp(f, args, analyzeArgs(args, $\sigma$))
  }
def analyzeApp(f: Expr, args: List[Expr], $\sigma$: Rep[Store]): 
  Rep[Store] = f match {
    case Var(x) => analyzeAbsApp(args, $\sigma$(x), $\sigma$)
    case Op(_) => analyzeArgs(args, $\sigma$)
    case Lam(vars, body) =>
      val $\sigma$_* = $\sigma$.update(vars, args.map(primEval(_, $\sigma$)))
      analyzeCall(body, $\sigma$_*)
  }
def analyzeArgs(args: List[Expr], $\sigma$: Rep[Store]): Rep[Store] = 
  args match {
    case Nil => $\sigma$
    case Lam(vars, body)::rest => 
      analyzeArgs(rest, analyzeCall(body, $\sigma$))
    case _::rest => analyzeArgs(rest, $\sigma$)
  }
  \end{lstlisting}

  \end{subfigure}
  \caption{Comparison of AC (left) and SAI (right). Only core code are shown.}
  \label{compare_ac_sai}
\end{figure*}

\paragraph{Original AC}
The analysis presented by \citeauthor{Boucher:1996:ACN:647473.727587} is 0-CFA
for a small CPS language consisting of abstractions, applications, recursion and
primtive operators. Figure \ref{compare_ac_sai} (left) shows the AC
implementation: different syntactic constructs are handled in different functions
(e.g., @compCall@, @compApp@, etc.).
Before refactoring into AC, these functions would take the store as a
regular argument; now every function returns a value of type @CompAnalysis@,
which is a function (closure) value that takes and returns stores. After the
first run of analysis, we have traversed the AST and obtained a single closure
takes a store and returns a store. Therefore, the ASTs are static terms, and the
stores are dynamic terms. Now we can invoke the compiled closure with an initial
store to complete the analysis.

\paragraph{AC through Staging}

On the other side, our approach does exactly the same thing through staging: the
AST are static, and stores are dynamic, therefore we annotate the type @Store@
with @Rep@ (shown in Figure \ref{compare_ac_sai} (right)). In fact, the only
changes of our appraoch using LMS framework are the types of @Store@ to
@Rep[Store]@, since they will be known at the next stage; other parts do not need to change. 
The generated code
will just looks-up and update the store. The functions at current stage are
eliminated in the residual program, therefore we do not generate higher-order
functions, which provides additional performance improvement. Nevertheless,
the support from LMS framework is necessary, such as staging support for @Map@,
we consider these efforts are reusable and complete orthogonal to the
implementation of analysis.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Control-flow Analysis} \label{cfa}

In this section, to demonstrate the SAI approach is widely applicable, we extend
and refactor the staged abstract interpreter we presented in Section \ref{sai}
to various control-flow analysis (CFA). We first discuss calling-context
sensitive analysis for staged analyzers by tweaking allocation strategy; then we
will see how to implement store-widening with staged monads, which is a common
optimization in CFA; finally, we integrate abstract garbage collection with
staging.

\subsubsection{Context-sensitivity}

The abstract addresses we introduced in Section \ref{sai} is simply identical to
the variable names, which is a context-insensitive schema.
Through tweaking the address allocation strategy, we can achieve various
context-sensitive analysis \cite{DBLP:conf/icfp/Gilray0M16}. In this section, we
demonstrate a calling-context-sensitive analysis, i.e., $k$-CFA-like analysis
\cite{DBLP:journals/jfp/HornM12}, in our staged abstract interpreter.

\begin{lstlisting}
  type Time = List[Expr]
  case class KCFAAddr(x: Ident, time: Time) extends Addr
\end{lstlisting}

The timestamp is essentially a finite list of expressions that tracks the $k$
recent calling contexts. The definition of abstract addresses @KCFAAddr@ is also
changed to contain a identifier and the time when it get allocated, meaning that
this address points to some values under such calling context. If $k$ is 0, we
obtain a monovariant analysis as demonstrated before; if $k > 0$, we obtain a
family of analysis with increasing precision.

\begin{lstlisting}
  type AnsM[T] = RepReaderT[RepStateT[
                   RepStateT[RepSetReaderStateM[Cache, Cache, ?], Time, ?], //Add Time into a RepStateT
                 Store, ?], Env, T]
  type Result = (Rep[Set[(Value, Store, Time)]], Rep[Cache])
\end{lstlisting}

We then integrate the timestamp into our moand stack by adding a staged
@StateT@ monad. The result type is passingly get updated to contain a set of
tuples, where the time is added.

\begin{lstlisting}
  def tick(e: Expr): AnsM[Unit] = for {
    τ <- get_time; u <- liftM(StateTMonad.put((e::τ).take($k$)))
  } yield u
  def alloc(x: String): AnsM[Addr] = for {
    τ <- get_time
  } yield KCFAAddr(x, τ)
\end{lstlisting}

Every time when we call the @eval@ function, we refresh the timestamp by calling
the @tick@ function, which update the timestamp in the state monad. The @tick@
function is implemented as prepending the current expression being evaluated @e@
to the existing calling context, and then taking the first $k$ elements from the list.
The type @Config@ used in caching is also changed to included the timestamp.

\subsubsection{Store-widened Analysis}

An commonly used technique to improve the running time in control-flow analysis
is store-widening. As shown by \cite{Darais:2015:GTM:2814270.2814308,
DBLP:journals/pacmpl/DaraisLNH17}, store-widening analysis can be easily
implemented in monadic abstract interpreter by swaping the @StateT@ (for store)
and the @SetT@ (for nondeterminism). In this section, we show this optimization
is orthogonal to staging and we just need to swap the same \textit{staged}
version of monads. After the adjustment, we have the following monad stack:

\begin{lstlisting}
  type AnsM[T] = RepReaderT[RepSetT[RepStateT[
                   RepReaderT[RepStateT[IdM, Cache, ?], Cache, ?], Store, ?], ?], Env, T]
\end{lstlisting}



\subsubsection{Abstract Garbage Collection}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Modular Analysis for Free} \label{modular}

% using different k?

One of the challenges of modern static analysis is program usually depends on
large libraries programs~\cite{toman_et_al:LIPIcs:2017:7121}. If we can analyze
programs and libraries separately and reuse the result without losing precision,
then we can reduce the overhead of repeatedly analyzing libraries.
Some static analyzers compute a summary for a function or a module, which
can be reused later, however they are mostly too conservative or unsound, which
both lead to imprecision. In this section, we show meta-programming can be used
to create sound summaries in the code generation from, such generated code can
be resued later. The summary contains information that not depend on the
function argument.

As we see when compiling a higher-order function (closure), we specialize the
abstract interpreter to the body expression of that lambda term under some
environment, without knowing the actual arguments to the function. We may reuse
the infrastratures and refactor the @close@ function mentioned in Section
\ref{sai} to achieve modular analysis. The @close@ function takes a lambda term
@f@ and an environment, produces a compiled closure, which can be freely used at
the next stage.

\begin{lstlisting}
  def close(ev: Eval => Ans)(f: Expr, ρ: Rep[Env]): Rep[CompiledClo] = { ...}
\end{lstlisting}

\paragraph{Partially-Static Context} Under a polyvariant analysis (like
$k$-CFA), such summary generated by specializing static analyzer even could do
more for calling context by partially-static data optimization. For example,
consider that we have a library consists of three functions @f@, @g@ and @h@,
where @f@ calls @g@ and @h@ internally and returns @y@ fianlly (we extend the
language to have multiple arguments). @g@ and @h@ do not depend on @f@. Now we
would like to modularlly specialize these functions, starting from @f@.

\begin{lstlisting}
  def f(a, b, c) = val x = g(a, b); val y = h(x, c);  y
\end{lstlisting}

Since $k$ is bounded to a fixed number, after the analysis running $k$ steps
inside of @f@, the calling context will purely depend on the static structure of
our module being analyzed.

\iffalse
Another perspective: programs are data for an abstract interpreter, so if we
have $n$ programs, then maybe there can be $n$ stages. Probably we can analyze
first $m$ programs, and generate a residual abstract interpreter waiting for the
rest $(n-m)$ programs. These $(n-m)$ programs might be (abstract) arguments for
the first $n$ programs, and the abstract interpreter itself might be a partial
abstract interpreter.

\subsection{Numerical Analysis in Imperative Languages} \label{cases_imp}

Now we consider in a first-order imperative language, we may care more about the
data-flow because the control-flow is relatively easy to obtain. In this
section, we show the staging of other abstract domains, particularly an interval
domain for numbers. It has been shown that specializing abstract domains with
respect to the structure of analyzed program significantly improves the
performance: a recent example is online decomposition of polyhedra
\cite{DBLP:conf/popl/SinghPV17, Singh:2017:PCD:3177123.3158143}. In this
section, we first show how to support imperative language features in the
generic abstract interpreter. Then we present a similar idea for the interval
domain and show that the specialization is feasible by staging systematically.

\subsubsection{Scaling to Imperative Languages}

To evaluates an assignment, we first evaluate its right-hand side, and then put
the value into the slot where the address of @v@ points to. For simplicity, we
elect to make the value of the assignment be an @void()@ value.

\begin{lstlisting}
case Assign(x, e) =>
  val (v, $\sigma$_*) = ev(e, $\rho$, $\sigma$)
  (void(), put($\sigma$_*, get($\rho$, x), v))
\end{lstlisting}

To evaluates a @while@ loop statement, we evaluate the condition first. Then
similar to how we treat @branch0@, we have a generic @branch@ function but works
on boolean values. For the @true@ branch, we recursively call @ev@ on a newly
constructed expression @Seq(e, While(t, e))@ meaning that first evaluates @e@
and the repeat the loop. Otherwise, for the other branch, we simply return a
void value and current store.

\begin{lstlisting}
case While(t, e) =>
  val (tv, t$\sigma$) = ev(t, $\rho$, $\sigma$)
  branch(tv, ev(Seq(e, While(t, e)), $\rho$, t$\sigma$), (void(), t$\sigma$))
\end{lstlisting}

\subsubsection{Staged Interval}

Interval domain is a relative simple domain which consists of two numeric
fields, an upper bound and a lower bound. Here we annotate the upper bound and
lower bound fields to be of type @Rep[Double]@. The operations such as @+@ of
two intervals produce a new

\begin{lstlisting}
case class Interval(lb: Rep[Double], ub: Rep[Double]) {
  def +(that: Interval): Interval = that match {
    case Interval(lb_, ub_) => Interval(lb + lb_, ub + ub_)
  }
  ...
}
\end{lstlisting}

\fi
