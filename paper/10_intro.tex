\section{Introduction}

Futamura projections \cite{Futamura1999, futamura1971partial} reveal the close
connection between compilers and interpreters. The first Futamura projection
specifically shows that specializing an interpreter with respect to the input
program yields an equivalent executable. Partial evaluation
\cite{DBLP:books/daglib/0072559} was the first proposed approach to realize
Futamura projections, which first identifies the binding-time of variables in
the program, i.e., they can be known whether statically or dynamically, and then
evaluates the static part, and finally generates a residual program that solely
relies on the dynamic part. However, given an arbitrary program, precisely
analyzing its binding-time is hard in general. As an alternative and pragmatic
approach to specialization and partial evaluation, multi-stage programming (MSP
for short) \cite{taha1999multi, DBLP:conf/pepm/TahaS97} requires the stage
annotations (i.e., binding-time annotations) to be explicit from the programmers.
These staging annotations identify which part of the inputs should be
specialized. As a classical example, we use the power function to introduce
the idea of MSP:

\begin{lstlisting}
def power(b: Rep[Int], e: Int): Rep[Int] = 
  if (e == 0) 1 else e * power(b, e-1)
\end{lstlisting}

If the programmer identifies that @b@ will be known in the future stage (as shown
on its type @Rep[Int]@ -- a representation of @Int@) and @e@ is known at the
current stage, say 5, then we can specialize the function @power@ with
respect to @e = 5@, and generate a specialized function where the overhead of
recursion is eliminated. The following is the generated code:

\begin{lstlisting}
def power5(b: Int) = {
  val x1 = x0 * x0; val x2 = x0 * x1; val x3 = x0 * x2; val x4 = x0 * x3
  x4
}
\end{lstlisting}

Now to concretely illustrate the idea of Futamura projection, consider that we
have an interpreter @eval@ of some language as following:

\begin{lstlisting}
def eval(e: Expr)(arg: Input): Value = ...
\end{lstlisting}

where $e$ is the program of type @Expr@ and @arg@ is the input to program $e$,
and @eval@ produces a value.
Semantically, the interpreter satisfies $ \texttt{eval}(e)(arg) = [\![ e ]\!] arg$ for all
programs and inputs. Given a program $e_0$, by applying the first Futamura
projection, we may obtain a specialized interpreter
$\texttt{eval}_{\texttt{e0}}$ with respect to $e_0$, and by definition of the interpreter, 
$\texttt{eval}_{\texttt{e0}}(arg) = [\![ e_0 ]\!] arg $. If we generalize the
argument @arg@ to an environment (as well as a companion heap object, if for the
interest of using assginments) that contains values mapped from free variables
in $e$, the interpreter is a standard environment/store-passing interpreter,
where the specialization through MSP is also applicable.

Roughly at the same time when Futamura projections was visioned in 1970s,
\citeauthor{DBLP:conf/popl/CousotC77} proposed abstract interpretation as a
lattice-based semantic approach to construct sound static analysis, by
approximation of fixpoints \cite{DBLP:conf/popl/CousotC77}. However,
constructing sound abstract interpreters was considered abstruse and complicated
for a long time.
Recent progress such as Abstracting Abstract Machines (AAM)
uncovers a systematic method to derive sound abstract interpreters from their
concrete counterparts, where the soundness can be easily established by
examining the transformation of semantics.
The AAM approach also has been been applied to different variants of
definitional interpreters and abstract machines \cite{DBLP:journals/jfp/HornM12,
DBLP:conf/icfp/HornM10, DBLP:journals/pacmpl/DaraisLNH17}.
% \todo{CEK example?}

Now we have seen two orthogonal transformations of interpreters: by
specliazation, we can refactor interpreters to code generators; by approximation,
we can refactor interpreters to static analyzers. As an intellectual quest, it
is natural to ask --- starting from an interpreter, \textbf{can we derive a
sound abstract interpreter but it works like a code generator which produces fast
analysis code?} Conceptually, it is similar to the concrete setting: for an
abstract interpreter $\widehat{\texttt{eval}}: \texttt{Expr} \to
\widehat{\texttt{Env}} \to \widehat{\texttt{Value}}$, which takes a program, an abstract
environment and returns abstract values, we would like to specialize it with
repsect to a program $e_0$ and produce an code generator
$\widehat{\texttt{eval}}_{\texttt{e0}} : \widehat{\texttt{Env}} \to
\widehat{\texttt{Value}}$, such that $
\widehat{\texttt{eval}}_{\texttt{e0}}(\widehat{\rho}) =
\widehat{\texttt{eval}}(e_0)(\widehat{\rho})$, but the staged one leaves the
abstract environment $\widehat{\rho}$ dynamic.

\begin{figure}[h]
  \begin{tikzpicture}
  \matrix (m) [matrix of math nodes,row sep=3em,column sep=3em,minimum width=2em]
  {
    \begin{smallmatrix} \text{unstaged} \\ \text{abstract} \\ \text{interpreters} \end{smallmatrix} & \begin{smallmatrix} \text{staged} \\ \text{abstract} \\ \text{interpreters} \end{smallmatrix} \\
      \begin{smallmatrix} \text{unstaged} \\ \text{concrete} \\ \text{interpreters} \end{smallmatrix} & \begin{smallmatrix} \text{staged} \\ \text{concrete} \\ \text{interpreters} \end{smallmatrix} \\ };
  \path[-stealth]
    (m-1-1) edge node [above, font=\scriptsize] {$\updownarrows$} (m-1-2)
    (m-2-1) edge node [left, font=\scriptsize] {$\alpha$} (m-1-1)
    (m-2-1) edge node [below, font=\scriptsize] {$\updownarrows$} (m-2-2)
    (m-2-2) edge node [right, font=\scriptsize] {$\alpha$} (m-1-2);
  \end{tikzpicture}
  \caption{The confluence of specialization and approximation.}
  \label{confluence}
\end{figure}

In this paper, we study the confluence of two old ideas --- Futamura projection
and abstract interpretation, but from the perspective of their recent
realizations --- multi-stage programming and definitional abstract interpreters.
To be specific, we present the application of the first Futamura projection on
definitional abstract interpreters, which intersects as staged abstract
interpreters. By borrowing the ideas from monadic interpreters
\cite{Steele:1994:BIC:174675.178068, DBLP:conf/popl/LiangHJ95,
DBLP:journals/pacmpl/DaraisLNH17, Sergey:2013:MAI:2491956.2491979} and embedded
domain-specific languages \cite{DBLP:conf/snapl/RompfBLSJAOSKDK15,
DBLP:journals/jfp/CaretteKS09, DBLP:conf/icfp/GibbonsW14}, we develop an
approach to fulfill four different semantics (Figure~\ref{confluence}) in a
unified framework. The four semantics share one same generic interpreter.
However, by instantiating the interpreter differently, it can behave as a
concrete interpreter, an abstracter interperter, or their code-generator
versions, respectively. The key idea that enables this flexibility is to use
abstract type members abstract over concrete/abstract components (e.g., concrete
values or abstract values), as well as the binding-time of them (e.g., static or
dyanmic). By staging, the overhead caused by the monadic layers is eliminated in
the generated code.

Besides the intellectual merit shown in the confluence diagram, this paper
contributes to a practical problem in static analysis: \textbf{can we speed up a
static analyzer without any intrusive changes and soundness compromises?} Static
analysis is known as a trade-off game between \textit{precision} and
\textit{efficiency}, we argue that applying meta-programming (multi-stage
programming, in particular) with abstract interpreters is an effective approach
to improve the latter one while the former one is untouched. Specialization
removes the overhead of abstractions introduced by high-level programming, but
not changes the meaning of the program -- in this case, the abstract semantics
implemented by the analyzer. Indeed, specializing analysis with respect to a
program by partial evaluation \cite{damian1999partial, amtoft1999partial,
Boucher:1996:ACN:647473.727587, ashley:practical} is not a new idea, despite the
fact that most of the previous work are ad-hoc on a particular analysis or
requiring significant changes to the analysis. For example, abstract compilation
\cite{Boucher:1996:ACN:647473.727587} requires the whole analyzer to be
rewritten as closure generation from. In this paper, we show a systematic
approach to meta-programming achieves the performance goal with less intrusive
changes; in fact, a case study (Section~\ref{cs_ac}) comparing with abstract
compilation shows, by utilizing type-based stage annotations, the analyzer
program does not need any changes.

\todo{modular analysis: a whole-program abstract
  interpreter can be run on different components of program separately, we
  achieve this by meta-programming mechnically}

\iffalse
Likewise, specializing static analyses by partial evaluation emerged in late 90s
, and indeed it is able to effectively remove the interpretive
overhead of repeatedly traversing the abstract syntax tree. However, these
previous works focused mostly on one particular analysis, or required to
completely rewrite the analyzer. Hence, it is worth to investigate the idea from
a modern perspective based on generative programming, especially for a general
abstract interpreter that models direct-style $\lambda$-calculus, imperative
features such as mutation, and different abstract domains. One technical
challenge is the binding-time engineering when non-determinism, fixed-point
iterations, different abstract domains and staging are introduced at the same
time. In this paper, we present an end-to-end design and implementation of
staging an abstract interpreter; that means not only the interpreter that
traverses the abstract syntax tree, but also the data structure of abstract
domains, abstract environment and heap, and the fixed-point iteration are
staged.

\todo{rewrite this paragraph}
On the other hand, the abstract interpreter, as a semantic artifact, should be
written in a style that is easy for people to communicate the formulation and
abstraction, but also can be implemented efficiently. As the slogan of
multi-stage programming said, "abstraction without regret", we draw connection
between high-level description and efficient implementation of abstract
interpreters, just like the connection between concrete interpreters and
compilers drawn by Futamura projection. Particularly, we show an easy but
systematic way of adding stage annotations to the abstract interpreter without
changing the code of the interpreter skeleton, which is shared between four concrete
or abstract, unstaged or staged interpreters. We use the LMS framework for staging,
which allows us only use types to annotate the binding-time. Therefore, the
proposed approach bridges the gap between designing sound static analysis and
implementing efficient program analyzer.

%Of course, all interpreters are metalinguistic abstractions, but some
%interpreters are more "abstract" than others \todo{maybe rephrase}.
%Particularly, we also would like a systematic approach to optimize program
%analyzers and meanwhile minimally modifying the analyzer programs.

When staging a concrete interpreter, the programmers need to distinguish static
and dynamic values --- the given program to be executed by the interpreter is
classified as static because it is known at compile-time, and the inputs to that
program are dynamic. However, when staging an abstract interpreter, this
distinction does not exist anymore. Because the abstract interpreter
instantiates all the inputs as some form of abstract values, which are usually
top elements in their abstract domains and are also statically known. Then what
is the point of staging if there is no such distinction? A surprising by-product
of thinking about this question is to realize that we can apply the staged
abstract interpreter on \textit{open} programs, and the free variables
representing other parts of the program (e.g., libraries) are dynamic inputs,
therefore we obtain a modular analysis through staging, mechanically.
\note{TR: I don't understand this. The program structure is static, the abstract values
are still dynamic, no? They change in every iteration of the fixpoint algorithm}
\note{TR: why is it a big deal. On Closed programs we obtain a constant factor,
but modular analysis has different asymptotics}
\fi

When targeting higher-order programs, either staged concrete interpreters or
staged abstract interpreters are able to compile a closure, i.e., specialize the
call of interpreter with respect to the body expression of the lambda term
without knowing the actual argument value. By generalizing this observation, we
can actually specialize the abstract interpreter with any open programs, which
unexpectedly leads to a modular analysis and improves scalability. An open
program contains free variables, which represent other parts of the program and
will be left as dynamic values. For instance, one challenge in static analysis
of modern software is that the programs are usually shipped with large library
code \cite{toman_et_al:LIPIcs:2017:7121}, for example, it has been shown that
analyzing a simple "Hello World" program in Java depends on additional 3,000
classes in the library \cite{DBLP:conf/oopsla/KulkarniMZN16}. A precise
whole-program analysis formulated as abstract interpreters can be very expensive
due to the scale of the program and the inherent complexity of the algorithm.
However, analyzing these libraries is sometimes unnecessarily repeated. When
applying the staged abstract interpreter on such open programs, we leave the
unknown arguments and calling contexts as dynamic values and generate partial
analyzing result which is represented as a residual program. The partial
analyzing result can be reused and composed with the analyzing result of
application code when available later. Therefore we mechanically obtain a
modular analysis through staging, even though the original algorithm is
formulated as a whole-program analysis.

\iffalse
It has been observed that partially applying context-sensitivity on selected
portion of the program could improve the precision and efficiency
\cite{zipper2018, Kastrinis:2013:HCP:2491956.2462191}. We show that staging
abstract interpreters as an approach to effectively implement hybird
context-sensitivity\todo{}.
\fi

\note{
  Expression problem \cite{DBLP:conf/icfp/GibbonsW14, DBLP:conf/ecoop/KrishnamurthiFF98, expproblem},
  deep embedding, four different interpretations.
}

\paragraph{Contributions and Outline}

\begin{itemize}
  \item We progressively present a unifying construction that naturally extends the
    first Futamura projection to abstract interpreters. Starting from a generic
    interface of the interpreter for a tiny functional language
    (Section~\ref{generic_if}), we first show the instantiation of concrete
    interpretation in Section~\ref{unstaged_conc}. Then we lift it to the staged
    version by adding stage annotations to the semantic operations
    (Section~\ref{stagedinterp}); and to the abstract interpreter by applying
    abstraction to the environment, store, and values. Finally, we show the
    combination of the two transformations, dubbed \textit{staged abstract
    interpreters}, can be easily derived (Section~\ref{sai}).
    
  \item \todo{modular analysis} As shown in Section~\ref{sai}, we demonstrate
    that if we apply the abstract interpreters on open programs, it not only
    improves the \textit{efficiency} but also the \textit{scalability}, which are
    two major issues in static analysis.
    
  \item In Section~\ref{cases_study}, we conduct three case studies to
    demonstrate the applicability and usefulness of the proposed approach to
    specialize analyses.
    (1) We first revisit the Abstract Compilation (AC)
    \cite{Boucher:1996:ACN:647473.727587} technique in Section~\ref{cs_ac}.
    Our approach and AC are able to achieve the same goal, however, as we show,
    one of the advantage of staging through types is it does not require
    whole-analyzer refactoring.
    (2) We extend the staged abstract interpreter shown in Section~\ref{sai} to
    two variants of control-flow analysis: context-sensitive pushdown
    control-flow analysis and \todo{precise-store?}.
    (3) In Section~\ref{cases_imp}, we plug a \note{staged numerical domain}
    \todo{widening} into the staged abstract interpreter, demonstrating that a
    numerical analysis on imperative languages can be also implemented and
    speeded-up in this manner.
    
  \item Based on the instantiation of control-flow analysis, we empirically
    evaluate the performance improvement by staging and specialization
    (Section~\ref{evaluation}). We compare the both context-insensitive and
    \note{context-sensitive} analysis.
    
\end{itemize}
