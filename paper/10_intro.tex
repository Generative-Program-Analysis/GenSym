\section{Introduction} \label{intro}

Futamura projections \cite{Futamura1999, futamura1971partial} reveal the close
connection between compilers and interpreters. The first Futamura projection
specifically shows that specializing an interpreter with respect to the input
program yields an equivalent executable. Partial evaluation
\cite{DBLP:books/daglib/0072559} was the first proposed approach to realize
Futamura projections: it identifies the binding-time of variables in
the program (i.e., they can be known statically or dynamically),
evaluates the static part, and finally generates a residual program that solely
relies on the dynamic part. However, given an arbitrary program, precisely
analyzing its binding-times is hard in general. As an alternative and pragmatic
approach to specialization and partial evaluation, multi-stage programming (MSP
for short) \cite{taha1999multi, DBLP:conf/pepm/TahaS97} requires the staging
annotations (i.e., binding-time annotations) to be explicit from the
programmers. The staging annotations can be either syntactic (as in
MetaML/MetaOCaml, through quasi-quote, escape, etc.) or type-based \todo{cite}.
These staging annotations identify which part of the inputs should be
specialized. As a classical example, we use the power function and type-based
annotations to introduce the idea of MSP:

\begin{lstlisting}
  def power(b: Rep[Int], e: Int): Rep[Int] = if (e == 0) 1 else b * power(b, e - 1)
\end{lstlisting}

If the programmer identifies that @b@ will be known in the future stage (as shown
on its type @Rep[Int]@ -- a representation of @Int@) and @e@ is known at the
current stage, say 5, then we can specialize the function @power@ with
respect to @e = 5@, and generate a specialized function where the overhead of
recursion is eliminated. Conceptually, the generated code looks like the following:

\begin{lstlisting}
  def power5(b: Int): Int = b * b * b * b * b
\end{lstlisting}

Now to concretely illustrate the idea of Futamura projection, consider that we
have an operational interpreter @eval@ of some language:

\begin{lstlisting}
  def eval(e: Expr)(arg: Input): Value = ...
\end{lstlisting}

where $e$ is the program of type @Expr@ and @arg@ is the input to program $e$,
and @eval@ produces a value.
Semantically, the interpreter satisfies $ \texttt{eval}(e)(arg) = [\![ e ]\!] arg$ for all
programs and inputs. Given a program $e_0$, by applying the first Futamura
projection, we may obtain a specialized interpreter
$\texttt{eval}_{\texttt{e0}}$ with respect to $e_0$ -- the so-called \textit{equivalent executable}. 
By definition of the interpreter, 
$\texttt{eval}_{\texttt{e0}}(arg) = [\![ e_0 ]\!] arg $. If we generalize the
argument @arg@ to an environment (as well as an accompanying heap object to
support mutable variables) that maps free variables in $e$ to some
values, the interpreter is a standard environment-passing interpreter,
where the specialization through MSP is also applicable.

Roughly at the same time when Futamura projections was envisioned in 1970s,
\citet{DBLP:conf/popl/CousotC77} proposed abstract interpretation as a
lattice-based semantic approach to construct sound static analysis, by
approximation of fix-points. However, constructing sound abstract interpreters
was considered abstruse and complicated for a long time.
Recent progress such as Abstracting Abstract Machines (AAM)
\cite{DBLP:conf/icfp/HornM10} uncovers a systematic method to derive sound
abstract interpreters from their concrete counterparts, where the soundness can
be easily established by examining the transformation of semantics. The AAM
approach also has been been applied to different variants of definitional
interpreters and abstract machines \cite{DBLP:journals/jfp/HornM12,
DBLP:conf/icfp/HornM10, DBLP:journals/pacmpl/DaraisLNH17}.

\paragraph{Futamura Projection of Abstract Interpreters}

Now we have seen two orthogonal transformations of interpreters: by
specialization, we can refactor interpreters to code generators; by
approximation, we can refactor interpreters to static analyzers. As an
intellectual quest, we should naturally ask -- starting from an abstract
interpreter, \textit{can we derive a sound abstract interpreter while it
produces optimized code that does the analysis?}
Conceptually, it is similar to the concrete setting: for an
abstract interpreter $\widehat{\texttt{eval}}: \texttt{Expr} \to
\widehat{\texttt{Env}} \to \widehat{\texttt{Value}}$, which takes a program, an abstract
environment and returns abstract values, we would like to specialize it with
respect to some program $e_0$ and produce a compiled analysis
$\widehat{\texttt{eval}}_{\texttt{e0}} : \widehat{\texttt{Env}} \to
\widehat{\texttt{Value}}$, such that $
\widehat{\texttt{eval}}_{\texttt{e0}}(\widehat{\rho}) =
\widehat{\texttt{eval}}(e_0)(\widehat{\rho})$, but the compiled one leaves the
abstract environment $\widehat{\rho}$ as remaining input.

\vspace{-1em}
\begin{figure}[h]
  \begin{tikzpicture}
  \matrix (m) [matrix of math nodes,row sep=3em,column sep=3em,minimum width=2em]
  {
    \begin{smallmatrix} \text{unstaged} \\ \text{abstract} \\ \text{interpreters} \\ (\text{Sec.}~\ref{unstaged_abs}) \end{smallmatrix} &
    \begin{smallmatrix} \text{staged} \\ \text{abstract} \\ \text{interpreters} \\ (\text{Sec.}~\ref{sai}) \end{smallmatrix} \\
    \begin{smallmatrix} \text{unstaged} \\ \text{concrete} \\ \text{interpreters} \\ (\text{Sec.}~\ref{prelim}) \end{smallmatrix} &
    \begin{smallmatrix} \text{staged} \\ \text{concrete} \\ \text{interpreters} \\ (\text{Sec.}~\ref{stagedinterp}) \end{smallmatrix} \\ };
  \path[-stealth]
    (m-1-1) edge node [above, font=\scriptsize] {$\updownarrows$} (m-1-2)
    (m-2-1) edge node [left, font=\scriptsize] {$\alpha$} (m-1-1)
    (m-2-1) edge node [below, font=\scriptsize] {$\updownarrows$} (m-2-2)
    (m-2-2) edge node [right, font=\scriptsize] {$\alpha$} (m-1-2);
  \end{tikzpicture}
  \caption{The confluence of specialization and approximation.}
  \label{confluence}
\end{figure}
\vspace{-1em}

In this paper, we study the confluence of two old ideas --- Futamura projection
and abstract interpretation, but from the perspective of their recent
realizations --- multi-stage programming and definitional abstract interpreters.
To be specific, we present the application of the first Futamura projection on
definitional abstract interpreters, which intersects as staged abstract
interpreters. We develop a unified approach to fulfill the four different
semantics, as shown in Figure~\ref{confluence}. Starting with a generic but
parametric interpreter shared by the four semantics, by simply instantiating
the interpreter differently, we can automatically derive a concrete interpreter,
or an abstract interpreter, or their code-generator versions, respectively.

\iffalse
embedded domain-specific languages \cite{DBLP:conf/snapl/RompfBLSJAOSKDK15,
DBLP:journals/jfp/CaretteKS09, DBLP:conf/icfp/GibbonsW14,
Hofer:2008:PED:1449913.1449935},
The key idea that enables this flexibility is to use abstract type members first
abstract over concrete/abstract components (e.g., concrete values or abstract
values), then as well as the binding-time of them (e.g., static or dynamic). By
staging, the overhead caused by the monadic layers is eliminated in the
generated code.
\fi

\paragraph{Abstraction without Regret}

To enable such flexibility between different semantics, we combine generic 
programming and generative programming, and extensively use abstract type members 
over the following abstractions, which can be later instantiated.

\begin{itemize}
\item Monadic abstractions, which encapsulate the following two abstractions.
  The interpreter itself is written in monadic form, and by plugging different monads, 
  the underlying semantics of the interpreter is changed.
  \todo{mention that direct-style also works?}
  %\cite{Steele:1994:BIC:174675.178068, DBLP:conf/popl/LiangHJ95, DBLP:journals/pacmpl/DaraisLNH17, Sergey:2013:MAI:2491956.2491979}
\item Env-store-value abstractions, which control whether the interpreter
  behaves as a concrete interpreter or an abstract interpreter.
  The environments, stores and values are initially declared as abstract types in the generic interpreter.
  As shown by the AAM approach \cite{DBLP:journals/jfp/HornM12,
  DBLP:conf/icfp/HornM10}, if we widen the values to abstract domains (such as
  intervals) and refactor the store to a mapping from finite addresses to sets of
  values, we can derive a computable and sound abstract interpreter.
\item Binding-time abstractions, which control whether the interpreter produces
  values or generates code. The binding-times differentiate whether data are 
  known statically or dynamically. Unsurprisingly, such binding-time
  information can be encoded into higher-kinded types 
    \cite{DBLP:journals/jfp/CaretteKS09, Ofenbeck:2017:SGP:3136040.3136060}.
\end{itemize}

We show that these well-known abstractions can be combined smoothly, and more
importantly the price is almost free. Through staging, the major overhead of
abstractions -- from the monadic layers -- is completely eliminated in the
generated code. \todo{put some numbers} However, we do not lose any advantages of this high-level
programming style (e.g., modularity and extensibility); the meaning of the program also
remains the same, particularly, the abstract semantics that does the analysis
when instantiated as an abstract interpreter.

\paragraph{Performance via Specialization}

Besides the intellectual merit shown in the confluence diagram, this paper
contributes to a practical problem in static analysis: \textit{can we speed up a
static analyzer without any intrusive changes and soundness compromises?} Static
analysis is known as a trade-off game between \textit{precision} and
\textit{efficiency}, we argue that applying meta-programming (multi-stage
programming, in particular) with abstract interpreters is an effective approach
to improve the latter one while the former one is untouched.
In the literature, specializing analysis with respect to the input program
\cite{damian1999partial, amtoft1999partial, Boucher:1996:ACN:647473.727587,
ashley:practical} is not a new, despite the fact that most of the previous work
are ad-hoc on a particular analysis or requiring significant changes to the
analyzer. For example, abstract compilation is one of the techniques
\cite{Boucher:1996:ACN:647473.727587}. But it requires the whole analyzer to be
rewritten to the closure generation from. In this paper, we advocate a systematic
approach to meta-programming that achieves the performance goal with less
intrusive changes; in fact, a case study (Section~\ref{cs_ac}) comparing our
approach with abstract compilation shows that, by utilizing type-based stage
annotations, the analyzer program does not need any changes.

\paragraph{Modularity via Meta-Programming} \todo{compress this part}

Our next contribution is to enable modular analysis based on a whole-program
analyzer by meta-programming. Modern software are shipped with large libraries
\cite{toman_et_al:LIPIcs:2017:7121}, and analyzing programs with large libraries
is usually expensive or unnecessarily repeated for whole-program analyzing
algorithms. For example, it has been shown that analyzing a simple ``Hello
World'' program in Java depends on additional 3,000 classes in the library
\cite{DBLP:conf/oopsla/KulkarniMZN16}. A natural idea is to analyze libraries
and generate summaries, later we can plugin the necessary information into the
summaries and finish the analysis; therefore such summaries should be modular
and reusable. Meta-programming gives us a chance to readily tweak a
whole-program analyzing algorithm into an analyzer that works modularly and
produces summaries. Still, the idea is to specialize the abstract interpreter
that was used for whole-program analysis, but on a subset of the program, for
examples, a set of generic functions for list manipulation. Such specialization
works on lambda expressions and generates code, which is partial analysis
results and can be completed by running with the necessary arguments (for
example, an environment that closes the lambda term) at next stages. Multiple
partial analysis results also can be reused later and composed together as they
are just normal programs at the next stage. Through staging, we mechanically
obtain a modular analysis, even though the original algorithm is formulated as a
whole-program analysis.

\paragraph{Contributions} The contribution of the paper is twofold:
1) Intellectually, we present a unifying construction that naturally extends the first 
   Futamura projection to abstract interpreters. 
2) Pragmatically, staging abstract interpreters is an approach to improve performance 
   while untouch the soundness. We study the applicability and evaluate the performance 
   of this approach on various analyses.

The paper is organized as follows: \todo{restate this part}

\begin{itemize}
  \item Starting from a generic interface of the interpreter for a functional language
    (Section~\ref{generic_if}), we first show the instantiation of concrete
    interpretation (Section~\ref{unstaged_conc}). Then we lift it to the staged
    version by replacing the binding-time types (Section~\ref{stagedinterp}); and to
    the abstract interpreter by applying abstraction to the environment, store, and
    values (Section~\ref{unstaged_abs}). Finally, we show the combination of the two
    transformations, dubbed \textit{staged abstract interpreters}, can be readily
    derived (Section~\ref{sai}).

  \item We demonstrate that if we apply the abstract interpreter on components
    of the program separately, it not only improves the \textit{efficiency} but
    also the \textit{scalability} (Section~\ref{sai} \todo{}), which are two
    major issues in static analysis.

  \item We conduct two case studies to show the applicability and usefulness
    of applying meta-programming to static analysis (Section~\ref{cases_study}).
    (1) We first revisit the Abstract Compilation (AC)
    \cite{Boucher:1996:ACN:647473.727587} technique (Section~\ref{cs_ac}).
    Our approach and AC are able to achieve the same goal, however, as we show,
    one of the advantage of staging through types is it does not require
    whole-analyzer refactoring.
    (2) We extend the staged abstract interpreter to two variants of
    control-flow analysis: context-sensitive pushdown control-flow analysis and
    \todo{store-widened?} (Section~\ref{sai}).
    
  \item Based on the instantiation of control-flow analysis, we empirically
    evaluate the performance improvement by staging and specialization
    (Section~\ref{evaluation}). We compare the both context-insensitive and
    \note{context-sensitive} analysis.
    
\end{itemize}

\todo{mention LMS somewhere}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\iffalse
Likewise, specializing static analyses by partial evaluation emerged in late 90s
, and indeed it is able to effectively remove the interpretive
overhead of repeatedly traversing the abstract syntax tree. However, these
previous works focused mostly on one particular analysis, or required to
completely rewrite the analyzer. Hence, it is worth to investigate the idea from
a modern perspective based on generative programming, especially for a general
abstract interpreter that models direct-style $\lambda$-calculus, imperative
features such as mutation, and different abstract domains. One technical
challenge is the binding-time engineering when non-determinism, fixed-point
iterations, different abstract domains and staging are introduced at the same
time. In this paper, we present an end-to-end design and implementation of
staging an abstract interpreter; that means not only the interpreter that
traverses the abstract syntax tree, but also the data structure of abstract
domains, abstract environment and heap, and the fixed-point iteration are
staged.

\todo{rewrite this paragraph}
On the other hand, the abstract interpreter, as a semantic artifact, should be
written in a style that is easy for people to communicate the formulation and
abstraction, but also can be implemented efficiently. As the slogan of
multi-stage programming said, "abstraction without regret", we draw connection
between high-level description and efficient implementation of abstract
interpreters, just like the connection between concrete interpreters and
compilers drawn by Futamura projection. Particularly, we show an easy but
systematic way of adding stage annotations to the abstract interpreter without
changing the code of the interpreter skeleton, which is shared between four concrete
or abstract, unstaged or staged interpreters. We use the LMS framework for staging,
which allows us only use types to annotate the binding-time. Therefore, the
proposed approach bridges the gap between designing sound static analysis and
implementing efficient program analyzer.

%Of course, all interpreters are metalinguistic abstractions, but some
%interpreters are more "abstract" than others \todo{maybe rephrase}.
%Particularly, we also would like a systematic approach to optimize program
%analyzers and meanwhile minimally modifying the analyzer programs.

When staging a concrete interpreter, the programmers need to distinguish static
and dynamic values --- the given program to be executed by the interpreter is
classified as static because it is known at compile-time, and the inputs to that
program are dynamic. However, when staging an abstract interpreter, this
distinction does not exist anymore. Because the abstract interpreter
instantiates all the inputs as some form of abstract values, which are usually
top elements in their abstract domains and are also statically known. Then what
is the point of staging if there is no such distinction? A surprising by-product
of thinking about this question is to realize that we can apply the staged
abstract interpreter on \textit{open} programs, and the free variables
representing other parts of the program (e.g., libraries) are dynamic inputs,
therefore we obtain a modular analysis through staging, mechanically.
\note{TR: I don't understand this. The program structure is static, the abstract values
are still dynamic, no? They change in every iteration of the fixpoint algorithm}
\note{TR: why is it a big deal. On Closed programs we obtain a constant factor,
but modular analysis has different asymptotics}

When targeting higher-order programs, either staged concrete interpreters or
staged abstract interpreters are able to compile a closure, i.e., specialize the
call of interpreter with respect to the body expression of the lambda term
without knowing the actual argument value. By generalizing this observation, we
can actually specialize the abstract interpreter with any open programs, which
unexpectedly leads to a modular analysis and improves scalability. An open
program contains free variables, which represent other parts of the program and
will be left as dynamic values. For instance, one challenge in static analysis
of modern software is that the programs are usually shipped with large library
code \cite{toman_et_al:LIPIcs:2017:7121}, for example, it has been shown that
analyzing a simple "Hello World" program in Java depends on additional 3,000
classes in the library \cite{DBLP:conf/oopsla/KulkarniMZN16}. A precise
whole-program analysis formulated as abstract interpreters can be very expensive
due to the scale of the program and the inherent complexity of the algorithm.
However, analyzing these libraries is sometimes unnecessarily repeated. When
applying the staged abstract interpreter on such open programs, we leave the
unknown arguments and calling contexts as dynamic values and generate partial
analyzing result which is represented as a residual program. The partial
analyzing result can be reused and composed with the analyzing result of
application code when available later. Therefore we mechanically obtain a
modular analysis through staging, even though the original algorithm is
formulated as a whole-program analysis.

It has been observed that partially applying context-sensitivity on selected
portion of the program could improve the precision and efficiency
\cite{zipper2018, Kastrinis:2013:HCP:2491956.2462191}. We show that staging
abstract interpreters as an approach to effectively implement hybird
context-sensitivity\todo{}.
\fi

%\note{Expression problem \cite{DBLP:conf/icfp/GibbonsW14, DBLP:conf/ecoop/KrishnamurthiFF98, expproblem},}
