\section{Empirical Evaluation} \label{evaluation}

To evaluate how the performance can be improved by our staged abstract interpreters
approach, we scale our toy interpreter to a large subset of the Scheme language
and evaluate the analyzing time against with the unstaged versions. The result
shows that the staged versions are averagely 10+ times as fast as the unstaged
version, as strong evidence of our \textit{abstraction without regret}
approach.

\paragraph{Implementation and Evaluation Environment}
We implement the abstract interpreters for control-flow analysis, i.e., a
value-flow analysis that computes which lambda terms can be called at each
call-sites in functional languages. With other suitable abstract domains, the
abstract interpreter also can be extended to other analyses. A front-end
desugars Scheme to a small core language that makes the analyzer easy to
implement. We implement two common monovariant 0-CFA-like analysis, one is
equipped with store-widening (0CFA-SW for short), the other does not (0CFA for
short). The store-widened analysis is expected to be faster than the one
without.  Our prototype implementation currently generates Scala code; the
generated Scala code will be compiled and executed on JVM. it is possible to
generate C/C++ code in the future. In the experiments, we implement several
optimizations mentioned in Section \ref{staged_ds}, specifically, the selective
caching scheme and IR-level rewriting rules (for pairs, lists, sets, and maps) exploiting
partially-static data. These optimizations are useful to reduce the size of
generated code.

We use Scala 2.12.8 and Oracle JVM 1.8.0-191 \footnote{The options for
running JVM is set to the following: \texttt{-Xms2G -Xmx8G -Xss1024M
-XX:MaxMetaspaceSize=2G - XX:ReservedCodeCacheSize=2048M}},
running on an Ubuntu 16.04 LTS (kernel 4.4.0) machine. All of our
evaluations were performed on a machine with 4 Intel Xeon Platinum
8168 CPU at 2.7GHz and 3 TiB of RAM. Although the machine has 96 cores
(192 threads) in total, the abstract interpreters are single-threaded.
To minimize the effect of warming-up from the HotSpot JIT compiler,
all the experiments are executed for 20 times and we report the
statistical median value of the running times. We set a 5 minutes
timeout.

\paragraph{Benchmarks}
The benchmark programs we used in the experiment are collected from several
previous papers \cite{Johnson:2013:OAA:2500365.2500604, ashley:practical,
DBLP:journals/corr/abs-1102-3676} and existing artifacts
\footnote{https://github.com/ilyasergey/reachability} for control-flow
analysis.  Some of the benchmarks are small programs designed for
experiments, such as the \lstinline[keywordstyle=,flexiblecolumns=false,mathescape=false,basicstyle=\tt]{kcfa-worst-}$n$ series, which are intended
to challenge $k$-CFA; while some other benchmarks are from real-world
applications, for examples, the RSA public key encryption algorithm
\lstinline[keywordstyle=,flexiblecolumns=false,mathescape=false,basicstyle=\tt]{rsa}. In the Figure \ref{evaluation_result}, we report the number of
AST node after parsing and desugaring (excluding comments) as a proper
measurement of program size.

\paragraph{Result}

\vspace{-1em}
\begin{figure}[h]
\footnotesize
\begin{tabular}{@{}ll|lll|lll@{}}
\toprule
    program             &\#AST & unstaged   & staged     & $\frac{\text{unstaged}}{\text{staged}}$ & unstaged   & staged    & $\frac{\text{unstaged}}{\text{staged}}$  \\ 
    \midrule
                        &      & \multicolumn{3}{c}{w/o store-widening}  &  \multicolumn{3}{c}{w/ store-widening}\\
    \midrule
    @fib@               & 32   & 3.288 ms   & 0.154 ms   & 21.33x      & 1.434 ms   & 0.098 ms  &  14.62x       \\
    @rsa@               & 451  & 238.171 ms & 23.333 ms  & 10.20x      & 11.977 ms  & 1.197 ms  &  10.00x       \\
    @church@            & 120  & 61.001 s   & 4.277 s    & 14.26x      & 2.338 ms   & 0.534 ms  &  4.37x        \\
    @fermat@            & 310  & 23.540 ms  & 2.885 ms   & 8.05x       & 7.146 ms   & 0.915 ms  &  7.81x        \\
    @mbrotZ@            & 331  & 665.456 ms & 66.070 ms  & 10.07x      & 11.008 ms  & 1.476 ms  &  7.45x        \\
    @lattice@           & 609  & 29.230 s   & 2.627 s    & 11.12x      & 16.432 ms  & 2.427 ms  &  6.76x        \\
    @kcfa-worst-16@     & 182  & 44.431 ms  & 3.211 ms   & 13.83x      & 4.425 ms   & 0.850 ms  &  5.20x        \\
    @kcfa-worst-32@     & 358  & 284.268 ms & 9.065 ms   & 31.35x      & 10.109 ms  & 1.661 ms  &  6.08x        \\
    @kcfa-worst-64@     & 710  & 2.165 s    & 0.0425 s   & 50.85x      & 23.269 ms  & 3.312 ms  &  7.02x        \\
    @solovay-strassen@  & 523  & 5.078 s    & 0.766 s    & 6.62x       & 18.757 ms  & 3.142 ms  &  5.96x       \\
    @regex@             & 550  & -          & -          & -           & 6.803 ms   & 1.088 ms  &  6.24x       \\
    @matrix@            & 1732 & -          & -          & -           & 85.611 ms  & 9.297 ms  &  9.20x       \\
    \bottomrule
\end{tabular}
\caption{Evaluation result for monovariant control-flow analysis.} \label{evaluation_result}
\end{figure}

Figure~\ref{evaluation_result} shows the evaluation result, comparing the
performance improved by staging on the two monovariant CFAs The
\textit{unstaged} and \textit{staged} columns show the median time to finish
the analysis. The column $\frac{\text{unstaged}}{\text{staged}}$ shows the
improvement. The dash - in some cells indicates timeout.  As
expected, the staged version significantly outperforms the unstaged version on
all benchmarks. We observe that simple and homogeneous domain used in the
benchmark usualy leads to better speed-ups.  For 0CFA w/o store-widening, we
observe an average speed-up of ~17x times; for 0CFA w/ store-widening, the
average speed-up is ~7x times.  The @kcfa-worst-64@ benchmark has the most
speed-up among all the tests. Although it has a comparablely large size of AST,
it has a quite simple structure which only involves booleans, functions and
applications. We conjecture that the simple domain and structure allow the
generated code to be efficiently optimized by the JVM. But on the corresponding
0CFA-SW, we do not observe such great speed-up. Due to the complexity of
generated code and next-stage running platform (JVM), we leave the detailed
empirical analysis of which static analysis can benefit more from
specialization as future work.

%Our evaluation result provides an abstraction without regret
%approach to construct optimizing abstract interpreters -- the user may write a
%high-level modular and compositional abstract interpreter, and then derive a
%staged version in a principled way that runs significantly faster.

