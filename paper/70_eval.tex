\section{Empirical Evaluation} \label{evaluation}

To evaluate how the performance can be improved by our staged abstract interpreters
approach, we scale our toy interpreter to a large subset of the Scheme language
and evaluate the analyzing time against with the unstaged versions. The result
shows that the staged versions are averagely 11x times as fast as the unstaged
version, as strong evidence of our \textit{abstraction without regret}
approach.

\paragraph{Implementation and Evaluation Environment}
We implement the abstract interpreters for control-flow analysis, i.e., a
value-flow analysis focus on lambda terms in functional languages. A front-end
desugars Scheme to a small core language that makes the analyzer easy to
implement. As described in Section \ref{cfa}, we implement two monovariant
0-CFA-like analysis, one is equipped with store-widening, the other does not.
We also implement several optimizations mentioned in Section \ref{staged_ds} 
(less than ~30 LOC in total).

We use Scala 2.12.7 and Oracle JVM 1.8.0-191, running on an Ubuntu 16.04 LTS
(kernel 4.4.0) machine. All of our evaluations were performed on a machine with 4 Intel
Xeon Platinum 8168 CPU at 2.7GHz and 3 TiB of RAM. Although the machine has 96
cores (192 threads) in total, the abstract interpreters are single-threaded.
To minimize the effect of warming-up from the JVM JIT compiler, all the
experiments are executed for 20 times and we report the statistical median value
of the running times. We set a 5 minutes timeout.

\paragraph{Benchmarks}
The benchmark programs we used in the experiment are collected from several
previous papers \cite{Johnson:2013:OAA:2500365.2500604, ashley:practical,
DBLP:journals/corr/abs-1102-3676} and existing artifacts
\footnote{https://github.com/ilyasergey/reachability} for control-flow analysis.
Some of the benchmarks are small programs designed for experiments, such as the
@kcfa-worst-@$n$ series, which are intended to challenge $k$-CFA; while some
other benchmarks are from real-world applications, for examples, the RSA public
key encryption algorithm @rsa@. In the Figure \ref{evaluation_result}, we report
the number of AST node after parsing and desugaring (excluding comments) as a
proper measurement of program size.

\iffalse
We used the following benchmark programs:
\begin{itemize}
  %\item \textbf{fib}: Calculates the $n$-th fibonacci number.
  \item \textbf{rsa}: The RSA public key encryption algorithm.
  \item \textbf{kcfa3}: A difficult benchmark for $k$-CFA.
  \item \textbf{church}: Church numerals with additions and mutiplications.
  \item \textbf{fermat}: Fermat and Solovay-Strassen primality testing.
  \item \textbf{kcfa-worst-case-}\textit{n}: 
    Benchmark programs that are supposed to be tough cases for $k$-CFA; 
    the number $n$ indicates the depth of nesting lambda terms.
\end{itemize}
\fi

\paragraph{Result}

\begin{figure}[h]
\footnotesize
\begin{tabular}{@{}ll|lll|lll@{}}
\toprule
    program             &\#AST & unstaged   & staged     & $\frac{\text{unstaged}}{\text{staged}}$ & unstaged   & staged    & $\frac{\text{unstaged}}{\text{staged}}$  \\ 
    \midrule
                        &      & \multicolumn{3}{c}{w/o store-widening (sec.)}  &  \multicolumn{3}{c}{w/ store-widening (ms)}\\
    \midrule
    @fib@               & 32   & 0.013      & 0.002      & 6.50x      & 1.537 & 0.067  &  22.87x       \\
    @rsa@               & 451  & 0.232      & 0.042      & 5.52x      & 15.113& 1.449 &  10.43x       \\
    @church@            & 120  & 67.342     & 18.917     & 3.56x      & 2.962 & 0.291 &  10.19x       \\
    @fermat@            & 310  & 0.024      & 0.004      & 6.00x      & 7.624 & 0.622 &  12.26x       \\
    @mbrotZ@            & 331  & 0.763      & 0.127      & 6.01x      & 12.229& 0.729 &  16.77x       \\
    @lattice@           & 609  & 31.874     & 8.816      & 3.62x      & 17.943& 0.909 &  19.75x      \\
    @kcfa-worst-16@     & 182  & 0.045      & 0.006      & 7.50x      & 4.658 & 0.785 &  5.93x     \\
    @kcfa-worst-32@     & 358  & 0.284      & 0.022      & 12.91x     & 10.005& 1.680 &  5.95x     \\
    @kcfa-worst-64@     & 774  & 2.18       & 0.129      & 16.90x     & 22.999& 3.872 &  5.94x     \\
    @solovay-strassen@  & 523  & 5.094      & 1.189      & 4.28x      & 19.118& 0.654 &  29.25x      \\
    @regex@             & 550  & -          & -          & -          & 7.133 & 0.624 &  11.43x      \\
    @matrix@            & 1891 & -          & -          & -          & 86.176& 4.592 &  18.76x      \\
    \bottomrule
\end{tabular}
\caption{Evaluation result for monovariant control-flow analysis.} \label{evaluation_result}
\end{figure}

Figure~\ref{evaluation_result} shows the evaluation result, comparing the
performance improved by staging on 0CFA without and with store-widening. The
\textit{unstaged} and \textit{staged} columns show the median time to finish the
analysis. The column $\frac{\text{unstaged}}{\text{staged}}$
shows the improvement from staging. The dash - in some cells indicates timeout.
As we can see from the table, the staged version significantly outperforms the
unstaged version on all benchmarks, especially for the @kcfa-worst-@$n$ series.
We observe the most improvement of 29x times faster than the unstaged version on
@solovay-strassen@. On average, the staging produces code that runs 11x times faster
than the unstaged analyzer.

We also observe that much overhead of unstaged analyzers are from the monadic
layers. Thus our evaluation result provides an abstraction without regret
approach to construct optimizing abstract interpreters -- the user may write a
high-level modular and composable abstract interpreter, and then derive a staged
version in a principled way that runs significantly faster.

\iffalse
As we can see from the table, the staged version significantly outperforms the
unstaged especially for the difficult ones. However, for some benchmarks, we
observe degraded performance. We conjecture this happens due to the fact that
the large size of the generated code is unoptimized -- as what we test here is a
simple implementation almost identical to what we described in the previous
sections, we believe the performance still can be improved if more engineering
effort is taken, such as optimizing code generation and avoiding code
duplication.
\fi
