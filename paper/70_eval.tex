\section{Empirical Evaluation} \label{evaluation}

To evaluate how the performance can be improved by our staged abstract interpreters
approach, we scale our toy interpreter to a large subset of the Scheme language
and evaluate the analyzing time against with the unstaged versions. The result
shows that the staged versions are averagely 10+ times as fast as the unstaged
version, as strong evidence of our \textit{abstraction without regret}
approach.

\paragraph{Implementation and Evaluation Environment}
We implement the abstract interpreters for control-flow analysis, i.e., a
value-flow analysis that computes which lambda terms can be called at each
call-sites in functional languages. With other suitable abstract domains, the
abstract interpreter also can be extended to other analyses. A front-end
desugars Scheme to a small core language that makes the analyzer easy to
implement. As described in Section \ref{cfa}, we implement two monovariant
0-CFA-like analysis, one is equipped with store-widening, the other does not.
\todo{why store widening?} Our prototype implementation currently generates
Scala code; the generated Scala code will be compiled and executed also on JVM.
it is possible to also generate C/C++ code in the future. In the experiments, we
implement several optimizations mentioned in Section \ref{staged_ds},
specifically, the selective caching and rewriting rules exploiting
partially-static data. These optimizations are useful to reduce the size of
generated code.

We use Scala 2.12.8 and Oracle JVM 1.8.0-191 \footnote{The options for running
  JVM is set to the following: \texttt{-Xms2G -Xmx8G -Xss1024M
  -XX:MaxMetaspaceSize=2G - XX:ReservedCodeCacheSize=2048M}}, running on an Ubuntu 16.04 LTS
(kernel 4.4.0) machine. All of our evaluations were performed on a machine with 4 Intel
Xeon Platinum 8168 CPU at 2.7GHz and 3 TiB of RAM. Although the machine has 96
cores (192 threads) in total, the abstract interpreters are single-threaded.
To minimize the effect of warming-up from the HotSpot JIT compiler, all the
experiments are executed for 20 times and we report the statistical median value
of the running times. We set a 5 minutes timeout.

\paragraph{Benchmarks}
The benchmark programs we used in the experiment are collected from several
previous papers \cite{Johnson:2013:OAA:2500365.2500604, ashley:practical,
DBLP:journals/corr/abs-1102-3676} and existing artifacts
\footnote{https://github.com/ilyasergey/reachability} for control-flow analysis.
Some of the benchmarks are small programs designed for experiments, such as the
@kcfa-worst-@$n$ series, which are intended to challenge $k$-CFA; while some
other benchmarks are from real-world applications, for examples, the RSA public
key encryption algorithm @rsa@. In the Figure \ref{evaluation_result}, we report
the number of AST node after parsing and desugaring (excluding comments) as a
proper measurement of program size.

\paragraph{Result}

\begin{figure}[h]
\footnotesize
\begin{tabular}{@{}ll|lll|lll@{}}
\toprule
    program             &\#AST & unstaged   & staged     & $\frac{\text{unstaged}}{\text{staged}}$ & unstaged   & staged    & $\frac{\text{unstaged}}{\text{staged}}$  \\ 
    \midrule
                        &      & \multicolumn{3}{c}{w/o store-widening}  &  \multicolumn{3}{c}{w/ store-widening}\\
    \midrule
    @fib@               & 32   & 3.288 ms   & 0.154 ms   & 21.33x      & 1.434 ms   & 0.098 ms  &  14.62x       \\
    @rsa@               & 451  & 238.171 ms & 23.333 ms  & 10.20x      & 11.977 ms  & 1.197 ms  &  10.00x       \\
    @church@            & 120  & 61.001 s   & 4.277 s    & 14.26x      & 2.338 ms   & 0.534 ms  &  4.37x        \\
    @fermat@            & 310  & 23.540 ms  & 2.885 ms   & 8.05x       & 7.146 ms   & 0.915 ms  &  7.81x        \\
    @mbrotZ@            & 331  & 665.456 ms & 66.070 ms  & 10.07x      & 11.008 ms  & 1.476 ms  &  7.45x        \\
    @lattice@           & 609  & 29.230 s   & 2.627 s    & 11.12x      & 16.432 ms  & 2.427 ms  &  6.76x        \\
    @kcfa-worst-16@     & 182  & 44.431 ms  & 3.211 ms   & 13.83x      & 4.425 ms   & 0.850 ms  &  5.20x        \\
    @kcfa-worst-32@     & 358  & 284.268 ms & 9.065 ms   & 31.35x      & 10.109 ms  & 1.661 ms  &  6.08x        \\
    @kcfa-worst-64@     & 710  & 2.057 s    & 0.029 s    & 70.23x      & 23.269 ms  & 3.312 ms  &  7.02x        \\
    @solovay-strassen@  & 523  & 5.078 s    & 0.766 s    & 6.62x       & 18.757 ms  & 3.142 ms  &  5.96x       \\
    @regex@             & 550  & -          & -          & -           & 6.803 ms   & 1.088 ms  &  6.24x       \\
    @matrix@            & 1732 & -          & -          & -           & 85.611 ms  & 9.297 ms  &  9.20x       \\
    \bottomrule
\end{tabular}
\caption{Evaluation result for monovariant control-flow analysis.} \label{evaluation_result}
\end{figure}

Figure~\ref{evaluation_result} shows the evaluation result, comparing the
performance improved by staging on two monovariant CFA (without and with store-widening). The
\textit{unstaged} and \textit{staged} columns show the median time to finish the
analysis. The column $\frac{\text{unstaged}}{\text{staged}}$
shows the improvement from staging. The dash - in some cells indicates timeout.
As we can see from the table, the staged version significantly outperforms the
unstaged version on all benchmarks.
On average, the staging produces code that runs 17x times faster
than the unstaged analyzer.

We also observe that much overhead of unstaged analyzers are from the monadic
layers. Thus our evaluation result provides an abstraction without regret
approach to construct optimizing abstract interpreters -- the user may write a
high-level modular and composable abstract interpreter, and then derive a staged
version in a principled way that runs significantly faster.

\todo{analyze some interesting examples}