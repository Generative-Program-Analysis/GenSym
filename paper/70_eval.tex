\section{Empirical Evaluation} \label{evaluation}

To evaluate how the performance can be improved by our staged abstract interpreters
approach, we scale our toy interpreter to a large subset of the Scheme language
and evaluate the analyzing time against with the unstaged versions. The result
shows that the staged versions are averagely 10+ times as fast as the unstaged
version, as strong evidence of our \textit{abstraction without regret}
approach.

\paragraph{Implementation and Evaluation Environment}
We implement the abstract interpreters for control-flow analysis, i.e., a
value-flow analysis that computes which lambda terms can be called at each
call-sites in functional languages. With other suitable abstract domains, the
abstract interpreter also can be extended to other analyses. A front-end
desugars Scheme to a small core language that makes the analyzer easy to
implement. As described in Section \ref{cfa}, we implement two monovariant
0-CFA-like analysis, one is equipped with store-widening, the other does not.
\todo{why store widening?} Our prototype implementation currently generates
Scala code; the generated Scala code will be compiled and executed also on JVM.
it is possible to also generate C/C++ code in the future. In the experiments, we
implement several optimizations mentioned in Section \ref{staged_ds},
specifically, the selective caching and rewriting rules exploiting
partially-static data. These optimizations are useful to reduce the size of
generated code.

We use Scala 2.12.8 and Oracle JVM 1.8.0-191 \footnote{The options for
running JVM is set to the following: \texttt{-Xms2G -Xmx8G -Xss1024M
-XX:MaxMetaspaceSize=2G - XX:ReservedCodeCacheSize=2048M}},
running on an Ubuntu 16.04 LTS (kernel 4.4.0) machine. All of our
evaluations were performed on a machine with 4 Intel Xeon Platinum
8168 CPU at 2.7GHz and 3 TiB of RAM. Although the machine has 96 cores
(192 threads) in total, the abstract interpreters are single-threaded.
To minimize the effect of warming-up from the HotSpot JIT compiler,
all the experiments are executed for 20 times and we report the
statistical median value of the running times. We set a 5 minutes
timeout.

\paragraph{Benchmarks}
The benchmark programs we used in the experiment are collected from several
previous papers \cite{Johnson:2013:OAA:2500365.2500604, ashley:practical,
DBLP:journals/corr/abs-1102-3676} and existing artifacts
\footnote{https://github.com/ilyasergey/reachability} for control-flow
analysis.  Some of the benchmarks are small programs designed for
experiments, such as the @kcfa-worst-@$n$ series, which are intended
to challenge $k$-CFA; while some other benchmarks are from real-world
applications, for examples, the RSA public key encryption algorithm
@rsa@. In the Figure \ref{evaluation_result}, we report the number of
AST node after parsing and desugaring (excluding comments) as a proper
measurement of program size.

\paragraph{Result}

\begin{figure}[h]
\footnotesize
\begin{tabular}{@{}ll|lll|lll@{}}
\toprule
    program             &\#AST & unstaged   & staged     & $\frac{\text{unstaged}}{\text{staged}}$ & unstaged   & staged    & $\frac{\text{unstaged}}{\text{staged}}$  \\ 
    \midrule
                        &      & \multicolumn{3}{c}{w/o store-widening}  &  \multicolumn{3}{c}{w/ store-widening}\\
    \midrule
    @fib@               & 32   & 3.288 ms   & 0.154 ms   & 21.33x      & 1.434 ms   & 0.098 ms  &  14.62x       \\
    @rsa@               & 451  & 238.171 ms & 23.333 ms  & 10.20x      & 11.977 ms  & 1.197 ms  &  10.00x       \\
    @church@            & 120  & 61.001 s   & 4.277 s    & 14.26x      & 2.338 ms   & 0.534 ms  &  4.37x        \\
    @fermat@            & 310  & 23.540 ms  & 2.885 ms   & 8.05x       & 7.146 ms   & 0.915 ms  &  7.81x        \\
    @mbrotZ@            & 331  & 665.456 ms & 66.070 ms  & 10.07x      & 11.008 ms  & 1.476 ms  &  7.45x        \\
    @lattice@           & 609  & 29.230 s   & 2.627 s    & 11.12x      & 16.432 ms  & 2.427 ms  &  6.76x        \\
    @kcfa-worst-16@     & 182  & 44.431 ms  & 3.211 ms   & 13.83x      & 4.425 ms   & 0.850 ms  &  5.20x        \\
    @kcfa-worst-32@     & 358  & 284.268 ms & 9.065 ms   & 31.35x      & 10.109 ms  & 1.661 ms  &  6.08x        \\
    @kcfa-worst-64@     & 710  & 2.057 s    & 0.029 s    & 70.23x      & 23.269 ms  & 3.312 ms  &  7.02x        \\
    @solovay-strassen@  & 523  & 5.078 s    & 0.766 s    & 6.62x       & 18.757 ms  & 3.142 ms  &  5.96x       \\
    @regex@             & 550  & -          & -          & -           & 6.803 ms   & 1.088 ms  &  6.24x       \\
    @matrix@            & 1732 & -          & -          & -           & 85.611 ms  & 9.297 ms  &  9.20x       \\
    \bottomrule
\end{tabular}
\caption{Evaluation result for monovariant control-flow analysis.} \label{evaluation_result}
\end{figure}

Figure~\ref{evaluation_result} shows the evaluation result, comparing
the performance improved by staging on two monovariant CFA (without
and with store-widening). The \textit{unstaged} and \textit{staged}
columns show the median time to finish the analysis. The column
$\frac{\text{unstaged}}{\text{staged}}$ shows the improvement from
staging. The dash - in some cells indicates timeout.  As we can see
from the table, the staged version significantly outperforms the
unstaged version on all benchmarks.  On average, the staging produces
code that runs 17x times faster than the unstaged analyzer.

\todo{analyze some interesting examples}

Our evaluation result provides an abstraction without regret
approach to construct optimizing abstract interpreters -- the user may write a
high-level modular and compositional abstract interpreter, and then derive a
staged version in a principled way that runs significantly faster.

