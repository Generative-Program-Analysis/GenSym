\section{Empirical Evaluation} \label{evaluation}

To evaluate how much performance can be improved by the staging, we scale our
toy staged abstract interpreter to support a large subset of the Scheme language and evaluate
the analysis time against the unstaged versions. The result shows that the
staged versions are, on average, 10+ times as fast as the unstaged version, as
evidence of our \textit{abstraction without regret} approach.

\vspace{-5pt}
\paragraph{Implementation and Evaluation Environment} We implement the abstract
interpreters for control-flow analysis, i.e., a value-flow analysis that
computes which lambda terms can be called at each call-site.
With other suitable abstract domains, the abstract interpreter
can also be extended to other analyses.
A front-end desugars Scheme to a small core language that makes the analyzer
easy to implement. We implement two common monovariant 0-CFA-like analyses: one
is equipped with store-widening (0CFA-SW for short), the other is not (0CFA
for short). It is traditionally expected that the store-widened analysis be faster than the
alternative.  Our prototype implementation currently generates Scala code; the
generated Scala code will be compiled and executed on the JVM, though it is possible to
generate C/C++ code in the future. In the experiments, we implement several
optimizations mentioned in Section \ref{staged_ds}, specifically, the selective
caching scheme and IR-level rewriting rules (for pairs, lists, sets, and maps)
that exploit partially-static data. These optimizations are useful to reduce
the size of generated code.

We use Scala 2.12.8 and Oracle JVM 1.8.0-191 \footnote{The options for
running the JVM are set to the following: \texttt{-Xms2G -Xmx8G -Xss1024M
-XX:MaxMetaspaceSize=2G - XX:ReservedCodeCacheSize=2048M}},
running on an Ubuntu 16.04 LTS (kernel 4.4.0) machine. All of our
evaluations were performed on a machine with 4 Intel Xeon Platinum
8168 CPU at 2.7GHz and 3 TB of RAM. Although the machine has 96 cores
(192 threads) in total, the abstract interpreters are single-threaded.
To minimize the effect of warm-up from the HotSpot JIT compiler,
all the experiments are executed for 20 times and we report the
statistical median value of the running times. We set a 5 minute
timeout.

\paragraph{Benchmarks}
The benchmark programs we used in the experiment are collected from
previous papers \cite{Johnson:2013:OAA:2500365.2500604, ashley:practical,
DBLP:journals/corr/abs-1102-3676} and existing artifacts
\footnote{https://github.com/ilyasergey/reachability} for control-flow
analysis.  Some of the benchmarks are small programs designed for
experiments, such as the @kcfa-worst-@$n$ series, which are intended
to challenge $k$-CFA; while some other benchmarks are from real-world
applications, for example, the RSA public key encryption algorithm
@rsa@. In Figure \ref{evaluation_result}, we report the number of
AST nodes after parsing and desugaring (excluding comments) as a proper
measurement of program size.

\paragraph{Result}

\begin{figure}[t] \footnotesize
\begin{tabular}{@{}ll|lll|lll@{}}
\toprule
    program             &\#AST & unstaged   & staged     & $\frac{\text{unstaged}}{\text{staged}}$ & unstaged   & staged    & $\frac{\text{unstaged}}{\text{staged}}$  \\
    \midrule
                        &      & \multicolumn{3}{c}{w/o store-widening}  &  \multicolumn{3}{c}{w/ store-widening}\\
    \midrule
    @fib@               & 32   & 3.288 ms   & 0.154 ms   & 21.33x      & 1.434 ms   & 0.098 ms  &  14.62x       \\
    @rsa@               & 451  & 238.171 ms & 23.333 ms  & 10.20x      & 11.977 ms  & 1.197 ms  &  10.00x       \\
    @church@            & 120  & 61.001 s   & 4.277 s    & 14.26x      & 2.338 ms   & 0.534 ms  &  4.37x        \\
    @fermat@            & 310  & 23.540 ms  & 2.885 ms   & 8.05x       & 7.146 ms   & 0.915 ms  &  7.81x        \\
    @mbrotZ@            & 331  & 665.456 ms & 66.070 ms  & 10.07x      & 11.008 ms  & 1.476 ms  &  7.45x        \\
    @lattice@           & 609  & 29.230 s   & 2.627 s    & 11.12x      & 16.432 ms  & 2.427 ms  &  6.76x        \\
    @kcfa-worst-16@     & 182  & 44.431 ms  & 3.211 ms   & 13.83x      & 4.425 ms   & 0.850 ms  &  5.20x        \\
    @kcfa-worst-32@     & 358  & 284.268 ms & 9.065 ms   & 31.35x      & 10.109 ms  & 1.661 ms  &  6.08x        \\
    @kcfa-worst-64@     & 710  & 2.165 s    & 0.0425 s   & 50.85x      & 23.269 ms  & 3.312 ms  &  7.02x        \\
    @solovay-strassen@  & 523  & 5.078 s    & 0.766 s    & 6.62x       & 18.757 ms  & 3.142 ms  &  5.96x       \\
    @regex@             & 550  & -          & -          & -           & 6.803 ms   & 1.088 ms  &  6.24x       \\
    @matrix@            & 1732 & -          & -          & -           & 85.611 ms  & 9.297 ms  &  9.20x       \\
    \bottomrule
\end{tabular}
\vspace{-0.5em}
\caption{Evaluation result for monovariant control-flow analysis.} \label{evaluation_result}
\vspace{-2em}
\end{figure}

Figure~\ref{evaluation_result} shows the evaluation result, comparing the
performance improved by staging on the two monovariant CFAs. The
\textit{unstaged} and \textit{staged} columns show the median time (excluding
pre-compilation) to finish the analysis. The column
$\frac{\text{unstaged}}{\text{staged}}$ shows the improvement. The dash @-@ in
some cells indicates timeout.  As expected, the staged version significantly
outperforms the unstaged version on all benchmarks.
%We observe that simple value domains used in the benchmark usually leads to
%better speed-ups, for example, the @kcfa-worst@ series only uses lambdas and
%booleans.
For 0CFA without store-widening, we observe an average speedup of ~17x times;
for 0CFA with store-widening, the average speedup is ~7x times.
\texttt{kcfa-worst-64} has the highest speedup among all the tests. Although it
has a comparably large size of AST, it has a rather simple structure which only
involves booleans, lambda terms, and applications. We conjecture that the simple
domain and structure allow the generated code to be efficiently optimized by the
JVM. However, on the corresponding 0CFA-SW, we do not observe such a significant
speedup. Due to the complexity of generated code and next-stage running platform
(JVM), we leave the detailed empirical analysis of which static analysis can
benefit more from the specialization as future work.

%Our evaluation result provides an abstraction without regret
%approach to construct optimizing abstract interpreters -- the user may write a
%high-level modular and compositional abstract interpreter, and then derive a
%staged version in a principled way that runs significantly faster.

