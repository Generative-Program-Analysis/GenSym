\section{Empirical Evaluation} \label{evaluation}

To evaluate the performance can be improved by our staging abstract interpreters
approach, we scale our toy interpreter to a large subset of the Scheme language
and evaluate the analyzing time against with the unstaged versions. The result
shows an average \todo{X}\% running time improvement, as a strong evidence of
our \textit{abstraction without regret} approach to construct abstract
interpreters.

We implement the abstract interpreters for control-flow analysis, i.e., a
value-flow analysis focus on lambda terms in functional languages. A frontend
desugars Scheme to a small core language that make the analyzer easy to
implement. Macros and @call/cc@ is not handled. As described in Section
\ref{cfa}, we implement a monovariant 0-CFA-like analysis, a context-sensitive
$1$-CFA-like analysis, both their versions with and without store-widening.

\paragraph{Setting}
We use Scala 2.12.7 and Oracle JVM 1.8.0-191, running on an Ubuntu 16.04 LTS
(kernel 4.4.0) machine. All of our evaluation were performed on a machine with 4 Intel
Xeon Platinum 8168 CPU at 2.7GHz and 3 TiB of RAM. Although the machine has 96
cores (192 threads) in total, the abstract interpreters are single-threaded.
To minimize the effect of warming-up from the JVM JIT compiler, all the
experiments are executed for 20 times and we report the statistical median value
of the running times (in seconds).

\paragraph{Benchmarks}
The benchmark programs we used in the experiment are collected from several previous papers
\cite{Johnson:2013:OAA:2500365.2500604, ashley:practical,
DBLP:journals/corr/abs-1102-3676} and exisiting artifacts
\footnote{https://github.com/ilyasergey/reachability} for control-flow analysis.
Some of the benchmarks are small programs designed for experiments, such as the @kcfa-worst-@$n$ series, 
which are intended to challenge $k$-CFA; while some other benchmarks are from real-world applications, 
for examples, the RSA public key encryption algorithm @rsa@ and matrix computation @matrix@.
In the Table \ref{?}, we report the number of AST node after parsing and desugaring 
(excluding comments) as a proper measurement of program size.

\iffalse
We used the following benchmark programs:
\begin{itemize}
  %\item \textbf{fib}: Calculates the $n$-th fibonacci number.
  \item \textbf{rsa}: The RSA public key encryption algorithm.
  \item \textbf{kcfa3}: A difficult benchmark for $k$-CFA.
  \item \textbf{church}: Church numerals with additions and mutiplications.
  \item \textbf{fermat}: Fermat and Solovay-Strassen primality testing.
  \item \textbf{kcfa-worst-case-}\textit{n}: 
    Benchmark programs that are supposed to be tough cases for $k$-CFA; 
    the number $n$ indicates the depth of nesting lambda terms.
\end{itemize}
\fi

\paragraph{Result}

\begin{figure}[h]
\tiny
\begin{tabular}{@{}lllll@{}}
\toprule
    program          &\#AST & unstaged   & staged     & $\frac{\text{unstaged}}{\text{staged}}$ \\ \midrule
    @church@         & 120  & 71.811     & 23.990     & 299.34\%          \\
    @rsa@            & 451  & 0.308      & 0.064      & 459.15\%          \\
    @lattice@        & 609  & 31.874     & 8.816      & 358.69\%          \\
    @fermat@         & 310  & 0.043      & 0.006      & 716.67\%          \\
    @mbrotZ@         & 331  & 1.112      & 0.214      & 519.63\%          \\
    @kcfa-worst-16@  & 182  & 0.083      & 0.013      & 638.46\%          \\
    @kcfa-worst-32@  & 358  & 0.452      & 0.044      & 1027.27\%         \\
    @kcfa-worst-64@  & 774  & 3.329      & 0.236      & 1410.59\%         \\
    \bottomrule
\end{tabular}
\caption{Evaluation result for 0CFA without store-widening.} \label{evaluation_result}
\end{figure}

Figure~\ref{evaluation_result} shows the evaluation results. The
\textit{unstaged} and \textit{staged} columns show the median time to finish the
analysis (in seconds). The "$\Delta$" column shows the median runtime change
from the unstaged to the staged version. \todo{0cfa, 1cfa, store-widen}

\iffalse
As we can see from the table, the staged version significantly outperforms the
unstaged especially for the difficult ones. However, for some benchmarks, we
observe degraded performance. We conjecture this happens due to the fact that
the large size of the generated code is unoptimized -- as what we test here is a
simple implementation almost identical to what we described in the previous
sections, we believe the performance still can be improved if more engineering
effort is taken, such as optimizing code generation and avoiding code
duplication.
\fi
