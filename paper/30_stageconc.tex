\section{From Interpreters to Staged Interpreters} \label{stagedinterp}

In this section, based on the generic interpreter and concrete components we
presented in Section \ref{prelim}, we show how to obtain a staged concrete
interpreter by changing the monads and refactoring several primitive
operations. We begin by briefly introducing the Lightweight Modular Staging
framework in Scala.

\subsection{Multi-Stage Programming with LMS}

Lightweight modular staging (LMS) \cite{DBLP:conf/gpce/RompfO10} is a
multi-stage programming framework implemented as a Scala library that enables
dynamic code generation in a type-safe manner. Different from the approach of
MetaOCaml \cite{DBLP:conf/flops/Kiselyov14, DBLP:conf/gpce/CalcagnoTHL03} that
uses syntactic quotation and escape, LMS distinguishes binding-time solely based
on types. LMS provides a type constructor @Rep[T]@ where @T@ can be an arbitrary
type, indicating a value of @T@ will be known at the next stage. All operations
acting on @Rep[T]@ expressions will be residualized as generated code.

For the example we introduced in Section \ref{intro}, we have identified @b@ will 
be known later. To specialize it we write a function @snippet@ that provides a 
concrete value for @x@. After this, we may instantiate an @DslDriver@ object 
which provide the specialized program.

\begin{lstlisting}
  new DslDriver[Int, Int] {
    def power(b: Rep[Int], x: Int): Rep[Int] = if (x == 0) 1 else b * power(b, x-1)
    def snippet(b: Rep[Int]): Rep[Int] = power(b, 5)
  }
\end{lstlisting}

The LMS framework provides staging support for primitive data types such as
@Int@ and @Double@, and commonly-used data structures such as arrays and maps.
The idea is to construct intermediate representations (IR) for the next stage
program at the current stage. Such conversion from expressions of type @Rep[_]@ to
their IR is done by using the implicit design pattern. As we will see later,
implementing the staging support for custom classes is also straightforward.

\subsection{Staged Concrete Semantics}

To implement the staged concrete interpreter, we reuse the exactly same concrete
components from Section \ref{unstaged_conc}, but we instantiate the binding-time
type @R[T]@ as @Rep[T]@.

\paragraph{Staged Monads} We also need to use monads that operate on staged
values. Note that the monadic objects such as @StateT@ or the internal value
@R[S] => M[(A,S)]@ are not staged, only the data @S@ or @A@ are staged.
This is why we can completely eliminate the overhead caused by the monadic layers.
Here we use the prefix name @Rep@ to differentiate unstaged monads and staged
monads, such as @RepReaderT@. So the @AnsM[_]@ is instantiated as the same monad
stack before, but with @Rep@ versions.

\begin{lstlisting}
  type R[T] = Rep[T]
  type AnsM[T] = RepReaderT[RepStateT[RepIdM, Store, ?], Env, T]
\end{lstlisting}

We use @RepStateT@ as an example to give a sketch of how the @Rep@ versions of monad
transformers look like. The generic type @M[_]@ is a @RepMonad@ and used as the
inner monad, which means the pair @(A,S)@ is also staged when appears inside of the monad
@M[_]@. The @flatMap@ is similar to the unstaged one, but the function @f@ takes
a @Rep[A]@ value. Again, @f@ is not a next-stage function, but a current-stage
function that transforms a next-stage value of @Rep[A]@ to @StateT[M,S,B]@.
%\todo{highlight the changes in the code}

\begin{lstlisting}
  case class RepStateT[M[_]: RepMonad, S, A](run: Rep[S] => M[(A, S)]) {
    def flatMap[B](f: Rep[A] => StateT[M, S, B]): StateT[M, S, B] = ...
  }
\end{lstlisting}

Readers may notice that the conversion between unstaged monads and staged monads
is simply changing the unstaged data to staged data, which in fact can be
achieved without modifying the implementation of monads. This is true so far, but
as we will see, it is not that easy when introducing the nondeterminism monad
(@ListT@) for abstract interpreters, as the whole list will be a next-stage
value instead of merely the elements in the list. For this reason, we explicitly
distinguish and present the two versions of monads, as a more consistent
approach.

\paragraph{Primitive Operations} Most of the primitive operations can be easily
translated to their staged versions -- we just need to let them works on staged data.
As we mentioned before, we elaborate on how to handle functions and applications in
detail. One of the desired goals is to eliminate the monadic layers through
staging, this becomes a little bit tricky for closures because we actually need 
 to compile the lambda term instead of generating a @CloV@ value, but the @ev(e)@
returns a monad and prevents the specialization. To make this happen, when
closing a lambda term, we need to specialize the body expression by
\textit{collpasing} the @AnsM@ monads to normal values. The
following function @close@ illustrates the idea: @f@ is a current stage function
but takes two next-stage values @v@ and $\sigma$, inside of which, we eagerly
collapse the monads to values by @ev(e)(ρ_*)(σ_*)@, which has type
@Rep[(Value, Store)]@. Then we generate the next-stage value of the compiled
closure using @emit_compiled_clo@, containing the compiled closure.

\begin{lstlisting}
  def emit_compiled_clo(f: (Rep[Value], Rep[Store]) => Rep[(Value, Store)]): Rep[Value]
  def close(ev: Expr => Ans)(λ: Lam, ρ: Rep[Env]): Rep[Value] = {
    val Lam(x, e) = λ
    val f: (Rep[Value], Rep[Store]) => Rep[(Value, Store)] = {
      case (v: Rep[Value], σ: Rep[Store]) =>
        val α = alloc(σ, x); val ρ_* = ρ + (unit(x) → α); val σ_* = σ + (α → v)
        ev(e)(ρ_*)(σ_*)
    }; emit_compiled_clo(f)
  }
\end{lstlisting}

When applying a function in @ap_clo@, we also generate the next-stage value
for closure application through @emit_ap_clo@, assuming that the @rator@ is already an
IR of compiled closure, @rand@ is an arbitrary stage value, and $\sigma$ is the
latest store. @emit_ap_clo@ represents the result value of the application, which has type
@Rep[(Value, Store)]@. Note that @emit_ap_clo@ is an ordinary value from the
future, and there are no monads in the future, so we need to reify the value and
store back into the monad stack through @put_store@ and @yield@.

\begin{lstlisting}
  def emit_ap_clo(rator: Rep[Value], rand: Rep[Value], σ: Rep[Store]): Rep[(Value, Store)]
  def ap_clo(ev: Expr => Ans)(rator: Rep[Value], rand: Rep[Value]): Ans = for {
    σ <- get_store
    val res: Rep[(Value, Store)] = emit_ap_clo(rator, rand, σ)
    _ <- put_store(res._2)
  } yield res._1
\end{lstlisting}

\paragraph{Code Generation}

The method @emit_compiled_clo@ and @emit_ap_clo@ mentioned above produces
intermediate representations for the denoted operations in LMS. These IRs are
wrappers of the underlying staged data. We define these IR nodes using
@case class@ extending from @Def[T]@, meaning that they are next-staged definitions of
type @T@.

\begin{lstlisting}
  case class IRCompiledClo(f: Rep[((Value,Store)) => (Value, Store)], λ: Lam, ρ: Rep[Env]) extends Def[Value]
  case class IRApClo(rator: Rep[Value], rand: Rep[Value], σ: Rep[Store]) extends Def[(Value, Store)]
\end{lstlisting}

After constructing the IR graph, LMS provides an @emitNode@ to generate code for
each kind of IR node. To generate code for @IRApClo@, we know that @rator@ is a
compiled closure @IRCompiledClo@, which contains a next-stage function @f@, so
we just need to generate code that invokes @f@ with @rand@ and @σ@ as arguments
provided. Similarly, we directly generate a next-stage value @CompiledClo(f)@
for @IRCompiledClo@.

\begin{lstlisting}
  override def emitNode(sym: Sym[Any], rhs: Def[Any]) = rhs match {
    case IRApClo(rator, rand, σ) => emitValDef(sym, s"$\textdollar$rator.f($\textdollar$rand, $\textdollar$σ)")
    case IRCompiledClo(f, λ, ρ) => emitValDef(sym, s"CompiledClo($\textdollar$f, $\textdollar$λ, $\textdollar$ρ)")
    ...
  }
\end{lstlisting}
