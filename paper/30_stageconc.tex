\section{From Interpreters to Staged Interpreters} \label{stagedinterp}

In this section, based on the generic interpreter and concrete components we
presented in Section \ref{prelim}, we show how to stage the concrete interpreter
by changing the monad type and refactoring several primitive operations. We
begin by briefly introducing the Lightweight Modular Staging framework in Scala,
and then replay the same steps from the unstaged counterpart. At last, we briefly
describe code generation.

\subsection{Multi-Stage Programming with LMS}

Lightweight modular staging (LMS) \cite{DBLP:conf/gpce/RompfO10} is a
multi-stage programming framework implemented as a Scala library that enables
dynamic code generation in a type-safe manner. Different from the approach of
MetaML/MetaOCaml \cite{DBLP:conf/flops/Kiselyov14, DBLP:conf/gpce/CalcagnoTHL03} that
uses syntactic quotations and quasiquotations, LMS distinguishes binding-time
based on types. LMS provides a type constructor @Rep[T]@ where @T@ can be
an arbitrary type, indicating a value of @T@ will be known at the next stage.
All operations acting on @Rep[T]@ expressions will be residualized as generated
code.

A classic example for introducing multi-stage programming is the power function
that computes $b^x$, which is usually implemented as a recursive function:

\begin{lstlisting}
  def power(b: Int, x: Int): Int = if (x == 0) 1 else b * power(b, x - 1)
\end{lstlisting}

If @x@ is a value know at current stage, we may specialize the power function to
some value @x@ -- by unrolling the recursive calls. In LMS, this is fulfilled by
first adding the @Rep@ type to the variable known at next stage. In this case,
@b : Rep[Int]@ is know later, and @x : Int@ is know currently, as shown in the
below code recipe.
The way we use this @power@ function is to create a @DslDriver@ and override the
@snippet@ method give the currently know value @x@ (5 in this example).

\begin{lstlisting}
  new DslDriver[Int, Int] {
    def power(b: Rep[Int], x: Int): Rep[Int] = if (x == 0) 1 else b * power(b, x-1)
    def snippet(b: Rep[Int]): Rep[Int] = power(b, 5) // specialize the power to b^5
  }
\end{lstlisting}

The LMS framework provides staging support for primitive data types such as
@Int@ and @Double@, and commonly-used data structures such as lists and maps.
The idea behind the framework is to construct a sea-of-node intermediate
representations (IR) for the next stage program at the current stage
\cite{DBLP:conf/birthday/Rompf16}. For convinence, the conversion from
expressions of type @Rep[_]@ to their IR is done by using the implicit design
pattern. As we will see later, implementing staging and code generation
support for user-defined classes is also straightforward.

\subsection{Staged Concrete Semantics}

To implement the staged concrete interpreter, we replay the steps from
instantiating unstaged concrete interpreter in Section \ref{unstaged_conc}.
But now we use the @Rep@ type to annotate value domains, environments and
stores, and redefine the staged version of monads and primitive operations.

\paragraph{Staged Monads}
We use the same structure of monad stack in the unstaged interpreter: a reader
monad with a state monad. But now the monads operate on staged values, for
brivity, we call them \textit{staged monads}. In the code, we also use a @Rep@
prefix on the constructors and types to differentiate them. But it is important
to note that objects of a case class @ReaderT@/@StateT@ are not staged, the
monadic computation like @R => M[A]@ are also not staged. Instead, the internal
data that these monads operate on are staged. The following code snippet shows
the idea (we use light gray to highlight what have been changed to staged
version except naming):

\begin{lstlisting}[escapechar=!]
  case class RepReaderT[M[_]: RepMonad, R, A](run: !\hl{Rep[R]}! => M[A]) {
    def flatMap[B](f: !\hl{Rep[A]}! => ReaderT[M, R, B]): RepReaderT[M, R, B] =
      RepReaderT(r => RepMonad[M].flatMap(run(r))(a => f(a).run(r))); ... }
  case class RepStateT[M[_]: RepMonad, S, A](run: !\hl{Rep[S]}! => M[(A, S)]) {
    def flatMap[B](f: !\hl{Rep[A]}! => StateT[M, S, B]): RepStateT[M, S, B] =
      RepStateT(s => RepMonad[M].flatMap(run(s))(as => f(as._1).run(as._2))); ... }
\end{lstlisting}

The function @f@ passed to @flatMap@ is also a current-stage-known value, so
that all the invocation of @flatMap@s can be completed before generating code.
The fact that we only staged the data but not the monadic computation or monadic
values, is the reason that we can peel of the monad stack in the generated code.
Now the @AnsM[_]@ is instantiated as the same structure as before, but using the
@Rep@ versions.

\begin{lstlisting}
  type R[T] = Rep[T]
  type AnsM[T] = RepReaderT[RepStateT[RepIdM, Store, ?], Env, T]
\end{lstlisting}

TODOTODO====

We use @RepStateT@ as an example to give a sketch of how the @Rep@ versions of monad
transformers look like. The generic type @M[_]@ is a @RepMonad@ and used as the
inner monad, which means the pair @(A,S)@ is also staged when appears inside of the monad
@M[_]@. The @flatMap@ is similar to the unstaged one, but the function @f@ takes
a @Rep[A]@ value. Again, @f@ is not a next-stage function, but a current-stage
function that transforms a next-stage value of @Rep[A]@ to @StateT[M,S,B]@.
%\todo{highlight the changes in the code}


Readers may notice that the conversion between unstaged monads and staged monads
is simply changing the unstaged data to staged data, which in fact can be
achieved without modifying the implementation of monads. This is true so far, but
as we will see, it is not that easy when introducing the nondeterminism monad
(@ListT@) for abstract interpreters, as the whole list will be a next-stage
value instead of merely the elements in the list. For this reason, we explicitly
distinguish and present the two versions of monads, as a more consistent
approach.

\paragraph{Primitive Operations} Most of the primitive operations can be easily
translated to their staged versions -- we just need to let them works on staged data.
As we mentioned before, we elaborate on how to handle functions and applications in
detail. One of the desired goals is to eliminate the monadic layers through
staging, this becomes a little bit tricky for closures because we actually need 
 to compile the lambda term instead of generating a @CloV@ value, but the @ev(e)@
returns a monad and prevents the specialization. To make this happen, when
closing a lambda term, we need to specialize the body expression by
\textit{collpasing} the @AnsM@ monads to normal values. The
following function @close@ illustrates the idea: @f@ is a current stage function
but takes two next-stage values @v@ and $\sigma$, inside of which, we eagerly
collapse the monads to values by @ev(e)(ρ_*)(σ_*)@, which has type
@Rep[(Value, Store)]@. Then we generate the next-stage value of the compiled
closure using @emit_compiled_clo@, containing the compiled closure.

\begin{lstlisting}
  def emit_compiled_clo(f: (Rep[Value], Rep[Store]) => Rep[(Value, Store)]): Rep[Value]
  def close(ev: Expr => Ans)(λ: Lam, ρ: Rep[Env]): Rep[Value] = {
    val Lam(x, e) = λ
    val f: (Rep[Value], Rep[Store]) => Rep[(Value, Store)] = {
      case (v: Rep[Value], σ: Rep[Store]) =>
        val α = alloc(σ, x); val ρ_* = ρ + (unit(x) → α); val σ_* = σ + (α → v)
        ev(e)(ρ_*)(σ_*)
    }; emit_compiled_clo(f)
  }
\end{lstlisting}

When applying a function in @ap_clo@, we also generate the next-stage value
for closure application through @emit_ap_clo@, assuming that the @rator@ is already an
IR of compiled closure, @rand@ is an arbitrary stage value, and $\sigma$ is the
latest store. @emit_ap_clo@ represents the result value of the application, which has type
@Rep[(Value, Store)]@. Note that @emit_ap_clo@ is an ordinary value from the
future, and there are no monads in the future, so we need to reify the value and
store back into the monad stack through @put_store@ and @yield@.

\begin{lstlisting}
  def emit_ap_clo(rator: Rep[Value], rand: Rep[Value], σ: Rep[Store]): Rep[(Value, Store)]
  def ap_clo(ev: Expr => Ans)(rator: Rep[Value], rand: Rep[Value]): Ans = for {
    σ <- get_store
    val res: Rep[(Value, Store)] = emit_ap_clo(rator, rand, σ)
    _ <- put_store(res._2)
  } yield res._1
\end{lstlisting}

\subsection{A Little Bit Code Generation}

The method @emit_compiled_clo@ and @emit_ap_clo@ mentioned above produces
intermediate representations for the denoted operations in LMS. These IRs are
wrappers of the underlying staged data. We define these IR nodes using
@case class@ extending from @Def[T]@, meaning that they are next-staged definitions of type @T@.

\begin{lstlisting}
  type ValSt = (Value, Store)
  case class IRCompiledClo(f: Rep[(ValSt) => ValSt], λ: Lam, ρ: Rep[Env]) extends Def[Value]
  case class IRApClo(rator: Rep[Value], rand: Rep[Value], σ: Rep[Store]) extends Def[ValSt]
\end{lstlisting}

After constructing the IR graph, LMS provides an @emitNode@ to generate code for
each kind of IR node. To generate code for @IRApClo@, we know that @rator@ is a
compiled closure @IRCompiledClo@, which contains a next-stage function @f@, so
we just need to generate code that invokes @f@ with @rand@ and @σ@ as arguments
provided. Similarly, we directly generate a next-stage value @CompiledClo(f)@
for @IRCompiledClo@.

\begin{lstlisting}
  override def emitNode(sym: Sym[Any], rhs: Def[Any]) = rhs match {
    case IRApClo(rator, rand, σ) => emitValDef(sym, s"$\textdollar$rator.f($\textdollar$rand, $\textdollar$σ)")
    case IRCompiledClo(f, λ, ρ) => emitValDef(sym, s"CompiledClo($\textdollar$f, $\textdollar$λ, $\textdollar$ρ)")
    ...
  }
\end{lstlisting}
