\section{From Interpreters to Staged Interpreters} \label{stagedinterp}

In this section, based on the generic interpreter and concrete components we
presented in Section \ref{prelim}, we show how to obtain a staged concrete
interpreters by changing the monads and refactoring several primitive
operations. We begin by briefly introducing the Lightweight Modular Staging
framework in Scala.

\subsection{Multi-Stage Programming with LMS}

Lightweight modular staging (LMS) \cite{DBLP:conf/gpce/RompfO10} is a
multi-stage programming framework implemented as a Scala library that enables
dynamic code generation in a type-safe manner. Different from the approach of
MetaOCaml \cite{DBLP:conf/flops/Kiselyov14, DBLP:conf/gpce/CalcagnoTHL03} that
uses syntactic quotation and escape, LMS distinguishes binding-time solely based
on types. LMS provides a type constructor @Rep[T]@ where @T@ can be an arbitrary
type, indicating a value of @T@ will be known at next stage. All operations
acting a @Rep[T]@ expression will be residualized as generated code.

The LMS framework provides staging support for primitive data types such as
@Int@ and @Double@, and commonly-used data structures such as arrays and maps.
The idea is to construct intermediate representations (IR) for the next-stage
program at current stage. Such conversion from expressions of type @Rep[_]@ to
their IR is done by using implicit design pattern. As we will see later,
implementing the staging support for custom classes is also straightforward.

For example, let us go back and see how LMS can be used to specialize the
@power@ function from Section ~\ref{intro}.

\begin{lstlisting}
  new DslDriver[Int, Int] {
    def power(b: Rep[Int], x: Int): Rep[Int] = if (x == 0) 1 else b * power(b, x-1)
    def snippet(x: Rep[Int]): Rep[Int] = power(x, 5)
  }
\end{lstlisting}

\todo{Show some IR}

\subsection{Staged Concrete Semantics}

To implement the staged concrete interpreter, we reuse the exactly same concrete
components from Section \ref{unstaged_conc}, but we instantiate the binding-time
type @R[T]@ as @Rep[T]@.

\paragraph{Staged Monads} We also need to use monads that operate on staged
values. Note that the monadic objects such as @StateT@ or the internal value
@R[S] => M[(A,S)]@ are not staged, only the data @S@ or @A@ are staged.
This is why we can completely eliminate the overhead caused by the monadic layers.
Here we use the prefix name @Rep@ to differentitate unstaged monads and staged
monads, such as @RepReaderT@. So the @AnsM[_]@ is instantiated as the same monad
stack before, but with @Rep@ versions.

\begin{lstlisting}
  type R[T] = Rep[T]
  type AnsM[T] = RepReaderT[RepStateT[RepIdM, Store, ?], Env, T]
\end{lstlisting}

We use @RepStateT@ as an example to give a sketch of how the @Rep@ versions of monad
transformers look like. The generic type @M[_]@ is a @RepMonad@ and used as the
inner monad, which means the pair @(A,S)@ is also staged when appears inside of monad
@M[_]@. The @flatMap@ is similar to the unstaged one, but the function @f@ takes
a @Rep[A]@ value. Again, @f@ is not a next-stage function, but a current-stage
function that transforms a next-stage value of @Rep[A]@ to @StateT[M,S,B]@.
\todo{highlight the changes in the code}

\begin{lstlisting}
  case class RepStateT[M[_]: RepMonad, S, A](run: Rep[S] => M[(A, S)]) {
    def flatMap[B](f: Rep[A] => StateT[M, S, B]): StateT[M, S, B] = ...
  }
\end{lstlisting}

Readers may notice that the conversion between unstaged monads and staged monads
is simply changing the unstaged data to staged data, which in fact can be
achieved without modifying the implementation of monads. This is true so far, but
as we will see, it is not that easy when introducing the nondeterminism monad
(@ListT@) for abstract interpreters, as the whole list will be a next-stage
value instead of merely the elements in the list. For this reason, we explicitly
distinguish and present the two versions of monads, as a more consistent
approach.

\paragraph{Primitive Operations} Most of the primitive operations can be easily
translated to their staged versions -- we just need to let them works on staged data.
As we mentioned before, we elaborate how to handle functions and applications in
detail. One of the desired goal is to eliminate the monadic layers through
staging, this becomes a little bit tricky for clsoures, because we need actually
compiling the lambda term instead of generating a @CloV@ value, but the @ev(e)@
returns a monad and prevents the specialization. To make this happen, when
closing a lambda term, we need to staging into the body expression by
\textit{collpasing} the @AnsM@ monads to normal values. The
following function @close@ illustrates the idea: @f@ is a current stage function
but takes two next-stage values @v@ and $\sigma$, inside of which, we eagerly
collapse the monads to values by @ev(e)(ρ_*)(σ_*)@, which has type
@Rep[(Value, Store)]@. Then we generate an intermediate representation of the compiled
closure using @emit_compiled_clo@. The intermediate representation stands for a
next-stage value, and contains the compiled function, which will be generated as
a next-stage function.

\begin{lstlisting}
  def emit_compiled_clo(f: (Rep[Value], Rep[Store]) => Rep[(Value, Store)]): Rep[Value]
  def close(ev: EvalFun)(λ: Lam, ρ: Rep[Env]): Rep[Value] = {
    val Lam(x, e) = λ
    val f: (Rep[Value], Rep[Store]) => Rep[(Value, Store)] = {
      case (v: Rep[Value], σ: Rep[Store]) =>
        val α = alloc(σ, x); val ρ_* = ρ + (unit(x) → α); val σ_* = σ + (α + v)
        ev(e)(ρ_*)(σ_*)
    }; emit_compiled_clo(f)
  }
\end{lstlisting}

When applying a function in @ap_clo@, we generate an intermediate representation
for closure application through @emit_ap_clo@, assuming that the @rator@ is already an
IR of compiled closure, @rand@ is an arbitrary stage value, and $\sigma$ is the
latest store. @emit_ap_clo@ represents a next-stage value of type
@Rep[(Value, Store)]@, which is a collapsed monadic value from next stage, now
we need to reify the value and store back into the monad stack through
@put_store@ and @yield@.

\begin{lstlisting}
  def emit_ap_clo(rator: Rep[Value], rand: Rep[Value], σ: Rep[Store]): Rep[(Value, Store)]
  def ap_clo(ev: EvalFun)(rator: Rep[Value], rand: Rep[Value]): Ans = for {
    σ <- get_store
    val res: Rep[(Value, Store)] = emit_ap_clo(rator, rand, σ)
    _ <- put_store(res._2)
  } yield res._1
\end{lstlisting}

\paragraph{Code Generation}

The method @emit_compiled_clo@ and @emit_ap_clo@ mentioned above produces IR
node in LMS. We define these IR node as follow: \note{Exp, Def, quote are omitted}

\begin{lstlisting}
  case class IRCompiledClo(f: Rep[((Value,Store)) => (Value, Store)]) extends Rep[Value]
  case class IRApClo(rator: Rep[Value], rand: Rep[Value], σ: Rep[Store]) extends Rep[(Value, Store)]
\end{lstlisting}

After constructing the IR graph, LMS provides an @emitNode@ to generate code for
each kind of IR node. To generate code for @IRApClo@, we know that @rator@ is a
compiled closure @IRCompiledClo@, which contains a next-stage function @f@, so
we just need to generate code that invokes @f@ with @rand@ and @σ@ as arguments
provided. Similarly, we directly generate a next-stage value @CompiledClo(f)@
for @IRCompiledClo@.

\begin{lstlisting}
  override def emitNode(sym: Sym[Any], rhs: Def[Any]) = rhs match {
    case IRApClo(rator, rand, σ) => emitValDef(sym, s"$\textdollar$rator.f($\textdollar$rand, $\textdollar$σ)")
    case IRCompiledClo(f) => emitValDef(sym, s"CompiledClo($\textdollar$f)")
    ...
  }
\end{lstlisting}