\section{From Interpreters to Staged Interpreters} \label{stagedinterp}

In this section, based on the generic interpreter and concrete components we
presented in Section \ref{prelim}, we show how to stage the concrete
interpreter by changing the monad type and refactoring the primitive
operations. We begin by briefly introducing the Lightweight Modular Staging
framework in Scala, and then replay the same steps as we did for the unstaged
counterpart. Finally, we briefly describe the code generation.

\subsection{Multi-Stage Programming with LMS}

Lightweight Modular Staging (LMS) \cite{DBLP:conf/gpce/RompfO10} is a
multi-stage programming framework implemented as a Scala library. LMS enables
runtime code generation in a type-safe manner. Different from the approach of
MetaML/MetaOCaml \cite{DBLP:conf/flops/Kiselyov14, DBLP:conf/gpce/CalcagnoTHL03} which
uses syntactic quotations, LMS distinguishes binding-times based on types. 
To achieve this, LMS provides a type constructor @Rep[T]@, where @T@ can be an
arbitrary type, indicating that a value of type @T@ will be known at a future
stage.  Operations acting on @Rep[T]@ expressions will be residualized as
generated code.
A classic example for introducing multi-stage programming is the power function
that computes $b^x$. Usually, it is implemented as a recursive function:
\begin{lstlisting}
  def power(b: Int, x: Int): Int = if (x == 0) 1 else b * power(b, x - 1)
\end{lstlisting}

If @x@ is known at the current stage, we may specialize the power function to
the value @x@ by unrolling the recursive calls. In LMS, this is fulfilled by
first adding the @Rep@ type annotation to variables known at the next stage. In this case,
@b: Rep[Int]@ is known later, and @x: Int@ is known currently, as shown below.
The way we use this staged @power@ function is to create a @DslDriver@ and override the
@snippet@ method that supplies 5 as the currently known value for @x@.
\begin{lstlisting}
  new DslDriver[Int, Int] {
    def power(b: Rep[Int], x: Int): Rep[Int] = if (x == 0) 1 else b * power(b, x-1)
    def snippet(b: Rep[Int]): Rep[Int] = power(b, 5) // specialize the power to b^5
  }
\end{lstlisting}

The LMS framework provides staging support for primitive data types such as
@Int@ and @Double@, and for commonly-used data structures such as lists and
maps.  The framework internally constructs a sea-of-nodes intermediate
representation (IR) for the next-stage program when executing the current stage
program \cite{DBLP:conf/birthday/Rompf16}.
For convenience, the conversion from expressions of type @Rep[_]@ to their IR
is done by using the implicit design pattern. As we will see later,
implementing staging and code generation support for user-defined classes is
also straightforward.

\subsection{Staged Concrete Semantics}

To implement the staged concrete interpreter, we replay the steps from the
instantiation of the unstaged counterpart in Section \ref{unstaged_conc}.
However, now we use the @Rep@ type to annotate the value domains, environments and
stores, and define the staged version of monads and primitive operations.

\paragraph{Staged Monads}
We use the same monad stack structure as in the unstaged interpreter: a reader
monad with a state monad. The difference here is that, now the monads operate
on staged values.  For brevity, we call them \textit{staged monads}. In the
code snippet, we also use a @Rep@ prefix on the name of constructors and types
to differentiate them.
But it is important to note that the instances of @ReaderT@/@StateT@ are not
staged, and that the monadic computation, such as functions of @R => M[A]@, are
also not staged. Instead, the internal data that these monads operate on are
staged, i.e., values of type @R@ and @A@ in the reader monad, and values of type @S@ and
@A@ in the state monad.
The following code snippet shows the idea. We use \hl{light gray} to highlight
where @Rep@ types are added.
\begin{lstlisting}[escapechar=!]
  case class RepReaderT[M[_]: RepMonad, R, A](run: !\hl{Rep[R]}! => M[A]) {
    def flatMap[B](f: !\hl{Rep[A]}! => ReaderT[M, R, B]): RepReaderT[M, R, B] =
      RepReaderT(r => RepMonad[M].flatMap(run(r))(a => f(a).run(r))); ... }
  case class RepStateT[M[_]: RepMonad, S, A](run: !\hl{Rep[S]}! => M[(A, S)]) {
    def flatMap[B](f: !\hl{Rep[A]}! => StateT[M, S, B]): RepStateT[M, S, B] =
      RepStateT(s => RepMonad[M].flatMap(run(s))(as => f(as._1).run(as._2))); ... }
\end{lstlisting}

The function @f@ passed to the @flatMap@ is also a value known at the current stage.
Therefore, all invocations of @flatMap@ can be reduced before generating code.
The fact that we only stage data but not the monadic computation or monadic
values is the reason that we can peel of the monad stack in the generated code
through staging.  Now the type @AnsM[_]@ is instantiated as the same structure
as before, but using the @Rep@ versions of monad transformers:
\begin{lstlisting}
  type R[T] = Rep[T]
  type AnsM[T] = RepReaderT[RepStateT[RepIdM, Store, ?], Env, T]
\end{lstlisting}

Readers may notice that the conversion between unstaged monads and staged monads
is merely changing the type of unstaged data to staged data, which in fact can
be achieved without modifying the implementation of monads. This is true so far,
but as we will see, it is not as straightforward for the nondeterminism monad
(@SetT@) in the abstract interpreter, because not only the elements in the set are
staged, but the whole set is also staged, and we have no knowledge about how
many elements in the set. Therefore, any traversals over the set has to be
staged.  In this section, we explicitly distinguish the two versions of monadic
interfaces. Later, we will instantiate the staged set monad by manually fusing
the fragment of the monad stack inside of @SetT@.

\paragraph{Primitive Operations} Most of the primitive operations can be easily
translated to their staged versions -- we just need to change the types.  As we
mentioned before, we will illustrate in detail how functions and applications
are handled.  One problem here is what should we do for $\lambda$ terms when
staging them? We cannot directly create a next-stage defunctionalized closure
for it, because that means we still need to interpret over these closures at the
next stage and the interpretation overhead has not been eliminated. 
The solution is to compile the $\lambda$ term with its environment by calling
the evaluator at the current stage. The result is generating a next-stage
Scala function. The evaluator is passed as @ev@ to the method @close@.
The following code implements the idea:
\begin{lstlisting}
  type ValSt = (Value, Store)
  def emit_compiled_clo(f: (Rep[Value], Rep[Store]) => Rep[ValSt], λ: Lam, ρ: Exp[Env]): Rep[Value]
  def close(ev: Expr => Ans)(λ: Lam, ρ: Rep[Env]): Rep[Value] = {
    val Lam(x, e) = λ
    val f: (Rep[Value], Rep[Store]) => Rep[ValSt] = { case (v: Rep[Value], σ: Rep[Store]) =>
        val α = alloc(σ, x)
        ev(e)(ρ + (unit(x) → α))(σ + (α → v))
    }; emit_compiled_clo(f, λ, ρ)
  }
\end{lstlisting}

In @close@, we first create a function @f@ that takes a next-stage value and
store, and returns staged values and stores. The function @f@ is a
current-stage and we then use @emit_complied_clo@ to denote it to a next-stage
Scala function.
The method @emit_compiled_clo@ produces current-stage representations of
next-stage function values, i.e., of type @Rep[Value]@.
Inside of function @f@, we can access the evaluator via @ev@.
However, @ev@ produces a monadic value of type @AnsM@, which can only exist at
the current stage. To connect current-stage monadic values and future-stage
grounded values, the evaluator @ev@ is supplied with not only the body
expression @e@ to be specialized, but also with the prepared environment and
store.  By doing so, we have \textit{collapsed} the monadic values of @AnsM@ to
grounded values within staging; therefore in the future stage, there is no
monads.
\begin{lstlisting}
  def emit_ap_clo(fun: Rep[Value], arg: Rep[Value], σ: Rep[Store]): Rep[ValSt]
  def ap_clo(ev: EvalFun)(fun: Rep[Value], arg: Rep[Value]): Ans = for {
    σ  <- get_store
    vs <- lift[ValSt](emit_ap_clo(fun, arg, σ))
    _  <- put_store(vs._2)
  } yield vs._1
\end{lstlisting}

For function applications @ap_clo@, we have two \textit{next-stage} values @fun@
and @arg@. But what we can do with these values of type @Rep@ is limited. 
In fact, @fun@ does not have an intensional application operation we can use
directly. We only know that it is a current-stage representation of functions produced by @close@,
whose next-stage form is a Scala function.
With this fact, we can generate a next-stage function application for it, also
in our target language Scala. The method @emit_ap_clo@ produces a current-stage
representation of a next-stage function application, with necessary arguments
such as the latest store @σ@. Meanwhile, we still want the method @ap_clo@
implemented in monadic style, so that it can be smoothly connected with other
parts of the interpreter.  To achieve this, we lift the result values and
stores from the future stage into our current-stage monad stack, with a binding
@vs@.  Finally, we use @put_store@ to update the store with @vs._2@ and return
the value result @vs._1@ into the current-stage monadic value.

Now, compared with the unstaged interpreter, we observe a key difference of where
we use the evaluation function @ev@.  In the unstaged interpreter, the
invocation of @ev@ happens in @ap_clo@, i.e., at the application time; while in
the staged interpreter, we eagerly apply @ev@ when denoting the lambda terms to
our next-stage value domains, i.e., at the value-representation time.

\subsection{A Little Bit of Code Generation}

In the previous section, we showed the types of @emit_compiled_clo@ and
@emit_ap_clo@ without concrete implementations. In this section, we briefly
discuss the IR nodes generated by them and sketch the code generation.
We define the IR nodes @IRCompiledClo@ and @IRApClo@ using @case class@
extending from @Def[T]@. @Def[T]@ is a built-in type in the LMS framework,
representing next-stage value definitions of type @T@.
\begin{lstlisting}
  case class IRCompiledClo(f: Rep[(ValSt) => ValSt], λ: Lam, ρ: Rep[Env]) extends Def[Value]
  case class IRApClo(fun: Rep[Value], arg: Rep[Value], σ: Rep[Store]) extends Def[ValSt]
\end{lstlisting}

The IR nodes are manipulated by the LMS passes and finally generated as
next-stage Scala programs. To generate code, LMS provides an @emitNode@ method for
programmers to control over what code is generated for each kind of nodes. Here,
we match @IRCompiledClo@ and @IRApClo@, and generate definitions via
@emitValDef@ for them.  For compiled closures, we pass the function @f@ with its
accompanying syntactic term and environment to the next-stage value
representation @CompiledClo@. For applications, we know that @fun@ is
an instance of @CompiledClo@ at the next stage, and it has a callable field
@f@. Hence, we generate a next-stage function application @f(arg, σ)@.  The
invocations of @quote@ are used to generate proper names for other
next-stage values.
\begin{lstlisting}
override def emitNode(sym: Sym[Any], rhs: Def[Any]) = rhs match {
  case IRCompiledClo(f, λ, ρ) => emitValDef(sym, s"CompiledClo($\textdollar${quote(f)}, $\textdollar${quote(λ)}, $\textdollar${quote(ρ)})")
  case IRApClo(fun, arg, σ) => emitValDef(sym, s"$\textdollar$fun.f($\textdollar${quote(arg)}, $\textdollar${quote(σ)})")
  ...
}
\end{lstlisting}
