\subsection{From Interpreter to Abstract Interpreter}

Based on the generic interface we presented in Section~\ref{bg_lang}, now we describe the instantiation of an
abstract interpreter.
To keep the simplicity, we intend to use simply abstract domains, and just establish a 
context-/path-/flow-insensitive, store-widened analysis in this section --- indeed, it is coarse 
but enough to setup a foundation for the staged abstract interpreter we will present later.
However, in Section \note{X}, we will see how to regain context-/path-/flow-sensitivity 
for pushdown control-flow analysis; in Section \note{Y}, we will see how to instantiate 
an interval abstract domain for numerical analysis.

\todo{maybe put to related work} other big-step abstract interpreter, 
big-step abstract interpreter using delimited control operators \cite{Wei:2018:RAA:3243631.3236800},
monadic big-step abstract interpreter \cite{DBLP:journals/pacmpl/DaraisLNH17},
arrow-based abstract interpreter \cite{Keidel:2018:CSP:3243631.3236767}.

\paragraph{Abstract Instantiation.}
Our instantiation roughly follows the Abstracting Abstract Machines methodology 
\cite{DBLP:conf/icfp/HornM10, DBLP:journals/jfp/HornM12} that transforms the concrete semantics 
to abstract semantics. The store now maps addresses to sets of abstract values, meaning that an address points
to all possible values may occur at runtime, where the abstract value is either a closure or 
a single abstract number \texttt{NumV}, which stands for the top. The address space is also
constrained to be finite, in the monovariant setting, we simply use variable names for it.

\begin{lstlisting}
trait Abstract extends Semantics {
  case class Addr(x: Ident);  sealed trait AbsValue
  case class CloV($\lambda$: Lam, $\rho$: Env) extends AbsValue
  case class NumV() extends AbsValue
  type Value = Set[AbsValue]
  type Env = Map[Ident, Addr];  type Store = Map[Addr, Value]
}
\end{lstlisting}

\paragraph{Stage Polymorphic Lattices.}
To effectively reuse the code between unstaged and staged part, we also make the lattice structure to
be stage polymorphic. A lattice type class parameterizes over an element type \texttt{E} and defines
five operations for \texttt{R[E]}, such as meet, join, and ordering relation; again, \texttt{R} 
is the higher-kinded type indicating the binding-time.
\begin{lstlisting}
trait Lattice[E, R[+_]] {
  val bot: R[E];  val top: R[E]
  def $\sqsubseteq$(l1: R[E], l2: R[E]): R[Boolean]
  def $\sqcup$(l1: R[E], l2: R[E]): R[E]
  def $\sqcap$(l1: R[E], l2: R[E]): R[E]
}
\end{lstlisting}

For example, here we use powerset as the abstract domain, and the unstaged lattice 
of powerset can be easily implemented as follows:

\begin{lstlisting}
implicit def SetLattice[T]: Lattice[Set[T], NoRep] = 
  new Lattice[Set[T], NoRep] {
    lazy val bot: Set[T] = Set[T]()
    lazy val top: Set[T] = throw new NotImplementedError()
    def $\sqsubseteq$(l1: Set[T], l2: Set[T]): Boolean = l1 subsetOf l2
    def $\sqcup$(l1: Set[T], l2: Set[T]): Set[T] = l1 union l2
    def $\sqcap$(l1: Set[T], l2: Set[T]): Set[T] = l1 intersect l2
  }
\end{lstlisting}

The ordering relation is to ask whether one set is a subset of the other, 
join is to union the two set, and meet is to intersect the two sets.
Accordingly, other structures used in the rest of the paper such as tuples 
(products) and maps can be lifted to lattices element-wise or point-wise.
The code for them are elided.

\paragraph{Operations Implementation.}
The @get@ and @put@ operation on abstract stores slightly changed according to
the abstract semantics: @get@ uses bottom element in the @Value@ lattice if the queried
address not exists; @put@ performs the update by joining with the existing values.
@alloc@ simply invokes the @Addr@ constructor.
\texttt{close} and \texttt{num} lift syntactic literals to a singleton set that
contains one \texttt{CloV} or \texttt{NumV} object.

\todo{conditional, should use different store}
\begin{lstlisting}
object AbsInterp extends Abstract {
  type R[+T] = T
  val $\rho$0 = Map[Ident, Addr]();  val $\sigma$0 = Map[Addr, Value]()
  def get($\rho$: Env, x: Ident): Addr = $\rho$(x)
  def put($\rho$: Env, x: Ident, a: Addr): Env = $\rho$ + (x -> a)
  def get($\sigma$: Store, a: Addr): Value = 
    $\sigma$.getOrElse(a, Lattice[Value].bot)
  def put($\sigma$: Store, a: Addr, v: Value): Store =
    $\sigma$ + (a -> (v $\sqcup$ get($\sigma$, a)))
  def alloc($\sigma$: Store, x: Ident): Addr = Addr(x)
  def close(ev: EvalFun)($\lambda$: Lam, $\rho$: Env): Value = Set(CloV($\lambda$, $\rho$))
  def num(i: Lit): Value = Set(NumV())
  def apply_closure(ev: EvalFun)
    (f: Value, arg: Value, $\sigma$: Store): Ans = { var $\sigma$_* = $\sigma$
      val vs = for (CloV(Lam(x, e), $\rho$) <- f) yield {
        val $\alpha$ = alloc($\sigma$_*, x)
        val (v, v$\sigma$) = ev(e, put($\rho$, x, $\alpha$), put($\sigma$_*, $\alpha$, arg))
        $\sigma$_* = v$\sigma$; v
      }
    (vs.reduce(Lattice[Value].$\sqcup$), $\sigma$_*)
  }
  def branch0(cnd: Value, thn: =>Ans, els: =>Ans): Ans = thn $\sqcup$ els //TODO!
  def prim_eval(op: Symbol, v1: Value, v2: Value): Value = Set(NumV())
} // to be continued
\end{lstlisting}

\paragraph{Fixpoint Iteration.}
We described the abstract semantics modularly in the last section, however, the abstract 
interpreter may not terminate for some input program. In this section, we use a memoization
technique to ensure its termination. This technique is also called co-inductive caching algorithm
\cite{DBLP:journals/pacmpl/DaraisLNH17, Wei:2018:RAA:3243631.3236800} or truncated depth-first evaluation 
\cite{Rosendahl:AbsIntPL} and widely used in different analysis.

The idea is to set up two caches called @in@ and @out@, which are both mapping from the arguments of interpreter 
(@Config@) to the returned value of interpreter (@Ans@). The @in@ contains what we already computed from the last
iteration, the @out@ cache is what we computed after this iteration joined with the previous one.
When starting a new iteration, the @in@ is set to be the result of last iteration, i.e., @out@; the @out@ is set
to be empty. In the iteration, we first ask whether @out@ contains what we want, if yes then it is returned;
otherwise, we retrieve what we have from @in@, compute the result for this time, and put the joined result back to @out@.
Note that we also instrument the recursive call by putting @cached_ev@ to @evev@.
After one iteration, if @in == out@, then there is no more information can be gained, thus the iteration should 
end and we have reached the fixed point.

\begin{lstlisting}
case class CacheFix(evev: EvalFun => EvalFun) {
  var in = Map[Config, Ans](); var out = Map[Config, Ans]()
  def cached_ev(e: Expr, $\rho$: Env, $\sigma$: Store): Ans = {
    val cfg: Config = (e, $\rho$, $\sigma$)
    if (out.contains(cfg)) out(cfg)
    else {
      val ans0 = in.getOrElse(cfg, Lattice[(Value, Store)].bot)
      out = out + (cfg -> ans0)
      val ans1 = evev(cached_ev)(e, $\rho$, $\sigma$)
      out = out + (cfg -> (ans0 $\sqcup$ ans1));  ans1
    }
  }
  def iter(e: Expr, $\rho$: Env, $\sigma$: Store): Ans = {
    in = out; out = Map[Config, Ans](); cached_ev(e, $\rho$, $\sigma$)
    if (in == out) out((e, $\rho$, $\sigma$)) else iter(e, $\rho$, $\sigma$)
  }
}
override def eval_top(e: Expr, $\rho$: Env, $\sigma$: Store): Ans = 
  CacheFix(eval).iter(e, $\rho$, $\sigma$)
\end{lstlisting}

Finally, we override the definition of @eval_top@ by instantiating @CacheFix@ with @eval@ and start 
the first iteration.
