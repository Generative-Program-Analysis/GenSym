\section{Case Study} \label{cases_study}

In Section~\ref{sai}, we have shown staging an abstract interpreter is feasible and can be 
systematic after correctly identifying the binding-times, despite the fact that the abstract 
interpreter is intended to be imprecise and easy to implement. 
In this section, we conduct several case studies to show that it is also useful and widely
applicable to different analysis.

\subsection{Abstract Compilation a la Staging} \label{cs_ac}

\citeauthor{Boucher:1996:ACN:647473.727587} introduced abstract compilation (AC) as a new
implementation technique for abstract interpretation based static analysis \cite{Boucher:1996:ACN:647473.727587}.
The idea is inspired by partial evaluation and similar with the presented paper -- the program can be known 
statically therefore the overhead of interpretation can be eliminated. 
In AC, the compiled analysis can be represented by either text or closures (higher-order functions);
the closures can be executed immediately however the textual program need to be loaded firstly.

Specifically, \citeauthor{Boucher:1996:ACN:647473.727587} show how to compile a monovariant control-flow 
analysis \cite{Shivers:1991:SSC:115865.115884, Shivers:1988:CFA:53990.54007} 
for continuation-passing style (CPS) programs. Since the analyzed program is written in CPS, the analyzer 
is essentially a big-step control-environment abstract interpreter.
Closure generation compiles the analysis as a closure which takes an environment as argument.
And the overhead of traversing the abstract syntax tree of input program also has been eliminated.

In this section, we show that \citeauthor{Boucher:1996:ACN:647473.727587}'s abstract compilation can be 
understood and implemented as an instance of staging abstract interpreters.
We firstly revisit the original implementation of abstract compilation of 0-CFA,
and then reproduce their result by simply adding stage annotations.
The generated program of our approach improves approximately the same extent of speed,
but without changing a single line of the analyzer program (with the use of LMS).
However, closure generation requires more engineering effort, such as a whole-program 
conversion on the analyzer. Moreover, as shown in Section~\ref{staged_ds}, 
our approach is able to not only remove the interpretive overhead, but also specialize 
the data structures used in the analysis, for example, the environment that maps variables to sets of lambda.

\begin{figure*}
  \centering
  \begin{subfigure}[h]{0.49\textwidth}
    \centering
    \begin{lstlisting}
type CompAnalysis = Store => Store
def compProgram(prog: Expr): CompAnalysis = compCall(prog)
def compCall(call: Expr): CompAnalysis = call match {
  case Letrec(bds, body) =>
    val C1 = compCall(body); val C2 = compArgs(bds.map(_.value))
    ($\sigma$: Store) => C1(C2($\sigma$.update(bds.map(_.name), 
       bds.map(b => Set(b.value.asInstanceOf[Lam])))))
  case App(f, args) =>
    val C1 = compApp(f, args); val C2 = compArgs(args)
    ($\sigma$: Store) => C1(C2($\sigma$))
}
def compApp(f: Expr, args: List[Expr]): CompAnalysis = 
  f match {
    case Var(x) => ($\sigma$: Store) => 
      analysisAbsApp($\sigma$.lookup(x), args, $\sigma$)
    case Op(_) => compArgs(args)
    case Lam(vars, body) =>
      val C = compCall(body)
      ($\sigma$: Store) => 
        C($\sigma$.update(vars, args.map(primEval(_, $\sigma$))))
  }
def compArgs(args: List[Expr]): CompAnalysis = args match {
  case Nil => ($\sigma$: Store) => $\sigma$
  case (arg@Lam(vars, body))::rest =>
    val C1 = compCall(body); val C2 = compArgs(rest)
    ($\sigma$: Store) => C2(C1($\sigma$))
  case _::rest => compArgs(rest)
}
  \end{lstlisting}
  \end{subfigure}
\hfill
  \begin{subfigure}[h]{0.49\textwidth}
    \centering
    \begin{lstlisting}
def analyzeProgram(prog: Expr, $\sigma$: Rep[Store]): Rep[Store] = 
  analyzeCall(prog, $\sigma$)
def analyzeCall(call: Expr, $\sigma$: Rep[Store]): Rep[Store] = 
  call match {
    case App(f, args) => analyzeApp(f, args, analyzeArgs(args, $\sigma$))
    case Letrec(bds, body) =>
      val $\sigma$_* = $\sigma$.update(bds.map(_.name), 
        bds.map(b => Set(b.value.asInstanceOf[Lam])))
      val $\sigma$_** = analyzeArgs(bds.map(_.value), $\sigma$_*)
      analyzeCall(body, $\sigma$_**)
  }
def analyzeApp(f: Expr, args: List[Expr], $\sigma$: Rep[Store]): Rep[Store] = 
  f match {
    case Var(x) => analyzeAbsApp(args, $\sigma$(x), $\sigma$)
    case Op(_) => analyzeArgs(args, $\sigma$)
    case Lam(vars, body) =>
      val $\sigma$_* = $\sigma$.update(vars, args.map(primEval(_, $\sigma$)))
      analyzeCall(body, $\sigma$_*)
  }
def analyzeArgs(args: List[Expr], $\sigma$: Rep[Store]): Rep[Store] = 
  args match {
    case Nil => $\sigma$
    case Lam(vars, body)::rest => analyzeArgs(rest, analyzeCall(body, $\sigma$))
    case _::rest => analyzeArgs(rest, $\sigma$)
  }
  \end{lstlisting}

  \end{subfigure}
  \caption{Comparison of AC (left) and SAI (right). Only core code are shown.}
  \label{compare_ac_sai}
\end{figure*}

\subsubsection{Closure Generation}

The analysis presented by \citeauthor{Boucher:1996:ACN:647473.727587} is 0-CFA for a CPS language consisting
of lambda terms, applications, @letrec@ and primtive operators. The analyses for different syntactic 
structs are decomposed to different functions, such as @analyzeCall@ and @analyzeApp@. The idea of closure
generation is to rewrite these functions, where previously they may take both static arugments and dynamic arguments,
but after the rewrite only the static arguments is taken. In this case, the static arguments are syntactic terms;
the dynamic arguments are stores. After written in AC style, the functions like @compCall@ (compiled version of @analyzeCall@) 
returns a value of type
@CompAnalysis@, i.e., a closure that takes a store and returns a store. The result of multiple calls on such functions, for example,
@compCall@ and @compArgs@ can be composed. The generated closure only takes stores, because the input program
is specialized into the closure. The code is shown in Figure~\ref{compare_ac_sai} (left).

\subsubsection{Staged 0-CFA}

On the other side, our approach does exactly the same thing through staging: the syntactic terms are static, and 
stores are dynamic, therefore the generated code just looks-up and updates the store.
Figure~\ref{compare_ac_sai} (right) shows the code written with LMS. In fact, the only changes are
the type of @Store@ is replaced with @Rep[Store]@ indicating that the values of type @Store@  will be known
at the next stage. Indeed, a amount of additional engineering efforts are required to make this happen, 
including: the staged version @Map@ which is already included in LMS; implicit @lift@ function that transform a
current-stage constant value to next stage; next-stage representation of the syntactic terms, i.e., proper @toString@
functions of AST structs. We consider these efforts are relatively small, and does not interfere the actual analysis
we desire.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Control-flow Analysis} \label{cfa}

The target language we presented in Section~\ref{bg_lang} is essentially a higher-order functional language.
One fundamental analysis task for functional programs is the control-flow analysis, i,e., determining
which functions will possibly be applied at each call-site. 
The abstract interpreter we used in Section~\ref{unstaged_abs} and Section~\ref{sai} is already 
a store-widened 0-CFA-like abstraction; in last section, we also reviewed AC with 0-CFA.
In this section, based on the existing staged abstract interpreter, we further
develop the staging techniques with control-flow analyses, including recoverying a more precise store model;
different polyvariant analsis, and using staging as a implementation stategy for mixed sensitivity.

\subsubsection{A More Precise Store Model}

\subsubsection{Context/Path/Flow-Sensitivity}

\subsubsection{Mixed Sensitivity}

using different $k$ for $k$-CFA

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Numerical Analysis in Imperative Languages} \label{cases_imp}

Beyond control-flow analysis, staging data structures used in the abstract interpreter, such as abstract domain.
It has been shown that specializing abstract domains with respect to the structure of analyzed program significantly 
improves the performance: a recent example is online decomposition of polyhedra \cite{DBLP:conf/popl/SinghPV17, Singh:2017:PCD:3177123.3158143}.
In this section, we present a similar idea for interval abstract domain using an \textit{offline} fashion by staging.

\subsubsection{Scaling to Imperative Languages}

Assignments, Loops, Exceptions

\subsubsection{Specializing Interval Domains}

Intervals, Numerical Analysis

Options: Data-flow analysis, taint analysis
